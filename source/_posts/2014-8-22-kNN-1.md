title: 机器学习与R(8)-k nearest neighbors-1
date: 2014-08-22 19:46:35
category: 数据科学
tags: [数据科学,机器学习,R,机器学习与R]
---


今天我们介绍最简单但是却很实用的kNN分类算法的R实现。

## kNN简介

基于Nearest Neighbors的分类可以用于：

+ 比如文字识别，面部识别
+ 预测某人是否喜欢推荐的电影（Netflix）
+ 基因模式识别，比如用于检测某种疾病

通常最近邻居分类器适用于特征与目标类之间的关系为比较复杂的数字类型，或者说二者关系难以理解，但是相似类间特征总是相似。kNN算法：

+ 简单有效，对数据分布没有假设，数据训练也很快
+ 但是它没有模型输出，因此限制了对特征间关系的理解
+ 分类阶段也比较慢，耗费内存
+ 对nominal特征以及缺少数据需要预先处理

## 算法

kNN分类的思想很简单就是**物以类聚，人以群分**，根据与待分类数据集中的最近的k个训练集中的分类标签来对决定其类别。比如说我们已经知道如下数据：

食物|甜度|脆度|食物类别
---:|---:|---:|---:|
apple|10|9|水果
bacon|1|4|蛋白质
banana|10|1|水果
carrot|7|10|蔬菜
celery|3|10|蔬菜
cheese|1|1|蛋白质

kNN算法会所有特征作为多维空间的坐标，通过计算这些点之间的距离来计算每一个样本间的相似性。

![kNN算法](/img/rmachine/knn-algorim.png)

显然从这里我们可以看出组内数据间的相似性。此时如果我有一个西红柿需要分类，那么根据它的k个最近邻居我们就可以做出判断，k=1时分类与最近那个相同，k=3那么就是投票决定。

![kNN算法](/img/rmachine/knn-tomato.png)

前面讲到了距离那么必然涉及距离如何计算的。常用的有欧几里德距离：

$$ dist(p,q)=\sqrt{(p_1-q_1)^2+(p_2-q_2)^2+(p_3-q_3)^2+...} $$

因此我们可以计算：

食物|甜度|脆度|食物类别|与tomato的距离
---:|---:|---:|---:|---:|
grape| 8 |5 |水果|sqrt((6 - 8)^2 + (4 - 5)^2) = 2.2
green bean| 3| 7 |蔬菜|sqrt((6 - 3)^2 + (4 - 7)^2) = 4.2
nuts |3| 6|蛋白质| sqrt((6 - 3)^2 + (4 - 6)^2) = 3.6
orange| 7 |3|水果| sqrt((6 - 7)^2 + (4 - 3)^2) = 1.4

如果k=1，那么西红柿与orange最近，就应该是水果，如果k=3，就是投票，橘子和葡萄都说它是水果，那它就是水果了。

## k的选择

很明显k的选择对最终结果大有影响，这就是机器学习中几点的bias与variance取舍问题，鱼和熊掌不可兼得。如果k很大，那么可以减少干扰数据的影响，但是此时就导致了系统性偏差，比如如果去k为总的训练数据数，那么每次投票肯定都是训练数据中多的类别胜利。显然训练数据的系统性偏差会影响结果。而如果k=1，那么某个干扰数据或者异常数据会影响最终结果的准确性，所以我们始终是在bias与variance直接取舍。

![bias vs variance](/img/rmachine/knn-bias-variance.png)


k通常会在3～10直接取值，或者是k等于训练数据的平方根。比如15个数据，可能会取k=4

>另一个不常用的是k取较大值，但是我们在投票时权重不同

## 数据的预处理

有了计算距离的方法，也有了k的取值，是否我们就可以开始分类了？不要急，还有重要的一步就是，数据的预处理。简单考虑一下，比如我们度量各个特征的时候刻度单位不同，那么会带来什么问题。特征A取值是从0～1，另一个特征B则是0～10000，这里特征B的1000，不代表是特征A 的1的1000倍，因此我们要对数据进行标准化。传统的是采用最小最大值标准化方法：

$$ X_new=\frac{X-min(X)}{max(X)-min(X)} $$

这样$X_new$的取值就在0～1之间了。另一种方法则是z-score:

$$ X_new=\frac{X-\mu}{\sigma}=\frac{x-mean(x)}{sd(x)} $$


数据已经标准化了是否就ok了？不是的，欧几里德距离只能用于数字，对于nominal变量（就是分类变量啦），无法处理。这个时候我们可以通过dummy（哑元）来处理。比如：

+ 性别为male就取1
+ 其它取0

那如果不止2类，是多个呢？一种方法就是我创建（n-1)个哑元变量，还有一种就是如果你的分类变量是有顺序的，而且每一级间的间隔是固定的那你可以用比如1，2，3来代表冷，温暖，热。不过这时一定要注意这个间隔的问题，比如穷人，中产，富人，显然中产和穷人，富人和中产的差距就不是一样的。

## 懒惰的kNN

前面我们在什么是机器学习时说过机器学习有一个抽象的过程，而kNN实际没有抽象与泛化（generalization）的过程。kNN因此是懒惰的学习或者叫instance-based learning / rote learning。

kNN也是非参数化的，而后面我们要说的回归则是参数化的。尽管kNN分类器很懒惰，但是却很有用，后面我们用UCI的机器学习数据来看R里面的实现。

