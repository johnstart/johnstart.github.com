<?xml version="1.0" encoding="utf-8"?>
<search>
  
    
    <entry>
      <title><![CDATA[Python数据分析与可视化实战]]></title>
      <url>%2F2017%2F02%2F06%2F2017-02-6-data-analysis%2F</url>
      <content type="text"><![CDATA[终于完成了python数据科学系列的第一门课，不愿意付钱购买的也可以关注网站,我将把讲义全部发布到网站上,当然如果感兴趣的可以去网易云课堂Python数据分析与可视化实战购买,因为有许多内容的讲解是语音的,没有写成文字。同时课程也会提供答疑，以下为课程介绍： 课程组织课程包含6大模块加上1个实战案例模块。通过本课程学习你将掌握如何利用Python进行： 数据获取 数据清洗 数据整理 数据可视化 数据探索 Python统计分析通过课程学习你将掌握如何利用Python进行统计分析，包括： 描述性统计 利用Python模拟常用概率分布（正态分布，泊松分布，二项分布等） 统计推论（假设检验，卡方分析，方差分析等） 课程案例课程设计目标不是简单的讲Pandas,matplotlib等工具,而是带领你从问题出发,通过数据分析寻找答案.因此课程提供了大量实战案例分析，例如： 作为产品经理如何对应用中来自微信,微博,网站广告的用户次日留存率有无区别 如何对网站A/B测试结果进行分析 如何进行Cohort Analysis(群组分析) 如何进行网站访问日志分析 如何对呼叫中心的数据进行分析 交通违法数据分析 停车罚单分析 凶杀与时间的关系 民航航班的准点分析 电影评分数据分析 … 希望这将是你迈向python数据分析的第一门课.]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[PyData DC 2016的slides以及代码]]></title>
      <url>%2F2017%2F02%2F02%2F2017-2-2-pycon-2016-dc%2F</url>
      <content type="text"><![CDATA[PyData DC 2016 Using Dask for Parallel Computing in Python | [Code] Building Your First Data Pipelines | [Code] Doing frequentist statistics in Python | [Code] Machine Learning with Text in scikit-learn | [Code] Julia Tutorial | [Code] Parallel Python - Analyzing Large Datasets | [Code] Modern NLP in Python | [Code] Python useRs | [Code] Building Serverless Machine Learning Models in the Cloud | [Code] Learn How To Make Life Easier With Anaconda | [Code] The Five Kinds of Python Functions | [Slides] Sustainable Scrapers | [Slides] Open Data Dashboards &amp; Python Web Scraping | [Slides] | [Code] Agent-based Modeling in Python | [Code] Variational Inference in Python | [Slides] | [Jupyter NB] | [Code] Clustering: A Guide for the Perplexed | [Slides] | [Code] Logistic Regression: Behind The Scenes | [Slides] Visual diagnostics for more informed machine learning | [Slides] | [Code] Creating Python Data Pipelines in the Cloud | [Slides] Data Transformation: A Framework for Exploratory Data Analysis | [Jupyter NB] Interactive multi-scale time series exploration with matplotlib | [Code] Forecasting critical food violations at restaurants using open data | [Slides] | [Code] GraphGen: Conducting Graph Analytics over Relational Databases | [Project] NoSQL doesn’t mean No Schema | [Slides] Dask for ad-hoc distributed computing | [Slides] Eat Your Vegetables - Data Security for Data Scientists | [Slides] Becoming a Data Scientist:Advice From My Podcast Guests | [Slides] YouTubePlaylist with all 61 talks here.]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[儿子教会我的事4-欲取先予]]></title>
      <url>%2F2016%2F10%2F01%2F2016-10-learn-from-my-sons-5%2F</url>
      <content type="text"><![CDATA[欲取先予时间很快，儿子马上就要开始牙牙学语了。突然发现最好的教他喊爸爸，妈妈的方法就是“先喊他爸爸，妈妈”]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[儿子教会我的事4-坚持]]></title>
      <url>%2F2016%2F09%2F01%2F2016-09-learn-from-my-sons-4%2F</url>
      <content type="text"><![CDATA[坚持要逗儿子笑除了你需要笑一个窍门外，还有一个方法就是不断的重复同一个动作。最终他的笑点被完全打开，会一直咯咯咯的笑个不停。这就是坚持的力量，开始做一件事情不难，难的是你每天都能坚持。想想过去能有什么事情你每天都坚持吗？也许除了吃饭，睡觉就没有其他的了。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[儿子教会我的事3-笑]]></title>
      <url>%2F2016%2F08%2F01%2F2016-08-learn-from-my-sons-3%2F</url>
      <content type="text"><![CDATA[笑大儿子出生后2，3个月时就会朝着你笑，属于典型的见个人就笑的类型，以至于他妈说他是“傻白甜”。可是小儿子就不同了，他始终是木着一个“老干部”脸，从来不笑，开始要让他笑太难了。后来突然发现了逗他笑的关键是你要先笑，你对他笑然后在逗他成功率要高很多。我们的生活中又何尝不是如此呢，每天出门对你见到的人都抱以微笑吧，因为微笑是会感染的。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[儿子教会我的事2-好奇心]]></title>
      <url>%2F2016%2F07%2F01%2F2016-07-learn-from-my-sons-2%2F</url>
      <content type="text"><![CDATA[好奇心不知道什么时候开始慢慢的失去了对新鲜事物的好奇心，儿子的出现再次让我开始思考这个问题。进入了5个月后，小家伙的好奇心一天比一天强，这个世界每样东西对他都是新奇的，任何东西都想去看看。每天他乐此不彼的探索着，当我也开始陪伴他去发现“新奇的玩意”时发现保持一颗好奇的心是多么的美好，它会让你永远处在一种学习的状态。我想也许当我年龄大了，我还会去学习新东西。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[儿子教会我的事1-专注]]></title>
      <url>%2F2016%2F06%2F01%2F2016-06-learn-from-my-sons-1%2F</url>
      <content type="text"><![CDATA[专注三个月大的儿子对世界充满了好奇，任何响动都能引起他的注意，唯有一件事能让他不受影响那就是吃奶。小家伙一旦开始吃奶，随你干什么都不会受影响，你可以随意亲他，捏他，此时他只会专心干一件事-吃奶。你要是打断他，他还会以大哭来表示抗议。可是随着日子一天天的过去，小家伙开始被这个“花花世界”所困扰，吃奶时有人经过或者外面有一点响动就会停下来，转过头去看个究竟，最终我们只能采取“关禁闭式吃奶疗法”。有时想想我们成人也是如此，慢慢的一天天长大后变得不再专注，常常因为各种杂事而分心，什么时候能回到我们的初心？]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[限时免费R语言数据分析与可视化课程]]></title>
      <url>%2F2015%2F10%2F10%2F2015-10-10-free-course%2F</url>
      <content type="text"><![CDATA[R语言数据分析与可视化课程现限时免费开放(2015-10-10到2015-10-31)，课程内容： 本课程一共分5章： 第一章： “R” You Ready？–R语言基础 R语言发展历史以及应用场景 R程序与Rstudio安装 R的基本数据结构 R语言中的帮助系统 R语言中的包管理 第二章 钻石恒久远–数据可视化 R语言中的数据读入 ggplot简介 ggplot数据可视化 第三章 统计数字会撒谎–之R统计分析 假设检验，t检验 相关性分析 线性回归 第四章 点球成金–数据整理与数据分析 dplyr包使用 tidyr包使用 apply系列函数 第五章 编程一小时–R语言程序设计 if/else条件控制 循环与函数 程序调试 性能优化 课程案例:San Francisco过去3月的犯罪案件可视化分析 要获得免费的该课程优惠券（89元抵扣），只需要微博分享以下两门课程并@中继点以及3位好友即可。完成分享后，微博私信@中继点，索要优惠券** R机器学习实践课程 R语言量化交易入门]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[R数据分析与可视化基础视频课程上线]]></title>
      <url>%2F2014%2F12%2F31%2F2014-10-27-r-data-analysis-visualization%2F</url>
      <content type="text"><![CDATA[课程链接http://study.163.com/course/introduction/855058.htm 课程简介本课程一共分5章： 第一章： “R” You Ready？–R语言基础 R语言发展历史以及应用场景 R程序与Rstudio安装 R的基本数据结构 R语言中的帮助系统 R语言中的包管理 第二章 钻石恒久远–数据可视化 R语言中的数据读入 ggplot基础 ggplot数据可视化进阶 第三章 统计数字会撒谎–之R统计分析 假设检验，t检验 相关性分析 线性回归 第四章 点球成金–数据整理与数据分析 dplyr包使用 tidyr包使用 apply系列函数 棒球数据分析案例 第五章 编程一小时–R语言程序设计 if/else条件控制 循环与函数 程序调试 性能优化 课程讲解方式本课程以连载方式进行，从2014.10.24开始每周更新一章，2014.11.23日课程连载结束。 课程练习本课程练习以小组方式进行，请加入QQ 114517796(R数据分析与可视化)以方便分组，附加信息为你的云课堂用户名。课程练习以QQ群小组方式完成。 由于本人水平有限，错误之处在所难免，还望不吝指正！]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[R语言机器学习实践视频课程上线]]></title>
      <url>%2F2014%2F12%2F31%2F2014-10-27-R-machine-practice-training%2F</url>
      <content type="text"><![CDATA[课程链接http://study.163.com/course/introduction/854064.htm 课程章节第1章 掌握什么是机器学习第2章. K最近邻方法 KNN方法简介 R语言中KNN方法应用案例-乳腺癌分类 R语言中KNN方法应用案例-鸢尾属植物分类 第3章：朴素贝叶斯方法(Naive Bayes) 贝叶斯方法原理简介 R语言中利用贝叶斯方法进行垃圾信息分类 第4章 决策树方法第5章 分类规则（Classification Rule）第6章：分类模型评估第7章：Crosss Validation第8章：神经网络第9章： SVM第10章： 关联规则第11章： K-means方法第12章：模型优化 课程目标掌握什么是机器学习, 学习R语言中机器学习应用:分类，聚类，预测，关联 课程案例 乳腺癌诊断 垃圾信息分类 房价回归预测 社交网络用户分类画像 商品推荐 信用卡数据坏账风险分析 白酒质量评价 … 课程讲解方式 简单介绍原理（不会涉及详细数学推导）+利用R语言完成案例分析 课后练习+小组Kaggle竞赛 课程连载方式2014.10.24开始，每周连载一章 本课程有较多练习，将以小组方式完成，请加入QQ群 51489458 （R机器学习实践），附加信息为你的网易云课堂用户名。我们将通过QQ群进行小组分组，同时课程中的数据将在群中分享。课程的最终项目为小组方式参加Kaggle网站上的数据科学项目。 由于本人水平有限，错误之处在所难免，还望不吝指正！]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[RMySQL 安装]]></title>
      <url>%2F2014%2F11%2F01%2F2014-11-1-RMySQL%2F</url>
      <content type="text"><![CDATA[Windows下安装RMysql是很多人都碰到的一个问题，这里大致总结一下 安装mysql这个很简单，直接到网站下载就ok http://www.mysql.com/ 这里我用的是 mysql-5.5.17-win64.msi. 一路next，只是要注意”Client tools”是安装了的就可以了。 设置path 在windows里面设置系统路径h，定义一个”MYSQL_HOME”变量，把mysql安装的目录添加上就可以了。 拷贝libmysql.lib/libmsql.dll在比如安装目录”C:\Program Files\MySQL\MySQL Server 5.5\lib”下建立一个”opt”目录，然后把”C:\ProgramFiles\MySQL\MySQL Server 5.5\lib\libmysql.lib” 拷贝到 “C:\Program Files\MySQL\MySQL Server 5.5\lib\opt\” ，同时也把 “C:\ProgramFiles\MySQL\MySQL Server 5.5\lib\libmysql.dll” 拷贝到 “C:\Program Files\R-3.1.1\bin\i386” 下面 安装RMysql 首先安装 Rtools ，在这里选择合适版本下载http://cran.r-project.org/ 安装了Rtools就可以安装Rmysql了install.packages(“RMySQL”, type = “source”)]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[数据分析的视角看香港特区政府与学联对话]]></title>
      <url>%2F2014%2F10%2F24%2F2014-10-24-xulian-duihua%2F</url>
      <content type="text"><![CDATA[据新华社电 香港特区政府政务司司长林郑月娥等与学联进行了对话，这里不带任何政治色彩，也不对其进行解读，只是用数据分析的视角将对话内容可视化方式呈现。 数据来源:网上的对话实录 数据处理：将特区政府与学联的话分别分为两组，其余无修改 词云分析 利用R语言的tm包，Rwordseg包对文字进行分词，去掉停止词等，得到上图所示词云。大致看出学联对话主要在于对基本法，候选人，立法会的意见，要求选举权，被选举权，民主制度的时间表，路线图，要求约束力，其它大家可以自己解读… 以上是特区政府对话的词云，基本基本法出现频率太高，影响了词云。将基本法去掉后，得到如下词云 这里大家可以自己解读 聚类分析还是利用R语言对上述对话分析，得到学联的分析如下： 可以与前面的词云对比参照。类似特区政府的分析如下：]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[R语言可视化-所有R包词云]]></title>
      <url>%2F2014%2F10%2F24%2F2014-10-24-r-package-wordcloud%2F</url>
      <content type="text"><![CDATA[获取包信息直接访问R的网站就可以获得该信息,打开 http://cran.r-project.org/web/packages/available_packages_by_date.html 网页可以发现网站有一个html表 123456789101112131415161718192021require(XML)## Loading required package: XMLrequire(tm)## Loading required package: tmrequire(wordcloud)## Loading required package: wordcloud## Loading required package: Rcpp## Loading required package: RColorBrewerrequire(SnowballC)## Loading required package: SnowballCrequire(RColorBrewer)url = &quot;http://cran.r-project.org/web/packages/available_packages_by_date.html&quot;t = readHTMLTable(url)[[1]] 利用tm包对文档进行处理从网站可以发现表的第3列提供了包的描述信息，因此取第3列。依次去掉标点符号，转换为小写，去掉常用的the,a..等英文停止词。由于大部分包的描述都会包含data,analysis,model,function,package...所以这里把这些词也去掉 123456789package.corpus &lt;- Corpus(DataframeSource(data.frame(as.character(t[,3]))))package.corpus &lt;- tm_map(package.corpus, removePunctuation)package.corpus &lt;- tm_map(package.corpus, tolower)package.corpus &lt;- tm_map(package.corpus, removeNumbers)package.corpus &lt;- tm_map(package.corpus, removeWords, stopwords(&quot;english&quot;))package.corpus &lt;- tm_map(package.corpus, stemDocument)package.corpus &lt;- tm_map(package.corpus, removeWords, c(&quot;analysis&quot;,&quot;model&quot;,&quot;data&quot;,&quot;function&quot;,&quot;package&quot;,&quot;packag&quot;,&quot;method&quot;)) 转换为文档矩阵，并按照词的频率排序 12345package.tdm &lt;- TermDocumentMatrix(package.corpus)package.m &lt;- as.matrix(package.tdm)package.order &lt;- sort(rowSums(package.m),decreasing=TRUE)package.data &lt;- data.frame(word = names(package.order),freq=package.order)table(package.data$word,package.data$freq) 生成word cloud123456mypal &lt;- brewer.pal(8,&quot;Dark2&quot;)setwd(&quot;c:/baidu/rcourse/r-machine-training/img&quot;)png(&quot;packages_wordcloud.png&quot;, width=800,height=600)wordcloud(package.data$word,package.data$freq, scale=c(8,.2),min.freq=5,max.words=200, random.order=FALSE, rot.per=.15, colors=mypal)dev.off()]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[dplyr使用总结]]></title>
      <url>%2F2014%2F10%2F17%2F2014-10-17-dplyr%2F</url>
      <content type="text"><![CDATA[本文参考dplyr文档以及网上资料完成。 会apply,aggregate,也会plyr，为啥还要学dplyr?## 简单易学，通俗易懂，包教包会 速度与简洁的平衡：秒杀data frames操作以及plyr “链”式操作，写出清晰代码 ## 1234567891011121314151617181920require(plyr)require(dplyr)setwd(&quot;d:/project/datascience/rtraining/&quot;)set.seed(2014)nobs = 1e+06df = data.frame(group = as.factor( sample(1:1e+05, nobs, replace = TRUE)), variable = rpois(nobs, 100))print(object.size(df), units = &quot;MB&quot;)str(df)# 计算每组的均值#plyrsystem.time(grpmean&lt;- ddply(df, .(group), summarize, grpmean = mean(variable))) #26.79s# dplyrsystem.time(grpmean2&lt;-df %&gt;% group_by(group) %&gt;% summarize(grpmean = mean(variable))) # 0.27s dplyr基本功能## 5个词: filter, select, arrange, mutate, summarise (+group_by) 通吃数据库中数据以及data tables 支持各种数据表的join: inner join, left join, semi-join, anti-join Window 函数用来计算排名，offsets… 对比plyr更快，更简洁，更强 (也许还有些plyr的功能没有移植过来?) 开始之前先安装或者查看dplyr的版本 1234packageVersion(&quot;dplyr&quot;)packinfo = installed.packages (fields = c (&quot;Package&quot;, &quot;Version&quot;))packinfo[&quot;dplyr&quot;,c(&quot;Package&quot;, &quot;Version&quot;)]install.packages(&quot;dplyr&quot;) 好戏马上开演了加载包 12345library(dplyr)library(hflights)#explore datadata(hflights)head(hflights,3) hflights 是2011年从休斯敦机场起飞的航班 创建本地数据框tbl_df 创建一个”local data frame”，相当于一个wrapper。优势在于打印的时候显示更友好 12flights=tbl_df(hflights)flights filter : 行过滤 无需像普通data frame里面还需要写data frame变量名 简单易读 首参数为data frame 后面接着是条件 返回也是一个data frame 我想知道某月，某天的航班？12345678#原始flights[flights$Month==1 &amp; flights$DayofMonth==1,]#dplyrfilter(flights,Month==1,DayofMonth==1)filter(flights,UniqueCarrier==&quot;AA&quot;|UniqueCarrier==&quot;UA&quot;)filter(flights,UniqueCarrier %in% c(&quot;AA&quot;,&quot;UA&quot;)) select: 选择列 类似SQL的SELECT 查看离开，到达时间以及航班号 12345678flights[,c(&quot;DepTime&quot;,&quot;ArrTime&quot;,&quot;FlightNum&quot;)]# dplyrselect(flights,DepTime,ArrTime,FlightNum)# &quot;:&quot; 选择连续列，contains来匹配列名select(flights,Year:DayofMonth,contains(&quot;Taxi&quot;),contains(&quot;Delay&quot;))# `starts_with`, `ends_with`, `matches` 更精确匹配 链式操作(管道) 将多行语句嵌套写入一行，并提高可读性 %&gt;% (读成then) magrittr 包 Q: 所有延迟60分钟起飞的航空公司？ 12345filter(select(flights,UniqueCarrier,DepDelay),DepDelay&gt;60)flights %&gt;% select(UniqueCarrier,DepDelay)%&gt;% filter(DepDelay&gt;60) 链式操作(管道) 多个操作时，增加了代码的可读性 自动加载magrittr包提供支持 也可以在dplyr包之外使用 12345x1=1:5x2=2:6sqrt(sum((x1-x2)^2))(x1-x2)^2%&gt;%sum()%&gt;%sqrt() arrange: 排序Q: 按照起飞延迟对所有航空公司排序？ 123456789flights[order(flights$DepDelay),c(&quot;UniqueCarrier&quot;,&quot;DepDelay&quot;)]flights%&gt;% select(UniqueCarrier,DepDelay)%&gt;% arrange(DepDelay)flights%&gt;% select(UniqueCarrier,DepDelay)%&gt;% arrange(desc(DepDelay)) mutate: 添加新变量Q: 每个航班飞行速度是多少？ 123456789#?hflights可知AirTime是分钟数，所以这里需要换算成小时flights$Speed=flights$Distance/flights$AirTime*60flights[,c(&quot;Distance&quot;,&quot;AirTime&quot;,&quot;Speed&quot;)]flights%&gt;% select(Distance,AirTime)%&gt;% mutate(Speed=Distance/AirTime*60)flights=flights%&gt;%mutate(Speed=Distance/AirTime*60) summarise: 总结 对数据进行分组统计 首先用group_by 对数据分组 n(),n_distinct(vector),… Q: 根据目的地统计平均航班延迟 123456head(with(flights,tapply(ArrDelay,Dest,mean,na.rm=T)))head(aggregate(ArrDelay~Dest,flights,mean))flights%&gt;% group_by(Dest)%&gt;% summarise(avg_delay=mean(ArrDelay,na.rm=T)) summarise - 2 summarise_each 允许对多列同时进行统计 mutate_each 也可以 Q: 每个航空公司的平均航班取消与转飞比例Q: 每个航空公司最大/最小到达/离开延迟 1234567flights%&gt;% group_by(UniqueCarrier)%&gt;% summarise_each(funs(mean),Cancelled,Diverted)flights%&gt;% group_by(UniqueCarrier)%&gt;% summarise_each(funs(min(.,na.rm=T),max(.,na.rm=T)),matches(&quot;Delay&quot;)) summarise - 3Q: 每天总航班数目排序 n() 统计组中行数目 n_distinct(vector) 统计不同行数目 123456789flights %&gt;% group_by(Month,DayofMonth)%&gt;% summarise(flight_count=n())%&gt;% arrange(desc(flight_count))# 飞往每个目的地的总航班数与不同飞机编号(Tail Num)flights %&gt;% group_by(Dest) %&gt;% summarise(flight_count = n(), plane_count = n_distinct(TailNum)) tallytally 可以一步完成上述工作，第一次使用tally进行n()操作，再来一次就是sum(n) sort=TRUE 将对结果排序 123flights %&gt;% group_by(Month, DayofMonth) %&gt;% tally(sort = TRUE) 有时Grouping无需进行summarise也可以得到有意思的信息Q: 飞往每个目的地的取消航班与未取消航班对比 12345flights %&gt;% group_by(Dest) %&gt;% select(Cancelled) %&gt;% table() %&gt;% head() Join inner_join ：只包含同时出现在x,y表中的行 left_join: 包含所有x中以及y中匹配的行 semi_join：包含x中，在y中有匹配的行 anti_join： 包含x中不匹配y的行 让代码来说话1234x = data.frame(name = c(&quot;John&quot;, &quot;Paul&quot;, &quot;George&quot;, &quot;Ringo&quot;, &quot;Stuart&quot;, &quot;Pete&quot;),instrument = c(&quot;guitar&quot;, &quot;bass&quot;, &quot;guitar&quot;, &quot;drums&quot;, &quot;bass&quot;,&quot;drums&quot;))y = data.frame(name = c(&quot;John&quot;, &quot;Paul&quot;, &quot;George&quot;, &quot;Ringo&quot;, &quot;Brian&quot;),band = c(&quot;TRUE&quot;, &quot;TRUE&quot;, &quot;TRUE&quot;, &quot;TRUE&quot;, &quot;FALSE&quot;)) 12xy inner_join1inner_join(x, y) left_join1left_join(x, y) semi_join1semi_join(x, y) anti_join1anti_join(x, y) Window 函数 通常我们汇总是多个输入，一个输出（均值，和…) Window 函数是n个输入，返回n个值 ranking / ordering (min_rank,…), offset 函数 (lead / lag), cumulative aggregates ( cummean). windows function 统计每个航空公司出发延迟最长的2天12345678910111213# 出发延迟越大应该排在前面，所有用 `desc`flights %&gt;% group_by(UniqueCarrier) %&gt;% select(Month, DayofMonth, DepDelay) %&gt;% filter(min_rank(desc(DepDelay)) &lt;= 2) %&gt;% arrange(UniqueCarrier, desc(DepDelay))# `top_n` flights %&gt;% group_by(UniqueCarrier) %&gt;% select(Month, DayofMonth, DepDelay) %&gt;% top_n(2) %&gt;% arrange(UniqueCarrier, desc(DepDelay)) 每月相对上月的航班数变化12345678910flights %&gt;% group_by(Month) %&gt;% summarise(flight_count = n()) %&gt;% mutate(change = flight_count - lag(flight_count))# tallyflights %&gt;% group_by(Month) %&gt;% tally() %&gt;% mutate(change = n - lag(n)) 其它功能抽样，查看数据结构 12345# 随机选5行，不放回flights %&gt;% sample_n(5)# 按比例抽样，放回flights %&gt;% sample_frac(0.25, replace=TRUE) 1234# 查看数据结构str(flights)glimpse(flights) 连接数据库## dplyr 可以连接数据库 使用与本地数据框操作一样的语法 只支持生成SELECT语句 支持SQLite, PostgreSQL/Redshift, MySQL/MariaDB, BigQuery, MonetDB 数据库 先建立一个数据库123setwd(&quot;d:/project/datascience/rtraining/&quot;)my_db=src_sqlite(&quot;my_db.sqlite3&quot;,create=T)flights_sqlite=copy_to(my_db,hflights,temporary=F,indexes=list(c(&quot;Year&quot;,&quot;Month&quot;,&quot;DayofMonth&quot;),&quot;UniqueCarrier&quot;,&quot;Tailnum&quot;)) 查询1234567891011# 连接到SQLite数据库中的数据表my_db = src_sqlite(&quot;my_db.sqlite3&quot;)# 连接&quot;hflights&quot; 表flights_tbl = tbl(my_db, &quot;hflights&quot;)# 查询flights_tbl %&gt;% select(UniqueCarrier, DepDelay) %&gt;% arrange(desc(DepDelay))# 显示所有数据需要先collect() SQL命令123456789101112# 直接使用SQL命令。 sqldf包也可以干这个tbl(my_db, sql(&quot;SELECT * FROM hflights LIMIT 100&quot;))# 让dplyr返回执行的 SQL commandsflights_tbl %&gt;% select(UniqueCarrier, DepDelay) %&gt;% arrange(desc(DepDelay)) %&gt;% explain()hflights %&gt;% select(UniqueCarrier, DepDelay) %&gt;% arrange(desc(DepDelay)) Do## Do]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[R软件的升级]]></title>
      <url>%2F2014%2F10%2F12%2F2014-10-12-R-upgrade%2F</url>
      <content type="text"><![CDATA[R的升级一直是初学者的一个问题，因为R的安装包做的不怎么智能，每次安装新的R版本它不会覆盖老版本，因此之前装过的包在新版本上无法使用，许多新手只能又重复劳动一遍。 一个常用的解决这个问题的方法就是在你的.REnviron文件中指定R_LIBS的值。例如 R_LIBS=&quot;C:\my\pathto\library&quot; .REnviron文件保存在$HOME$目录下面，而要知道你的$HOME$在哪里，运行下面的命令 Sys.getenv(&quot;HOME&quot;) 上面的设置好了以后运行： update.packages( lib.loc = .libPaths()[1] , checkBuilt = TRUE ) 那么就可以对以前安装的包进行升级。 当然如果你老版本的包都安装在系统缺省目录中，那么先用 Sys.getenv(&quot;R_LIBS_USER&quot;)Sys.getenv(&quot;R_LIBS&quot;).libPaths() 命令找到之前的包的安装位置，之后把那里的所有包都copy到现在你指定的R_LIBS目录再执行上面的操作。 补充：除了上面的办法，还有一种就是一开始就在系统设置R_LIBS路径（windows，linux怎么设置环境变量就不用说了吧），以后安装包就会装到R_LIBS指定目录。升级R软件就和包是独立的了。 如果你有更好的办法请告诉我哦。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[R Your Ready? 100分钟R入门教程(上)]]></title>
      <url>%2F2014%2F10%2F11%2F2014-10-11-R-crash-course-1%2F</url>
      <content type="text"><![CDATA[内容 安装 R使用初步 vector,list,data frame read data packages R编程 table (选讲) t检验(选讲) 安装下载Windows下分别安装： R Rstudio Linux下安装: sudo apt-get install r-base R 初体验输入命令 R命令大小写敏感 R的命令实际就是一个个的函数 function.name( arguments, options ) quit() #退出R rm( myData ) #删除变量myData #代表注释 简单数据录入简单的小数据集通常可以用 c() 函数输入来获得一个数据向量 12my.data = c( 22, 38, 12, 23, 29, 18, 16, 24 )my.data # 隐式调用print(my.data) 1## [1] 22 38 12 23 29 18 16 24 修改向量中的值my.data[i]，注意R里面的索引是从1开始 12my.data[3] = 19my.data 1## [1] 22 38 19 23 29 18 16 24 理解R控制台里面的输出R不仅打印变量的值，同时还会输出索引.例如系统数据rivers给出了北美141条主要河流的长度（miles） 1rivers 1234567891011## [1] 735 320 325 392 524 450 1459 135 465 600 330 336 280 315## [15] 870 906 202 329 290 1000 600 505 1450 840 1243 890 350 407## [29] 286 280 525 720 390 250 327 230 265 850 210 630 260 230## [43] 360 730 600 306 390 420 291 710 340 217 281 352 259 250## [57] 470 680 570 350 300 560 900 625 332 2348 1171 3710 2315 2533## [71] 780 280 410 460 260 255 431 350 760 618 338 981 1306 500## [85] 696 605 250 411 1054 735 233 435 490 310 460 383 375 1270## [99] 545 445 1885 380 300 380 377 425 276 210 800 420 350 360## [113] 538 1100 1205 314 237 610 360 540 1038 424 310 300 444 301## [127] 268 620 215 652 900 525 246 360 529 500 720 270 430 671## [141] 1770 除了用以下方式访问 1rivers[121] 1## [1] 1038 还可以用类似python里面的slice的方式访问 1rivers[ 10:20 ] 1## [1] 600 330 336 280 315 870 906 202 329 290 1000 1rivers[1:5] 1## [1] 735 320 325 392 524 建立一个频数表假设我们想输入这样的一个数据： X f ----------- 7 3 6 0 5 2 4 7 3 5 2 1 1 1 1234X = 7:1 # X = c( 7, 6, 5, 4, 3, 2, 1 )f = c( 3, 0, 2, 7, 5, 1, 1 )the.data = rep( X, f ) # rep() == repeat functionthe.data 1## [1] 7 7 7 5 5 4 4 4 4 4 4 4 3 3 3 3 3 2 1 对频数表进行统计1table( the.data ) 123## the.data## 1 2 3 4 5 7 ## 1 1 5 7 2 3 1the.data 1## [1] 7 7 7 5 5 4 4 4 4 4 4 4 3 3 3 3 3 2 1 这里的输出第一行是代表了X列，而第二行则代表了f列。 对数字变量分组12my.data=c(1:10,15:19,24:30)my.data 1## [1] 1 2 3 4 5 6 7 8 9 10 15 16 17 18 19 24 25 26 27 28 29 30 123bins = seq( from=10, to=30, by=5)# bins = c( 10, 15, 20, 25, 30 ) table( cut( my.data, bins )) 123## ## (10,15] (15,20] (20,25] (25,30] ## 1 4 2 5 cut缺省不包含左侧，包含右侧 123bins = c(10, 15, 20, 25, 30)temp.table = cut( my.data, bins, right=FALSE )temp.table 1234## [1] &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## [9] &lt;NA&gt; [10,15) [15,20) [15,20) [15,20) [15,20) [15,20) [20,25)## [17] [25,30) [25,30) [25,30) [25,30) [25,30) &lt;NA&gt; ## Levels: [10,15) [15,20) [20,25) [25,30) 1table(temp.table) 123## temp.table## [10,15) [15,20) [20,25) [25,30) ## 1 5 1 5 一维向量的茎叶图表示1stem( my.data ) 1234567## ## The decimal point is 1 digit(s) to the right of the |## ## 0 | 123456789## 1 | 056789## 2 | 456789## 3 | 0 直方图1hist( the.data ) 1the.data 1## [1] 7 7 7 5 5 4 4 4 4 4 4 4 3 3 3 3 3 2 1 R的直方图缺省是以包含右侧为界，所以这里1，2是归入1～2 1hist( the.data, right=F ) 这个更清楚： 12bins = seq( .5, 7.5, 1 ) hist( the.data, breaks=bins ) Categorical (nominal) 数据.1234colors = c( "red", "black", "brown", "blonde" )freqs = c( 8 , 22 , 30 , 18 ) hair.color = rep( colors, freqs )table( hair.color ) 123## hair.color## black blonde brown red ## 22 18 30 8 1hair.color 123456789101112## [1] &quot;red&quot; &quot;red&quot; &quot;red&quot; &quot;red&quot; &quot;red&quot; &quot;red&quot; &quot;red&quot; ## [8] &quot;red&quot; &quot;black&quot; &quot;black&quot; &quot;black&quot; &quot;black&quot; &quot;black&quot; &quot;black&quot; ## [15] &quot;black&quot; &quot;black&quot; &quot;black&quot; &quot;black&quot; &quot;black&quot; &quot;black&quot; &quot;black&quot; ## [22] &quot;black&quot; &quot;black&quot; &quot;black&quot; &quot;black&quot; &quot;black&quot; &quot;black&quot; &quot;black&quot; ## [29] &quot;black&quot; &quot;black&quot; &quot;brown&quot; &quot;brown&quot; &quot;brown&quot; &quot;brown&quot; &quot;brown&quot; ## [36] &quot;brown&quot; &quot;brown&quot; &quot;brown&quot; &quot;brown&quot; &quot;brown&quot; &quot;brown&quot; &quot;brown&quot; ## [43] &quot;brown&quot; &quot;brown&quot; &quot;brown&quot; &quot;brown&quot; &quot;brown&quot; &quot;brown&quot; &quot;brown&quot; ## [50] &quot;brown&quot; &quot;brown&quot; &quot;brown&quot; &quot;brown&quot; &quot;brown&quot; &quot;brown&quot; &quot;brown&quot; ## [57] &quot;brown&quot; &quot;brown&quot; &quot;brown&quot; &quot;brown&quot; &quot;blonde&quot; &quot;blonde&quot; &quot;blonde&quot;## [64] &quot;blonde&quot; &quot;blonde&quot; &quot;blonde&quot; &quot;blonde&quot; &quot;blonde&quot; &quot;blonde&quot; &quot;blonde&quot;## [71] &quot;blonde&quot; &quot;blonde&quot; &quot;blonde&quot; &quot;blonde&quot; &quot;blonde&quot; &quot;blonde&quot; &quot;blonde&quot;## [78] &quot;blonde&quot; 1barplot( table( hair.color )) #一个函数的输出是另一个函数的输入 1barplot( table( hair.color ), col=c( "black","yellow","brown","red" )) 这里用到了一个函数的输出是另一个函数的输入 12temp.table = table( hair.color )barplot( temp.table ) 1rm( temp.table) 计算1sum( the.data ) 1## [1] 77 1sum( the.data ) ^ 2 1## [1] 5929 1sum( the.data ^ 2 ) 1## [1] 359 1the.data ^ 2 1## [1] 49 49 49 25 25 16 16 16 16 16 16 16 9 9 9 9 9 4 1 123x = c( 2, 3, 5 )y = c( 3, 8, 9 )sum( x * y ) # product of x,y 1## [1] 75 1sum(x) * sum(y) 1## [1] 200 数学函数12345sqrt()log() log10() sin() abs() 描述性统计1length( the.data ) 1## [1] 19 1length( hair.color ) 1## [1] 78 1median( the.data ) 1## [1] 4 1mean( the.data ) 1## [1] 4.053 1range( the.data ) # 区间 1## [1] 1 7 1IQR( the.data ) #R有9中计算Q1,Q3的方法,sigh 1## [1] 1.5 方差 1var( the.data ) 1## [1] 2.608 标准差 1sd( the.data ) 1## [1] 1.615 使用summary 1summary( the.data ) 12## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.00 3.00 4.00 4.05 4.50 7.00 不同的方法计算Q1，Q3 1quantile( the.data, c( .25,.75 ), type=2 ) # 教科书的方法 12## 25% 75% ## 3 5 是自己寻求答案的时候了-help1help( t.test ) # ?t.test 1## starting httpd help server ... done 1help.search( "distribution" ) # 搜索 vector,data frame 与 list,factorvector 取子集12345678910111213x=seq(2,12)x[]x[0]x[2]x[-2]x[2:4]x[-2:-5]names(x)#length(x)==11names(x)=LETTERS[1:11]names(x)x[c("A","B")]x[c(T,F)] vector操作123456789x=1:10x*3x+3x/3x^2sqrt(x)10:1-1:105:-7 vector 操作123456x=1:10y=1:5x+yx-yx*yx/y vector 操作123x=1:10y=1:3x+y 1## Warning: longer object length is not a multiple of shorter object length 12x&lt;5x&lt;y 1## Warning: longer object length is not a multiple of shorter object length 1any(x&lt;y) #? 1## Warning: longer object length is not a multiple of shorter object length 1all(x&lt;y) 1## Warning: longer object length is not a multiple of shorter object length 123q = c("Hockey", "Football", "Baseball", "Curling", "Rugby", "Lacrosse", "Basketball", "Tennis", "Cricket", "Soccer")nchar(q) Factorfactor是R中的一类特殊类型 12myfac=factor(c("basic","proficient","advanced","minimal"))class(myfac) 1## [1] &quot;factor&quot; 1myfac 12## [1] basic proficient advanced minimal ## Levels: advanced basic minimal proficient 如果我们想改变factor顺序怎么办？ Factor排序12myfac_o=ordered(myfac,levels=c("minimal","basic","proficient","advanced")) #?orderedmyfac_o 12## [1] basic proficient advanced minimal ## Levels: minimal &lt; basic &lt; proficient &lt; advanced 1summary(myfac_o) #?summary 12## minimal basic proficient advanced ## 1 1 1 1 将Factor转换为其他类型1as.character(myfac_o) 1## [1] &quot;basic&quot; &quot;proficient&quot; &quot;advanced&quot; &quot;minimal&quot; data framedata.frame函数创建数据框 123456x = 10:1y = -4:5q = c("Hockey", "Football", "Baseball", "Curling", "Rugby", "Lacrosse", "Basketball", "Tennis", "Cricket", "Soccer")theDF = data.frame(x, y, q)theDF 如果我想每一列的名字为预先我设定的值，而不是x,y,q，应该怎么办？ data frame 常用操作 行数，列数 每列的数据类型 数据框的维度 每列，每行的名称 123456789101112131415theDF = data.frame(First = x, Second = y, Sport = q)theDFnrow(theDF)ncol(theDF)str(theDF)dim(theDF)names(theDF)names(theDF)[1]names(theDF)=c("First","Second","Sport")rownames(theDF)rownames(theDF) = c("One", "Two", "Three", "Four", "Five", "Six", "Seven", "Eight", "Nine", "Ten")rownames(theDF)theDFrownames(theDF)=NULL 查看data frame123head(theDF,10)tail(theDF)?head data frame操作1234567891011theDF$SporttheDF[2,1]theDF[3,2:3]theDF[c(3, 5), 2]theDF[c(3, 5), 2:3]theDF[, 3]theDF[, 2:3]theDF[2, ]theDF[, c("First", "Sport")]#subset(theDF,theDF$Sport=="Hockey")?subset data frame操作1234567theDF["Sport"]class(theDF["Sport"])theDF[["Sport"]]class(theDF[["Sport"]])#[]与[[]]区别？theDF[, "Sport", drop = FALSE]class(theDF[, "Sport", drop = FALSE]) ## 123myData = matrix( c( 2, 5, 3, 4, 6, 7, 3, 2, 18, 22, 17, 10 ), ncol=3 )myData = as.data.frame( myData )myData 12345## V1 V2 V3## 1 2 6 18## 2 5 7 22## 3 3 3 17## 4 4 2 10 123colnames( myData ) = c( "Quiz1", "Quiz2", "Exam" )rownames( myData ) = c( "Jeff", "Barbara", "Rachel", "Bob" )myData 12345## Quiz1 Quiz2 Exam## Jeff 2 6 18## Barbara 5 7 22## Rachel 3 3 17## Bob 4 2 10 12rownames( myData )[4] = "Robb"myData 12345## Quiz1 Quiz2 Exam## Jeff 2 6 18## Barbara 5 7 22## Rachel 3 3 17## Robb 4 2 10 12myData[4] = c( 24, 28, 30, 20 )myData 12345## Quiz1 Quiz2 Exam V4## Jeff 2 6 18 24## Barbara 5 7 22 28## Rachel 3 3 17 30## Robb 4 2 10 20 12colnames( myData )[4] = "Paper"myData 12345## Quiz1 Quiz2 Exam Paper## Jeff 2 6 18 24## Barbara 5 7 22 28## Rachel 3 3 17 30## Robb 4 2 10 20 12myData$Major = c( "Psych", "Biology", "Math", "Business" ) # $myData 12345## Quiz1 Quiz2 Exam Paper Major## Jeff 2 6 18 24 Psych## Barbara 5 7 22 28 Biology## Rachel 3 3 17 30 Math## Robb 4 2 10 20 Business 1myData$Exam 1## [1] 18 22 17 10 12rm(myData) ls() 123## [1] &quot;bins&quot; &quot;colors&quot; &quot;f&quot; &quot;freqs&quot; &quot;hair.color&quot;## [6] &quot;my.data&quot; &quot;myfac&quot; &quot;myfac_o&quot; &quot;q&quot; &quot;the.data&quot; ## [11] &quot;theDF&quot; &quot;x&quot; &quot;X&quot; &quot;y&quot; list介绍vector里面每个变量的类型都是一样，data frame每列变量类型都一样，同时每列的长度一致。而list则可以长度不一致 Hadley Wickham’s Data structures 1234list(1,2,3)list(c(1,2,3))mylist1 = list(c(1, 2, 3), 3:7)mylist1 list操作12345678list(theDF,1:10)mylist2 = list(theDF, 1:10, mylist1)names(mylist2)names(mylist2) = c("data.frame", "vector", "list")names(mylist2)mylist2mylist3 = list(TheDataFrame = theDF, TheVector = 1:10, TheList = mylist2)names(mylist3) list操作1234mylist3[[1]]mylist3[["TheDataFrame"]]mylist3[[1]]$Sportlength(mylist3) 总结我们已经学习了 vector,list,data frame 列出工作空间中的变量: ls() 了解data frame的维度: dim() 列名称: colnames(), names(), dimnames() 行: rownames() , dimnames() 查看前几行: head()/tail() data frame的变量名称 利用索引取子集dataframe.name[ row.number, col.number ] 利用行名，列名取子集: dataframe.name[ “row.name”, “col.name” ] 修改列名: colnames() = … 修改行名: rownames() = … 添加一列到data frame: 2种方法，索引以及直接使用变量名 $ 练习1USArrests[ "Iowa", "UrbanPop" ] 1## [1] 57 12USArrests[ "Iowa", "UrbanPop" ] = 33USArrests[ "Iowa", "UrbanPop" ] 1## [1] 33 12USArrests[ "Iowa", "UrbanPop" ] = 57USArrests[ "Iowa", "UrbanPop" ] 1## [1] 57 1USArrests[ "Pennsylvania", ] # 注意那个, 12## Murder Assault UrbanPop Rape## Pennsylvania 6.3 106 72 14.9 1USArrests[ 38, ] 12## Murder Assault UrbanPop Rape## Pennsylvania 6.3 106 72 14.9 123#如果是访问某列USArrests[ "UrbanPop" ] 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051## UrbanPop## Alabama 58## Alaska 48## Arizona 80## Arkansas 50## California 91## Colorado 78## Connecticut 77## Delaware 72## Florida 80## Georgia 60## Hawaii 83## Idaho 54## Illinois 83## Indiana 65## Iowa 57## Kansas 66## Kentucky 52## Louisiana 66## Maine 51## Maryland 67## Massachusetts 85## Michigan 74## Minnesota 66## Mississippi 44## Missouri 70## Montana 53## Nebraska 62## Nevada 81## New Hampshire 56## New Jersey 89## New Mexico 70## New York 86## North Carolina 45## North Dakota 44## Ohio 75## Oklahoma 68## Oregon 67## Pennsylvania 72## Rhode Island 87## South Carolina 48## South Dakota 45## Tennessee 59## Texas 80## Utah 80## Vermont 32## Virginia 63## Washington 73## West Virginia 39## Wisconsin 66## Wyoming 60 1USArrests[3] 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051## UrbanPop## Alabama 58## Alaska 48## Arizona 80## Arkansas 50## California 91## Colorado 78## Connecticut 77## Delaware 72## Florida 80## Georgia 60## Hawaii 83## Idaho 54## Illinois 83## Indiana 65## Iowa 57## Kansas 66## Kentucky 52## Louisiana 66## Maine 51## Maryland 67## Massachusetts 85## Michigan 74## Minnesota 66## Mississippi 44## Missouri 70## Montana 53## Nebraska 62## Nevada 81## New Hampshire 56## New Jersey 89## New Mexico 70## New York 86## North Carolina 45## North Dakota 44## Ohio 75## Oklahoma 68## Oregon 67## Pennsylvania 72## Rhode Island 87## South Carolina 48## South Dakota 45## Tennessee 59## Texas 80## Utah 80## Vermont 32## Virginia 63## Washington 73## West Virginia 39## Wisconsin 66## Wyoming 60 1USArrests[ ,"UrbanPop" ] 123## [1] 58 48 80 50 91 78 77 72 80 60 83 54 83 65 57 66 52 66 51 67 85 74 66## [24] 44 70 53 62 81 56 89 70 86 45 44 75 68 67 72 87 48 45 59 80 80 32 63## [47] 73 39 66 60 1USArrests[ ,3] 123## [1] 58 48 80 50 91 78 77 72 80 60 83 54 83 65 57 66 52 66 51 67 85 74 66## [24] 44 70 53 62 81 56 89 70 86 45 44 75 68 67 72 87 48 45 59 80 80 32 63## [47] 73 39 66 60 12345# 这里需要注意，如果在取列是没有用那个`,`，那么结果是一个data frame，如果用了那么结果就是一个向量# 区别在这里USArrests[ "Murder" ] 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051## Murder## Alabama 13.2## Alaska 10.0## Arizona 8.1## Arkansas 8.8## California 9.0## Colorado 7.9## Connecticut 3.3## Delaware 5.9## Florida 15.4## Georgia 17.4## Hawaii 5.3## Idaho 2.6## Illinois 10.4## Indiana 7.2## Iowa 2.2## Kansas 6.0## Kentucky 9.7## Louisiana 15.4## Maine 2.1## Maryland 11.3## Massachusetts 4.4## Michigan 12.1## Minnesota 2.7## Mississippi 16.1## Missouri 9.0## Montana 6.0## Nebraska 4.3## Nevada 12.2## New Hampshire 2.1## New Jersey 7.4## New Mexico 11.4## New York 11.1## North Carolina 13.0## North Dakota 0.8## Ohio 7.3## Oklahoma 6.6## Oregon 4.9## Pennsylvania 6.3## Rhode Island 3.4## South Carolina 14.4## South Dakota 3.8## Tennessee 13.2## Texas 12.7## Utah 3.2## Vermont 2.2## Virginia 8.5## Washington 4.0## West Virginia 5.7## Wisconsin 2.6## Wyoming 6.8 1USArrests[ ,"Murder" ] 1234## [1] 13.2 10.0 8.1 8.8 9.0 7.9 3.3 5.9 15.4 17.4 5.3 2.6 10.4 7.2## [15] 2.2 6.0 9.7 15.4 2.1 11.3 4.4 12.1 2.7 16.1 9.0 6.0 4.3 12.2## [29] 2.1 7.4 11.4 11.1 13.0 0.8 7.3 6.6 4.9 6.3 3.4 14.4 3.8 13.2## [43] 12.7 3.2 2.2 8.5 4.0 5.7 2.6 6.8 1USArrests[1] 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051## Murder## Alabama 13.2## Alaska 10.0## Arizona 8.1## Arkansas 8.8## California 9.0## Colorado 7.9## Connecticut 3.3## Delaware 5.9## Florida 15.4## Georgia 17.4## Hawaii 5.3## Idaho 2.6## Illinois 10.4## Indiana 7.2## Iowa 2.2## Kansas 6.0## Kentucky 9.7## Louisiana 15.4## Maine 2.1## Maryland 11.3## Massachusetts 4.4## Michigan 12.1## Minnesota 2.7## Mississippi 16.1## Missouri 9.0## Montana 6.0## Nebraska 4.3## Nevada 12.2## New Hampshire 2.1## New Jersey 7.4## New Mexico 11.4## New York 11.1## North Carolina 13.0## North Dakota 0.8## Ohio 7.3## Oklahoma 6.6## Oregon 4.9## Pennsylvania 6.3## Rhode Island 3.4## South Carolina 14.4## South Dakota 3.8## Tennessee 13.2## Texas 12.7## Utah 3.2## Vermont 2.2## Virginia 8.5## Washington 4.0## West Virginia 5.7## Wisconsin 2.6## Wyoming 6.8 1USArrests[ ,1] 1234## [1] 13.2 10.0 8.1 8.8 9.0 7.9 3.3 5.9 15.4 17.4 5.3 2.6 10.4 7.2## [15] 2.2 6.0 9.7 15.4 2.1 11.3 4.4 12.1 2.7 16.1 9.0 6.0 4.3 12.2## [29] 2.1 7.4 11.4 11.1 13.0 0.8 7.3 6.6 4.9 6.3 3.4 14.4 3.8 13.2## [43] 12.7 3.2 2.2 8.5 4.0 5.7 2.6 6.8 123# 取10：20行USArrests[ 10:20, ] 123456789101112## Murder Assault UrbanPop Rape## Georgia 17.4 211 60 25.8## Hawaii 5.3 46 83 20.2## Idaho 2.6 120 54 14.2## Illinois 10.4 249 83 24.0## Indiana 7.2 113 65 21.0## Iowa 2.2 56 57 11.3## Kansas 6.0 115 66 18.0## Kentucky 9.7 109 52 16.3## Louisiana 15.4 249 66 22.2## Maine 2.1 83 51 7.8## Maryland 11.3 300 67 27.8 1USArrests[ c(10,11,12,13,14,15,16,17,18,19,20), ] 123456789101112## Murder Assault UrbanPop Rape## Georgia 17.4 211 60 25.8## Hawaii 5.3 46 83 20.2## Idaho 2.6 120 54 14.2## Illinois 10.4 249 83 24.0## Indiana 7.2 113 65 21.0## Iowa 2.2 56 57 11.3## Kansas 6.0 115 66 18.0## Kentucky 9.7 109 52 16.3## Louisiana 15.4 249 66 22.2## Maine 2.1 83 51 7.8## Maryland 11.3 300 67 27.8 1USArrests[ seq( from = 10, to = 20, by = 1 ), ] 123456789101112## Murder Assault UrbanPop Rape## Georgia 17.4 211 60 25.8## Hawaii 5.3 46 83 20.2## Idaho 2.6 120 54 14.2## Illinois 10.4 249 83 24.0## Indiana 7.2 113 65 21.0## Iowa 2.2 56 57 11.3## Kansas 6.0 115 66 18.0## Kentucky 9.7 109 52 16.3## Louisiana 15.4 249 66 22.2## Maine 2.1 83 51 7.8## Maryland 11.3 300 67 27.8 123# 取murder rate&gt;12的行USArrests[ Murder &gt; 12, ] # 怎么不对啊? 1## Error: object &apos;Murder&apos; not found 1USArrests[ "Murder" &gt; 12, ] 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051## Murder Assault UrbanPop Rape## Alabama 13.2 236 58 21.2## Alaska 10.0 263 48 44.5## Arizona 8.1 294 80 31.0## Arkansas 8.8 190 50 19.5## California 9.0 276 91 40.6## Colorado 7.9 204 78 38.7## Connecticut 3.3 110 77 11.1## Delaware 5.9 238 72 15.8## Florida 15.4 335 80 31.9## Georgia 17.4 211 60 25.8## Hawaii 5.3 46 83 20.2## Idaho 2.6 120 54 14.2## Illinois 10.4 249 83 24.0## Indiana 7.2 113 65 21.0## Iowa 2.2 56 57 11.3## Kansas 6.0 115 66 18.0## Kentucky 9.7 109 52 16.3## Louisiana 15.4 249 66 22.2## Maine 2.1 83 51 7.8## Maryland 11.3 300 67 27.8## Massachusetts 4.4 149 85 16.3## Michigan 12.1 255 74 35.1## Minnesota 2.7 72 66 14.9## Mississippi 16.1 259 44 17.1## Missouri 9.0 178 70 28.2## Montana 6.0 109 53 16.4## Nebraska 4.3 102 62 16.5## Nevada 12.2 252 81 46.0## New Hampshire 2.1 57 56 9.5## New Jersey 7.4 159 89 18.8## New Mexico 11.4 285 70 32.1## New York 11.1 254 86 26.1## North Carolina 13.0 337 45 16.1## North Dakota 0.8 45 44 7.3## Ohio 7.3 120 75 21.4## Oklahoma 6.6 151 68 20.0## Oregon 4.9 159 67 29.3## Pennsylvania 6.3 106 72 14.9## Rhode Island 3.4 174 87 8.3## South Carolina 14.4 279 48 22.5## South Dakota 3.8 86 45 12.8## Tennessee 13.2 188 59 26.9## Texas 12.7 201 80 25.5## Utah 3.2 120 80 22.9## Vermont 2.2 48 32 11.2## Virginia 8.5 156 63 20.7## Washington 4.0 145 73 26.2## West Virginia 5.7 81 39 9.3## Wisconsin 2.6 53 66 10.8## Wyoming 6.8 161 60 15.6 1## subset 1subset( USArrests, select = UrbanPop ) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051## UrbanPop## Alabama 58## Alaska 48## Arizona 80## Arkansas 50## California 91## Colorado 78## Connecticut 77## Delaware 72## Florida 80## Georgia 60## Hawaii 83## Idaho 54## Illinois 83## Indiana 65## Iowa 57## Kansas 66## Kentucky 52## Louisiana 66## Maine 51## Maryland 67## Massachusetts 85## Michigan 74## Minnesota 66## Mississippi 44## Missouri 70## Montana 53## Nebraska 62## Nevada 81## New Hampshire 56## New Jersey 89## New Mexico 70## New York 86## North Carolina 45## North Dakota 44## Ohio 75## Oklahoma 68## Oregon 67## Pennsylvania 72## Rhode Island 87## South Carolina 48## South Dakota 45## Tennessee 59## Texas 80## Utah 80## Vermont 32## Virginia 63## Washington 73## West Virginia 39## Wisconsin 66## Wyoming 60 1subset( USArrests, subset = Murder &gt; 12 ) 123456789101112## Murder Assault UrbanPop Rape## Alabama 13.2 236 58 21.2## Florida 15.4 335 80 31.9## Georgia 17.4 211 60 25.8## Louisiana 15.4 249 66 22.2## Michigan 12.1 255 74 35.1## Mississippi 16.1 259 44 17.1## Nevada 12.2 252 81 46.0## North Carolina 13.0 337 45 16.1## South Carolina 14.4 279 48 22.5## Tennessee 13.2 188 59 26.9## Texas 12.7 201 80 25.5 1subset( USArrests, subset = Murder &gt; 12 &amp; Assault &gt;= 255 ) 123456## Murder Assault UrbanPop Rape## Florida 15.4 335 80 31.9## Michigan 12.1 255 74 35.1## Mississippi 16.1 259 44 17.1## North Carolina 13.0 337 45 16.1## South Carolina 14.4 279 48 22.5 1subset( USArrests, subset = Murder &gt; 12 &amp; Assault &gt;= 255, select = Rape ) 123456## Rape## Florida 31.9## Michigan 35.1## Mississippi 17.1## North Carolina 16.1## South Carolina 22.5 sort12attach(USArrests)sort( Murder ) 1234## [1] 0.8 2.1 2.1 2.2 2.2 2.6 2.6 2.7 3.2 3.3 3.4 3.8 4.0 4.3## [15] 4.4 4.9 5.3 5.7 5.9 6.0 6.0 6.3 6.6 6.8 7.2 7.3 7.4 7.9## [29] 8.1 8.5 8.8 9.0 9.0 9.7 10.0 10.4 11.1 11.3 11.4 12.1 12.2 12.7## [43] 13.0 13.2 13.2 14.4 15.4 15.4 16.1 17.4 1sort( Murder, decreasing = TRUE ) 1234## [1] 17.4 16.1 15.4 15.4 14.4 13.2 13.2 13.0 12.7 12.2 12.1 11.4 11.3 11.1## [15] 10.4 10.0 9.7 9.0 9.0 8.8 8.5 8.1 7.9 7.4 7.3 7.2 6.8 6.6## [29] 6.3 6.0 6.0 5.9 5.7 5.3 4.9 4.4 4.3 4.0 3.8 3.4 3.3 3.2## [43] 2.7 2.6 2.6 2.2 2.2 2.1 2.1 0.8 不过如果我想data frame里面的数据根据某列来排序呢？ 12SO = order( Murder )SO 123## [1] 34 19 29 15 45 12 49 23 44 7 39 41 47 27 21 37 11 48 8 16 26 38 36## [24] 50 14 35 30 6 3 46 4 5 25 17 2 13 32 20 31 22 28 43 33 1 42 40## [47] 9 18 24 10 1head(USArrests[ SO, ]) 1234567## Murder Assault UrbanPop Rape## North Dakota 0.8 45 44 7.3## Maine 2.1 83 51 7.8## New Hampshire 2.1 57 56 9.5## Iowa 2.2 56 57 11.3## Vermont 2.2 48 32 11.2## Idaho 2.6 120 54 14.2 1USArrests[34,] 12## Murder Assault UrbanPop Rape## North Dakota 0.8 45 44 7.3 SO 成为了我们用了排序的dummy variable 1head(USArrests[ order( Murder ), ]) 1234567## Murder Assault UrbanPop Rape## North Dakota 0.8 45 44 7.3## Maine 2.1 83 51 7.8## New Hampshire 2.1 57 56 9.5## Iowa 2.2 56 57 11.3## Vermont 2.2 48 32 11.2## Idaho 2.6 120 54 14.2 12# 注意我们没有改变USArrestsUSArrests 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051## Murder Assault UrbanPop Rape## Alabama 13.2 236 58 21.2## Alaska 10.0 263 48 44.5## Arizona 8.1 294 80 31.0## Arkansas 8.8 190 50 19.5## California 9.0 276 91 40.6## Colorado 7.9 204 78 38.7## Connecticut 3.3 110 77 11.1## Delaware 5.9 238 72 15.8## Florida 15.4 335 80 31.9## Georgia 17.4 211 60 25.8## Hawaii 5.3 46 83 20.2## Idaho 2.6 120 54 14.2## Illinois 10.4 249 83 24.0## Indiana 7.2 113 65 21.0## Iowa 2.2 56 57 11.3## Kansas 6.0 115 66 18.0## Kentucky 9.7 109 52 16.3## Louisiana 15.4 249 66 22.2## Maine 2.1 83 51 7.8## Maryland 11.3 300 67 27.8## Massachusetts 4.4 149 85 16.3## Michigan 12.1 255 74 35.1## Minnesota 2.7 72 66 14.9## Mississippi 16.1 259 44 17.1## Missouri 9.0 178 70 28.2## Montana 6.0 109 53 16.4## Nebraska 4.3 102 62 16.5## Nevada 12.2 252 81 46.0## New Hampshire 2.1 57 56 9.5## New Jersey 7.4 159 89 18.8## New Mexico 11.4 285 70 32.1## New York 11.1 254 86 26.1## North Carolina 13.0 337 45 16.1## North Dakota 0.8 45 44 7.3## Ohio 7.3 120 75 21.4## Oklahoma 6.6 151 68 20.0## Oregon 4.9 159 67 29.3## Pennsylvania 6.3 106 72 14.9## Rhode Island 3.4 174 87 8.3## South Carolina 14.4 279 48 22.5## South Dakota 3.8 86 45 12.8## Tennessee 13.2 188 59 26.9## Texas 12.7 201 80 25.5## Utah 3.2 120 80 22.9## Vermont 2.2 48 32 11.2## Virginia 8.5 156 63 20.7## Washington 4.0 145 73 26.2## West Virginia 5.7 81 39 9.3## Wisconsin 2.6 53 66 10.8## Wyoming 6.8 161 60 15.6 12USArrests.sorted = USArrests[ SO, ]USArrests.sorted 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051## Murder Assault UrbanPop Rape## North Dakota 0.8 45 44 7.3## Maine 2.1 83 51 7.8## New Hampshire 2.1 57 56 9.5## Iowa 2.2 56 57 11.3## Vermont 2.2 48 32 11.2## Idaho 2.6 120 54 14.2## Wisconsin 2.6 53 66 10.8## Minnesota 2.7 72 66 14.9## Utah 3.2 120 80 22.9## Connecticut 3.3 110 77 11.1## Rhode Island 3.4 174 87 8.3## South Dakota 3.8 86 45 12.8## Washington 4.0 145 73 26.2## Nebraska 4.3 102 62 16.5## Massachusetts 4.4 149 85 16.3## Oregon 4.9 159 67 29.3## Hawaii 5.3 46 83 20.2## West Virginia 5.7 81 39 9.3## Delaware 5.9 238 72 15.8## Kansas 6.0 115 66 18.0## Montana 6.0 109 53 16.4## Pennsylvania 6.3 106 72 14.9## Oklahoma 6.6 151 68 20.0## Wyoming 6.8 161 60 15.6## Indiana 7.2 113 65 21.0## Ohio 7.3 120 75 21.4## New Jersey 7.4 159 89 18.8## Colorado 7.9 204 78 38.7## Arizona 8.1 294 80 31.0## Virginia 8.5 156 63 20.7## Arkansas 8.8 190 50 19.5## California 9.0 276 91 40.6## Missouri 9.0 178 70 28.2## Kentucky 9.7 109 52 16.3## Alaska 10.0 263 48 44.5## Illinois 10.4 249 83 24.0## New York 11.1 254 86 26.1## Maryland 11.3 300 67 27.8## New Mexico 11.4 285 70 32.1## Michigan 12.1 255 74 35.1## Nevada 12.2 252 81 46.0## Texas 12.7 201 80 25.5## North Carolina 13.0 337 45 16.1## Alabama 13.2 236 58 21.2## Tennessee 13.2 188 59 26.9## South Carolina 14.4 279 48 22.5## Florida 15.4 335 80 31.9## Louisiana 15.4 249 66 22.2## Mississippi 16.1 259 44 17.1## Georgia 17.4 211 60 25.8 123456rm( USArrests.sorted )# 从高到低排序SO = order( Murder, decreasing = TRUE )USArrests[ SO, ] 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051## Murder Assault UrbanPop Rape## Georgia 17.4 211 60 25.8## Mississippi 16.1 259 44 17.1## Florida 15.4 335 80 31.9## Louisiana 15.4 249 66 22.2## South Carolina 14.4 279 48 22.5## Alabama 13.2 236 58 21.2## Tennessee 13.2 188 59 26.9## North Carolina 13.0 337 45 16.1## Texas 12.7 201 80 25.5## Nevada 12.2 252 81 46.0## Michigan 12.1 255 74 35.1## New Mexico 11.4 285 70 32.1## Maryland 11.3 300 67 27.8## New York 11.1 254 86 26.1## Illinois 10.4 249 83 24.0## Alaska 10.0 263 48 44.5## Kentucky 9.7 109 52 16.3## California 9.0 276 91 40.6## Missouri 9.0 178 70 28.2## Arkansas 8.8 190 50 19.5## Virginia 8.5 156 63 20.7## Arizona 8.1 294 80 31.0## Colorado 7.9 204 78 38.7## New Jersey 7.4 159 89 18.8## Ohio 7.3 120 75 21.4## Indiana 7.2 113 65 21.0## Wyoming 6.8 161 60 15.6## Oklahoma 6.6 151 68 20.0## Pennsylvania 6.3 106 72 14.9## Kansas 6.0 115 66 18.0## Montana 6.0 109 53 16.4## Delaware 5.9 238 72 15.8## West Virginia 5.7 81 39 9.3## Hawaii 5.3 46 83 20.2## Oregon 4.9 159 67 29.3## Massachusetts 4.4 149 85 16.3## Nebraska 4.3 102 62 16.5## Washington 4.0 145 73 26.2## South Dakota 3.8 86 45 12.8## Rhode Island 3.4 174 87 8.3## Connecticut 3.3 110 77 11.1## Utah 3.2 120 80 22.9## Minnesota 2.7 72 66 14.9## Idaho 2.6 120 54 14.2## Wisconsin 2.6 53 66 10.8## Iowa 2.2 56 57 11.3## Vermont 2.2 48 32 11.2## Maine 2.1 83 51 7.8## New Hampshire 2.1 57 56 9.5## North Dakota 0.8 45 44 7.3 1USArrests[ order( Murder, decreasing = TRUE ), ] 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051## Murder Assault UrbanPop Rape## Georgia 17.4 211 60 25.8## Mississippi 16.1 259 44 17.1## Florida 15.4 335 80 31.9## Louisiana 15.4 249 66 22.2## South Carolina 14.4 279 48 22.5## Alabama 13.2 236 58 21.2## Tennessee 13.2 188 59 26.9## North Carolina 13.0 337 45 16.1## Texas 12.7 201 80 25.5## Nevada 12.2 252 81 46.0## Michigan 12.1 255 74 35.1## New Mexico 11.4 285 70 32.1## Maryland 11.3 300 67 27.8## New York 11.1 254 86 26.1## Illinois 10.4 249 83 24.0## Alaska 10.0 263 48 44.5## Kentucky 9.7 109 52 16.3## California 9.0 276 91 40.6## Missouri 9.0 178 70 28.2## Arkansas 8.8 190 50 19.5## Virginia 8.5 156 63 20.7## Arizona 8.1 294 80 31.0## Colorado 7.9 204 78 38.7## New Jersey 7.4 159 89 18.8## Ohio 7.3 120 75 21.4## Indiana 7.2 113 65 21.0## Wyoming 6.8 161 60 15.6## Oklahoma 6.6 151 68 20.0## Pennsylvania 6.3 106 72 14.9## Kansas 6.0 115 66 18.0## Montana 6.0 109 53 16.4## Delaware 5.9 238 72 15.8## West Virginia 5.7 81 39 9.3## Hawaii 5.3 46 83 20.2## Oregon 4.9 159 67 29.3## Massachusetts 4.4 149 85 16.3## Nebraska 4.3 102 62 16.5## Washington 4.0 145 73 26.2## South Dakota 3.8 86 45 12.8## Rhode Island 3.4 174 87 8.3## Connecticut 3.3 110 77 11.1## Utah 3.2 120 80 22.9## Minnesota 2.7 72 66 14.9## Idaho 2.6 120 54 14.2## Wisconsin 2.6 53 66 10.8## Iowa 2.2 56 57 11.3## Vermont 2.2 48 32 11.2## Maine 2.1 83 51 7.8## New Hampshire 2.1 57 56 9.5## North Dakota 0.8 45 44 7.3]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[R Your Ready? 100分钟R入门教程(下)]]></title>
      <url>%2F2014%2F10%2F11%2F2014-10-11-R-crash-course-2%2F</url>
      <content type="text"><![CDATA[Read in data工作环境相关123456ls() # 列出已创建变量remove( object.name ) # 删除变量dir() getwd() setwd("d:/project/datascience/rtraining/") read.tableread.table是R里面常用的读取ASCII文件数据的方式，通过制定参数sep,header可以对读入方式进行更细致的控制。 ?read.table 123setwd("d:/project/datascience/rtraining/")test.txt &lt;- read.table("./data/test.txt", header=T)print(test.txt) 123456## make model mpg weight price## 1 AMC Concord 22 2930 4099## 2 AMC Pacer 17 3350 4749## 3 AMC Spirit 22 2640 3799## 4 Buick Century 20 3250 4816## 5 Buick Electra 15 4080 7827 read.csvread.csv是read.table的特列，专门针对csv文件格式 12test.csv &lt;- read.csv("./data/test.csv", header=T)test.csv 123456## make model mpg weight price## 1 amc concord 22 2930 4099## 2 amc oacer 17 3350 4749## 3 amc spirit 22 2640 3799## 4 buick century 20 3250 4816## 5 buick electra 15 4080 7827 12test.csv1 &lt;- read.table("./data/test.csv", header=T, sep=",")test.csv1 123456## make model mpg weight price## 1 amc concord 22 2930 4099## 2 amc oacer 17 3350 4749## 3 amc spirit 22 2640 3799## 4 buick century 20 3250 4816## 5 buick electra 15 4080 7827 read.table读入其它格式文件12test.semi = read.table("./data/testsemicolon.txt", header=T, sep=";")test.semi 123456## make model mpg weight price## 1 AMC Concord 22 2930 4099## 2 AMC Pacer 17 3350 4749## 3 AMC Spirit 22 2640 3799## 4 Buick Century 20 3250 4816## 5 Buick Electra 15 4080 7827 12test.z = read.table("./data/testz.txt", header=T, sep="z")test.z 123456## make model mpg weight price## 1 AMC Concord 22 2930 4099## 2 AMC Pacer 17 3350 4749## 3 AMC Spirit 22 2640 3799## 4 Buick Century 20 3250 4816## 5 Buick Electra 15 4080 7827 scanscan是极为灵活的输入数据方式，可以读入几乎任何类型数据，数字，字符…。同时通过scan你可以直接从控制台输入。缺省条件下scan的参数what接收数字，制定what=””接受字符。不过需要注意与read.table不同，scan返回的是向量。 12x=scan()x 1## numeric(0) 12name.x=scan(,what="")name.x 1## character(0) 12x = scan("./data/scan.txt", what=list(age=0, name=""))x 12345## $age## [1] 12 24 35 20## ## $name## [1] &quot;bobby&quot; &quot;kate&quot; &quot;david&quot; &quot;michael&quot; read.fwfread.fwf读入固定宽度格式保存的文件。 12names = scan("./data/names.txt", what=character() )names 1## [1] &quot;model&quot; &quot;make&quot; &quot;mph&quot; &quot;weight&quot; &quot;price&quot; 12test.fixed = read.fwf("./data/testfixed.txt", col.names=names, width = c(5, 7, 2, 4, 4)) 1## Warning: incomplete final line found on &apos;./data/testfixed.txt&apos; 12# 有一个警告出来是什么原因呢？猜猜test.fixed 123456## model make mph weight price## 1 AMC Concord 22 2930 4099## 2 AMC Pacer 17 3350 4749## 3 AMC Spirit 22 2640 3799## 4 Buick Century 20 3250 4816## 5 Buick Electra 15 4080 7827 内部数据1data( USArrests ) 读取远程数据前面的read系列函数也支持读取远程数据 1pew_data = read.csv("http://bit.ly/11I3iuU") 1## Warning: cannot open: HTTP status was &apos;0 (nil)&apos; 1## Error: cannot open the connection 12pew_data = read.csv("http://www.pewinternet.org/files/old-media/Files/Data%20Sets/2013/Omnibus_Jan_2013_csv.csv")head(pew_data) 123456789101112131415161718192021## psraid sample state cregion usr pial1a pial1b pial1c pial1d pial1e pial2## 1 100002 1 18 2 S 2 2 2 2 2 NA## 2 100005 1 18 2 U 1 2 1 1 2 2## 3 100008 1 39 2 R 1 2 2 2 1 2## 4 100010 1 24 3 S 1 2 2 1 2 1## 5 100012 1 42 1 S 1 2 2 2 2 2## 6 100014 1 36 1 S 2 2 2 2 2 NA## pial3a pial3b pial3c pial4 employ par sex age educ2 hisp race inc weight## 1 NA NA NA NA 3 2 1 62 4 2 1 2 1.463## 2 2 2 2 NA 3 2 2 85 8 2 2 1 1.122## 3 2 2 2 NA 1 1 1 41 3 2 1 4 2.268## 4 2 2 2 NA 3 2 1 19 3 2 1 7 2.415## 5 2 2 2 NA 3 2 2 84 2 2 2 2 1.683## 6 NA NA NA NA 3 2 2 84 4 2 1 7 1.976## standwt## 1 0.4879## 2 0.3741## 3 0.7563## 4 0.8051## 5 0.5611## 6 0.6587 1#http://www.pewinternet.org/files/old-media/Files/Data%20Sets/2013/Omnibus_Jan_2013_csv.csv 其它格式数据与老外通话肯定得讲外语了，所以我们需要foreign包 12library(foreign)help(package=foreign) data.restore Read an S3 Binary Filelookup.xport Lookup Information on a SAS XPORT Format Libraryread.dbf Read a DBF Fileread.dta Read Stata binary filesread.epiinfo Read Epi Info data filesread.mtp Read a Minitab Portable Worksheetread.octave Read Octave Text Data Filesread.spss Read an SPSS data fileread.ssd Obtain a Data Frame from a SAS Permanent Dataset, via read.xportread.systat Obtain a Data Frame from a Systat Fileread.xport Read a SAS XPORT Format Librarywrite.dbf Write a DBF Filewrite.dta Write Files in Stata Binary Formatwrite.foreign Write text files and code to read them 访问数据库RPostgreSQLRMySQLRMongoRSQLiteRODBC 最后重点推出 sqldf 数据保存与加载123save( the.data, file="thedata.rda" )load( file = "thedata.rda" ) # rda == R Datawrite.table( USArrests, file = "arrests.txt", sep = "," ) R的绘图功能R 中的绘图系统: base graphics lattice graphics ggplot2 12earnings = read.table("./data/heights.csv",sep=" ")names(earnings) 12## [1] &quot;earn&quot; &quot;height1&quot; &quot;height2&quot; &quot;sex&quot; &quot;race&quot; &quot;hisp&quot; &quot;ed&quot; ## [8] &quot;yearbn&quot; &quot;height&quot; 1hist(earnings$earn) 1plot(earnings$earn ~ earnings$height) 1boxplot(earnings$earn ~ earnings$height) 1boxplot(earnings$earn ~ earnings$sex) 赐予我力量吧-packages packages 是R的动力源泉 5,000 packages ，提供各种功能 一些例子 ggplot2 package 高级回归包lme4 3d 图形 scatterplot3d package GIS 分析与地图 sp 文本挖掘 tm 预测建模 caret 与 Python, Java, C , C++的接口 服务器版本: Rserve 甚至扫雷都有- fun package 如何获取1234567891011install.packages('foo') # Name must be in quotesinstall.packages(c('foo','foo1','foo2'))install.packages(file_name_and_path, repos = NULL, type="source")# Packages 更新update.packages() # update allinstalled.packages()remove.packages()available.packages()old.packages()require("ggplot2")library() 查找 Packages packages on CRAN. CRAN Task Views. 非官方网站 RForge , GitHub 利用某包完成某功能 放狗google Google “doing X in R package” 或者CRAN taskviews CRAN taskviews 一些常用包plyr ggplot2 caret rattle lme4 reshape2 knitr dplyr 包里面包含了函数，代码，数据 help(package=&quot;ggplot2&quot;) vignettes 使用包 安装 加载 RStudio, Packages-&gt;Install Packages. 123install.packages('fields')#指定目录install.packages('fields', lib = '~/R') ## 12library(help="ggplot2")help(package="ggplot2") R编程if-else12val = rnorm(1)val 1## [1] 0.6169 12345if (val &lt; 0) &#123; "val is negative!"&#125; else &#123; "val is positive"&#125; 1## [1] &quot;val is positive&quot; 123x=rnorm(1)y = if ( x &lt; 0 ) -1 else 0 y 1## [1] -1 1x 1## [1] -0.681 嵌套if-else123456789# multiline if - else statement if ( x &lt; 0 ) &#123; x = x+1 print("Add one") &#125; else if ( x == 0 ) &#123; print("Zero")&#125; else &#123; print("Positive value")&#125; 1## [1] &quot;Add one&quot; ifelseifelse(test_condition, true_value, false_value) 12y = ifelse ( x &lt; 0, -1, 0 )y 1## [1] 0 123456# nested ifelse statement y = ifelse ( x &lt; 0, -1, ifelse (x &gt; 0, 1, 0) )# ifelse statement on a vectordigits = 0 : 9(odd = ifelse( digits %% 2 &gt; 0, TRUE, FALSE )) 1## [1] FALSE TRUE FALSE TRUE FALSE TRUE FALSE TRUE FALSE TRUE 练习定义一个向量，包含的10个数，数值在-10～10间均匀分布 x= as.integer( runif( 10, -10, 10 )) 利用ifelse 来求x的绝对值。（通常用abs()可以解决） ## 12x= as.integer( runif( 10, -10, 10 ))ifelse(x&lt;0,-x,x) 1## [1] 0 6 7 2 4 7 9 4 6 9 循环 重复执行 避免使用循环 for,repeat,while for抽象表达： 123for (variable in sequence) &#123; statement&#125; for1234myseq = seq(5,20,by=5)for (i in myseq) &#123; print(i)&#125; 1234## [1] 5## [1] 10## [1] 15## [1] 20 123for (i in seq(2,8, by=2)) &#123; print(i)&#125; 1234## [1] 2## [1] 4## [1] 6## [1] 8 while抽象表达： 123while (condition) &#123; statements&#125; while实现12345i = 0while (i &lt; 10) &#123; i = i + 1 print(i)&#125; 12345678910## [1] 1## [1] 2## [1] 3## [1] 4## [1] 5## [1] 6## [1] 7## [1] 8## [1] 9## [1] 10 注意不要出现无限循环！ Fibonacci123456myseq[1] = 0myseq[2] = 1for (i in seq(3,12)) &#123; myseq[i] = myseq[i-2] + myseq[i-1]&#125;myseq 1## [1] 0 1 1 2 3 5 8 13 21 34 55 89 12345678910myseq[1] = 0myseq[2] = 1i = 2currentVal = 1while (currentVal &lt; 500) &#123; myseq[i+1] = currentVal currentVal = myseq[i] + myseq[i+1] i = i+1&#125;myseq 1## [1] 0 1 1 2 3 5 8 13 21 34 55 89 144 233 377 next 与 breaknext 跳过当前的循环语句 123456for (i in seq(1,10)) &#123; if (i == 5) &#123; next &#125; print(i)&#125; 123456789## [1] 1## [1] 2## [1] 3## [1] 4## [1] 6## [1] 7## [1] 8## [1] 9## [1] 10 break 直接跳出循环： 12345678910val = 0i = 0while(TRUE) &#123; i = i + 1 val = val + rnorm(1) if (abs(val) &gt; 3) &#123; break &#125;&#125;val 1## [1] -3.359 1i 1## [1] 4 循环通常不是最佳方案12345678vals = rnorm(100)maxVal = vals[1]for (val in vals) &#123; if (val &gt; maxVal) &#123; maxVal = val &#125;&#125;maxVal 1## [1] 3.047 使用内部函数： 1max(vals) 1## [1] 3.047 向量化运算而不是循环: 12345myseq = seq(2,20, by =2)for(i in seq(1,10)) &#123; myseq[i] = myseq[i] + 2&#125;myseq 1## [1] 4 6 8 10 12 14 16 18 20 22 1seq(2,20, by =2)+2 1## [1] 4 6 8 10 12 14 16 18 20 22 applyapply(), sapply(), tapply(), lapply() 函数函数: arguments 输入，return 输出 为什么要函数： 重用 清晰 invisible return1234567histNormal = function(N) &#123; vals = rnorm(N) hist(vals) invisible(max(vals))&#125;histNormal(1000) 1max = histNormal(1000) 1max 1## [1] 2.989 多参数以及缺省值User-defined functions V: multiple arguments, default values12345678newFunction = function(num, threshold=0, modifier=2) &#123; if (num &lt; threshold) &#123; return(num / modifier) &#125; else &#123; return(num * modifier) &#125;&#125;newFunction(2.6) 1## [1] 5.2 R缺省从左到有匹配参数: 1newFunction(2.6, 3) 1## [1] 1.3 1newFunction(2.6, 3, 1.3) 1## [1] 2 当然你可以明确指定 1newFunction(2.6, modifier=1.3, threshold=3) 1## [1] 2 1hist(sapply(rnorm(10000), newFunction), breaks=60, freq=FALSE) 1hist(sapply(rnorm(10000), newFunction, modifier=1), breaks=60, freq=FALSE) ...1args(hist) 12## function (x, ...) ## NULL 123456histNormalWrapper = function(N, ...) &#123; vals = rnorm(N) hist(vals, ...)&#125;histNormalWrapper(1000) 1histNormalWrapper(1000, breaks=50) table12data( UCBAdmissions )ls() 1234567891011121314## [1] &quot;bins&quot; &quot;colors&quot; &quot;currentVal&quot; ## [4] &quot;digits&quot; &quot;earnings&quot; &quot;f&quot; ## [7] &quot;freqs&quot; &quot;hair.color&quot; &quot;histNormal&quot; ## [10] &quot;histNormalWrapper&quot; &quot;i&quot; &quot;max&quot; ## [13] &quot;maxVal&quot; &quot;my.data&quot; &quot;myfac&quot; ## [16] &quot;myfac_o&quot; &quot;mylist1&quot; &quot;mylist2&quot; ## [19] &quot;mylist3&quot; &quot;myseq&quot; &quot;name.x&quot; ## [22] &quot;names&quot; &quot;newFunction&quot; &quot;odd&quot; ## [25] &quot;pew_data&quot; &quot;q&quot; &quot;SO&quot; ## [28] &quot;test.csv&quot; &quot;test.csv1&quot; &quot;test.fixed&quot; ## [31] &quot;test.semi&quot; &quot;test.txt&quot; &quot;test.z&quot; ## [34] &quot;the.data&quot; &quot;theDF&quot; &quot;UCBAdmissions&quot; ## [37] &quot;USArrests&quot; &quot;val&quot; &quot;vals&quot; ## [40] &quot;x&quot; &quot;X&quot; &quot;y&quot; 1UCBAdmissions 1234567891011121314151617181920212223242526272829303132333435363738394041## , , Dept = A## ## Gender## Admit Male Female## Admitted 512 89## Rejected 313 19## ## , , Dept = B## ## Gender## Admit Male Female## Admitted 353 17## Rejected 207 8## ## , , Dept = C## ## Gender## Admit Male Female## Admitted 120 202## Rejected 205 391## ## , , Dept = D## ## Gender## Admit Male Female## Admitted 138 131## Rejected 279 244## ## , , Dept = E## ## Gender## Admit Male Female## Admitted 53 94## Rejected 138 299## ## , , Dept = F## ## Gender## Admit Male Female## Admitted 22 24## Rejected 351 317 一个三维数组，伯克利6个不同系的男女录取情况。 1ftable( UCBAdmissions ) 123456## Dept A B C D E F## Admit Gender ## Admitted Male 512 353 120 138 53 22## Female 89 17 202 131 94 24## Rejected Male 313 207 205 279 138 351## Female 19 8 391 244 299 317 转换成data frame 12UCB = as.data.frame( UCBAdmissions )UCB 12345678910111213141516171819202122232425## Admit Gender Dept Freq## 1 Admitted Male A 512## 2 Rejected Male A 313## 3 Admitted Female A 89## 4 Rejected Female A 19## 5 Admitted Male B 353## 6 Rejected Male B 207## 7 Admitted Female B 17## 8 Rejected Female B 8## 9 Admitted Male C 120## 10 Rejected Male C 205## 11 Admitted Female C 202## 12 Rejected Female C 391## 13 Admitted Male D 138## 14 Rejected Male D 279## 15 Admitted Female D 131## 16 Rejected Female D 244## 17 Admitted Male E 53## 18 Rejected Male E 138## 19 Admitted Female E 94## 20 Rejected Female E 299## 21 Admitted Male F 22## 22 Rejected Male F 351## 23 Admitted Female F 24## 24 Rejected Female F 317 2-way tables123library( MASS )data( caith )mode(caith) #似乎是个list 1## [1] &quot;list&quot; 1234567caith = as.matrix( caith ) # 变成一个矩阵# 加个row,colnamedimnames(caith) = list( Eyes = c( "blue", "light", "medium", "dark" ), Hair = c( "fair", "red", "medium", "dark", "black" ))caith 123456## Hair## Eyes fair red medium dark black## blue 326 38 241 110 3## light 688 116 584 188 4## medium 343 84 909 412 26## dark 98 48 403 681 85 12# addmarginaddmargins( caith ) 1234567## Hair## Eyes fair red medium dark black Sum## blue 326 38 241 110 3 718## light 688 116 584 188 4 1580## medium 343 84 909 412 26 1774## dark 98 48 403 681 85 1315## Sum 1455 286 2137 1391 118 5387 1margin.table( caith ) # 全部汇总 1## [1] 5387 1margin.table( caith, 1 ) # the row marginal sums (index 1) 123## Eyes## blue light medium dark ## 718 1580 1774 1315 1margin.table( caith, 2 ) # column marginal sums (index 2) 123## Hair## fair red medium dark black ## 1455 286 2137 1391 118 12#比例，行用1，列用2prop.table( caith, 1 ) 123456## Hair## Eyes fair red medium dark black## blue 0.45404 0.05292 0.3357 0.1532 0.004178## light 0.43544 0.07342 0.3696 0.1190 0.002532## medium 0.19335 0.04735 0.5124 0.2322 0.014656## dark 0.07452 0.03650 0.3065 0.5179 0.064639 1prop.table( caith, 1 ) * 100 123456## Hair## Eyes fair red medium dark black## blue 45.404 5.292 33.57 15.32 0.4178## light 43.544 7.342 36.96 11.90 0.2532## medium 19.335 4.735 51.24 23.22 1.4656## dark 7.452 3.650 30.65 51.79 6.4639 12#可视化mosaicplot( caith, shade=TRUE ) 1barplot( caith, beside=TRUE, legend=TRUE ) # not shown Multiway tables1HairEyeColor #contingency table 1234567891011121314151617## , , Sex = Male## ## Eye## Hair Brown Blue Hazel Green## Black 32 11 10 3## Brown 53 50 25 15## Red 10 10 7 7## Blond 3 30 5 8## ## , , Sex = Female## ## Eye## Hair Brown Blue Hazel Green## Black 36 9 5 2## Brown 66 34 29 14## Red 16 7 7 7## Blond 4 64 5 8 1as.data.frame( HairEyeColor ) # dataframe 123456789101112131415161718192021222324252627282930313233## Hair Eye Sex Freq## 1 Black Brown Male 32## 2 Brown Brown Male 53## 3 Red Brown Male 10## 4 Blond Brown Male 3## 5 Black Blue Male 11## 6 Brown Blue Male 50## 7 Red Blue Male 10## 8 Blond Blue Male 30## 9 Black Hazel Male 10## 10 Brown Hazel Male 25## 11 Red Hazel Male 7## 12 Blond Hazel Male 5## 13 Black Green Male 3## 14 Brown Green Male 15## 15 Red Green Male 7## 16 Blond Green Male 8## 17 Black Brown Female 36## 18 Brown Brown Female 66## 19 Red Brown Female 16## 20 Blond Brown Female 4## 21 Black Blue Female 9## 22 Brown Blue Female 34## 23 Red Blue Female 7## 24 Blond Blue Female 64## 25 Black Hazel Female 5## 26 Brown Hazel Female 29## 27 Red Hazel Female 7## 28 Blond Hazel Female 5## 29 Black Green Female 2## 30 Brown Green Female 14## 31 Red Green Female 7## 32 Blond Green Female 8 1ftable( HairEyeColor ) # flat contingency table 123456789101112131415161718## Sex Male Female## Hair Eye ## Black Brown 32 36## Blue 11 9## Hazel 10 5## Green 3 2## Brown Brown 53 66## Blue 50 34## Hazel 25 29## Green 15 14## Red Brown 10 16## Blue 10 7## Hazel 7 7## Green 7 7## Blond Brown 3 4## Blue 30 64## Hazel 5 5## Green 8 8 1ftable( HairEyeColor, col.vars="Hair" ) # 换一个不同列变量 12345678910## Hair Black Brown Red Blond## Eye Sex ## Brown Male 32 53 10 3## Female 36 66 16 4## Blue Male 11 50 10 30## Female 9 34 7 64## Hazel Male 10 25 7 5## Female 5 29 7 5## Green Male 3 15 7 8## Female 2 14 7 8 1summary( HairEyeColor ) # this works only for 3-way and up 12345## Number of cases in table: 592 ## Number of factors: 3 ## Test for independence of all factors:## Chisq = 165, df = 24, p-value = 5e-23## Chi-squared approximation may be incorrect 12hec = as.data.frame( HairEyeColor )head( hec ) # first six rows 1234567## Hair Eye Sex Freq## 1 Black Brown Male 32## 2 Brown Brown Male 53## 3 Red Brown Male 10## 4 Blond Brown Male 3## 5 Black Blue Male 11## 6 Brown Blue Male 50 Hair-行，Eye-列，统计频数 1xtabs( Freq ~ Hair + Eye, data=hec ) 123456## Eye## Hair Brown Blue Hazel Green## Black 68 20 15 5## Brown 119 84 54 29## Red 26 17 14 14## Blond 7 94 10 16 只去”Male” 1xtabs( Freq ~ Hair + Eye, data=hec, subset=Sex=="Male" ) 123456## Eye## Hair Brown Blue Hazel Green## Black 32 11 10 3## Brown 53 50 25 15## Red 10 10 7 7## Blond 3 30 5 8 更多列的统计 12ti = as.data.frame( Titanic )head( ti ) 1234567## Class Sex Age Survived Freq## 1 1st Male Child No 0## 2 2nd Male Child No 0## 3 3rd Male Child No 35## 4 Crew Male Child No 0## 5 1st Female Child No 0## 6 2nd Female Child No 0 1xtabs( Freq ~ Class + Age + Sex, data=ti ) 1234567891011121314151617## , , Sex = Male## ## Age## Class Child Adult## 1st 5 175## 2nd 11 168## 3rd 48 462## Crew 0 862## ## , , Sex = Female## ## Age## Class Child Adult## 1st 1 144## 2nd 13 93## 3rd 31 165## Crew 0 23 123HEC.Male = xtabs( Freq ~ Hair + Eye, data=hec, subset=Sex=="Male" )prop.table( HEC.Male, 1 ) * 100 123456## Eye## Hair Brown Blue Hazel Green## Black 57.143 19.643 17.857 5.357## Brown 37.063 34.965 17.483 10.490## Red 29.412 29.412 20.588 20.588## Blond 6.522 65.217 10.870 17.391 1results=chisq.test( HEC.Male ) 1## Warning: Chi-squared approximation may be incorrect 1results 12345## ## Pearson&apos;s Chi-squared test## ## data: HEC.Male## X-squared = 41.28, df = 9, p-value = 4.447e-06 1results$expected 123456## Eye## Hair Brown Blue Hazel Green## Black 19.67 20.27 9.434 6.624## Brown 50.23 51.77 24.090 16.914## Red 11.94 12.31 5.728 4.022## Blond 16.16 16.65 7.749 5.441 1results$residuals 123456## Eye## Hair Brown Blue Hazel Green## Black 2.7800 -2.0594 0.1844 -1.4080## Brown 0.3909 -0.2456 0.1855 -0.4654## Red -0.5621 -0.6579 0.5317 1.4853## Blond -3.2733 3.2709 -0.9876 1.0971 R之统计假设Single sample t-test.1the.data 1## [1] 7 7 7 5 5 4 4 4 4 4 4 4 3 3 3 3 3 2 1 12# 双尾t.test( the.data, mu=5 ) # H0 says mu=5, two-tailed 1234567891011## ## One Sample t-test## ## data: the.data## t = -2.557, df = 18, p-value = 0.01981## alternative hypothesis: true mean is not equal to 5## 95 percent confidence interval:## 3.274 4.831## sample estimates:## mean of x ## 4.053 12# 单侧 greater t.test( the.data, mu=5, alternative="greater" ) 1234567891011## ## One Sample t-test## ## data: the.data## t = -2.557, df = 18, p-value = 0.9901## alternative hypothesis: true mean is greater than 5## 95 percent confidence interval:## 3.41 Inf## sample estimates:## mean of x ## 4.053 12# lesst.test( the.data, mu=5, alternative="less" ) 1234567891011## ## One Sample t-test## ## data: the.data## t = -2.557, df = 18, p-value = 0.009904## alternative hypothesis: true mean is less than 5## 95 percent confidence interval:## -Inf 4.695## sample estimates:## mean of x ## 4.053 独立 t-test.12345x1 = c( 18, 25, 17, 20, 23 )x2 = c( 20, 30, 22, 25, 28, 30 )#pooled varianct.test( x1, x2, var.equal=TRUE ) 1234567891011## ## Two Sample t-test## ## data: x1 and x2## t = -2.24, df = 9, p-value = 0.05188## alternative hypothesis: true difference in means is not equal to 0## 95 percent confidence interval:## -10.51953 0.05287## sample estimates:## mean of x mean of y ## 20.60 25.83 123# unpooled variance(缺省)t.test( x1, x2 ) 1234567891011## ## Welch Two Sample t-test## ## data: x1 and x2## t = -2.29, df = 8.995, p-value = 0.04776## alternative hypothesis: true difference in means is not equal to 0## 95 percent confidence interval:## -10.40273 -0.06393## sample estimates:## mean of x mean of y ## 20.60 25.83 123# one-tailed (R 里面是 group 1 - group 2)t.test( x1, x2, var.equal=T, alternative="less" ) 1234567891011## ## Two Sample t-test## ## data: x1 and x2## t = -2.24, df = 9, p-value = 0.02594## alternative hypothesis: true difference in means is less than 0## 95 percent confidence interval:## -Inf -0.9497## sample estimates:## mean of x mean of y ## 20.60 25.83 1234# 实际场景all.scores = c( x1, x2 ) # combines x1 and x2 into one vectorall.scores 1## [1] 18 25 17 20 23 20 30 22 25 28 30 1234grp = c( "x1", "x2" ) # the group namesn = c( 5, 6 ) # the groups sizesgroups = rep( grp, n ) # create the labelsgroups 1## [1] &quot;x1&quot; &quot;x1&quot; &quot;x1&quot; &quot;x1&quot; &quot;x1&quot; &quot;x2&quot; &quot;x2&quot; &quot;x2&quot; &quot;x2&quot; &quot;x2&quot; &quot;x2&quot; 1t.test( all.scores ~ groups ) 1234567891011## ## Welch Two Sample t-test## ## data: all.scores by groups## t = -2.29, df = 8.995, p-value = 0.04776## alternative hypothesis: true difference in means is not equal to 0## 95 percent confidence interval:## -10.40273 -0.06393## sample estimates:## mean in group x1 mean in group x2 ## 20.60 25.83]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[2010~2014 飓风可视化尝试]]></title>
      <url>%2F2014%2F10%2F09%2F2014-10-09-hurricane%2F</url>
      <content type="text"><![CDATA[最近对数据可视化进行了一些研究学习，下面是参考Gaston Sanchez之前对2009～2010年飓风的可视化方法对2010~2014发生的飓风进行了一个可视化尝试，采用R实现。 数据获取International Best Track Archive for Climate Stewardship网站会收集飓风信息数据，我们这里选择下载csv格式数据。 12345678setwd("d:/project/datascience/hurricane/")row.names=c("Serial_Num","Season","Num","Basin","Sub_basin","Name", "ISO_time","Nature","Latitude","Longitude","Wind.WMO","Pres.WMO", "Center","Wind.WMO.Percentile","Pres.WMO.Percentile","Track_type")hurricane=read.csv("Allstorms.ibtracs_wmo.v03r06.csv",skip=3, header=F,stringsAsFactors=F)names(hurricane)=row.nameshead(hurricane) 123456789101112131415161718192021## Serial_Num Season Num Basin Sub_basin Name ISO_time## 1 1848011S09080 1848 2 SI MM XXXX848003 1848-01-11 06:00:00## 2 1848011S09080 1848 2 SI MM XXXX848003 1848-01-12 06:00:00## 3 1848011S09080 1848 2 SI MM XXXX848003 1848-01-13 06:00:00## 4 1848011S09080 1848 2 SI MM XXXX848003 1848-01-14 06:00:00## 5 1848011S09080 1848 2 SI MM XXXX848003 1848-01-15 06:00:00## 6 1848011S09080 1848 2 SI MM XXXX848003 1848-01-16 06:00:00## Nature Latitude Longitude Wind.WMO Pres.WMO Center Wind.WMO.Percentile## 1 NR -8.6 79.8 0 0 reunion -100## 2 NR -9.0 78.9 0 0 reunion -100## 3 NR -10.4 73.2 0 0 reunion -100## 4 NR -12.8 69.9 0 0 reunion -100## 5 NR -13.9 68.9 0 0 reunion -100## 6 NR -15.3 67.7 0 0 reunion -100## Pres.WMO.Percentile Track_type## 1 -100 main## 2 -100 main## 3 -100 main## 4 -100 main## 5 -100 main## 6 -100 main 1str(hurricane) 1234567891011121314151617## &apos;data.frame&apos;: 189855 obs. of 16 variables:## $ Serial_Num : chr &quot;1848011S09080&quot; &quot;1848011S09080&quot; &quot;1848011S09080&quot; &quot;1848011S09080&quot; ...## $ Season : int 1848 1848 1848 1848 1848 1848 1848 1848 1848 1848 ...## $ Num : int 2 2 2 2 2 2 2 2 2 2 ...## $ Basin : chr &quot; SI&quot; &quot; SI&quot; &quot; SI&quot; &quot; SI&quot; ...## $ Sub_basin : chr &quot; MM&quot; &quot; MM&quot; &quot; MM&quot; &quot; MM&quot; ...## $ Name : chr &quot;XXXX848003&quot; &quot;XXXX848003&quot; &quot;XXXX848003&quot; &quot;XXXX848003&quot; ...## $ ISO_time : chr &quot;1848-01-11 06:00:00&quot; &quot;1848-01-12 06:00:00&quot; &quot;1848-01-13 06:00:00&quot; &quot;1848-01-14 06:00:00&quot; ...## $ Nature : chr &quot; NR&quot; &quot; NR&quot; &quot; NR&quot; &quot; NR&quot; ...## $ Latitude : num -8.6 -9 -10.4 -12.8 -13.9 -15.3 -16.5 -18 -20.6 -22.8 ...## $ Longitude : num 79.8 78.9 73.2 69.9 68.9 67.7 67 67.4 69.8 72 ...## $ Wind.WMO : num 0 0 0 0 0 0 0 0 0 0 ...## $ Pres.WMO : num 0 0 0 0 0 0 0 0 0 0 ...## $ Center : chr &quot;reunion&quot; &quot;reunion&quot; &quot;reunion&quot; &quot;reunion&quot; ...## $ Wind.WMO.Percentile: num -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 ...## $ Pres.WMO.Percentile: num -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 ...## $ Track_type : chr &quot;main&quot; &quot;main&quot; &quot;main&quot; &quot;main&quot; ... 预处理数据通过对数据大致了解，我们知道Season实际也对应于年，即Season为1848就代表了该行对应1848。这里取2010年以后数据进行分析 1require(dplyr) 123hurricane.df=tbl_df(hurricane)hurricane.after.2010=hurricane.df%&gt;%filter(Season&gt;=2010)hurricane.after.2010 1234567891011121314151617## Source: local data frame [12,324 x 16]## ## Serial_Num Season Num Basin Sub_basin Name ISO_time## 1 2009317S10073 2010 1 SI MM ANJA 2009-11-13 06:00:00## 2 2009317S10073 2010 1 SI MM ANJA 2009-11-13 12:00:00## 3 2009317S10073 2010 1 SI MM ANJA 2009-11-13 18:00:00## 4 2009317S10073 2010 1 SI MM ANJA 2009-11-14 00:00:00## 5 2009317S10073 2010 1 SI MM ANJA 2009-11-14 06:00:00## 6 2009317S10073 2010 1 SI MM ANJA 2009-11-14 12:00:00## 7 2009317S10073 2010 1 SI MM ANJA 2009-11-14 18:00:00## 8 2009317S10073 2010 1 SI MM ANJA 2009-11-15 00:00:00## 9 2009317S10073 2010 1 SI MM ANJA 2009-11-15 06:00:00## 10 2009317S10073 2010 1 SI MM ANJA 2009-11-15 12:00:00## .. ... ... ... ... ... ... ...## Variables not shown: Nature (chr), Latitude (dbl), Longitude (dbl),## Wind.WMO (dbl), Pres.WMO (dbl), Center (chr), Wind.WMO.Percentile (dbl),## Pres.WMO.Percentile (dbl), Track_type (chr) 1dim(hurricane.after.2010) 1## [1] 12324 16 这里的ISO_time同时包含了年月日以及时间，我们这里先把月份给单独取出来。 1234567#拆分date.time=strsplit(hurricane.after.2010$ISO_time," ")#只取日期iso.date=unlist(lapply(date.time,function(x) x[1]))#取月份iso.month=substr(iso.date,6,7)hurricane.after.2010$Month=iso.month 对于没有命名的飓风一般都不是特别厉害，这里就不考虑了 1hurricane.after.2010=hurricane.after.2010%&gt;%filter(Name!="NAMED" &amp; Name!="NOT NAMED") 准备绘图1require(ggplot2) 1## Loading required package: ggplot2 1require(ggthemes) 1## Loading required package: ggthemes 1## Warning: package &apos;ggthemes&apos; was built under R version 3.1.1 1234567891011# 我们再给飓风加上一个ID，以飓风的Name+Season来命名# 这样后面我们可以获得飓风的路径用来绘图hurricane.after.2010$ID = as.factor(paste(hurricane.after.2010$Name, hurricane.after.2010$Season, sep = "."))# 将飓风Name转换为factorhurricane.after.2010$Name = as.factor(hurricane.after.2010$Name)# 经过试验发现其他地区数据有些错误，所以只使用NA与EP区域hurricane.after.2010$Basin=gsub("^ ", "", hurricane.after.2010$Basin)hurricane.after.2010=hurricane.after.2010%&gt;%filter(Basin=="NA"|Basin=="EP") 现在一切就绪开始绘图 123456789101112131415161718192021x.range=range(hurricane.after.2010$Longitude)y.range=range(hurricane.after.2010$Latitude)hurricane.map = ggplot(hurricane.after.2010, aes(x = Longitude, y = Latitude, group = ID)) + geom_polygon(data = map_data("world"), aes(x = long, y = lat, group = group), fill = "gray25", colour = "gray10", size = 0.2) + geom_path(data = hurricane.after.2010, aes(group = ID, colour = Wind.WMO), alpha = 0.5, size = 0.8) + xlim(x.range) + ylim(y.range) + labs(x = "", y = "")+ theme(panel.background = element_rect(fill = "gray10", colour = "gray30"), axis.text.x = element_blank(), axis.text.y = element_blank(), axis.ticks = element_blank(), panel.grid.major = element_blank(), panel.grid.minor = element_blank())+ ggtitle(label = "飓风 2010 - 2014")hurricane.map 按月绘制 12345678910111213141516171819hurricane.map.month = ggplot(hurricane.after.2010, aes(x = Longitude, y = Latitude, group = ID)) + geom_polygon(data = map_data("world"), aes(x = long, y = lat, group = group), fill = "gray25", colour = "gray10", size = 0.2) + geom_path(data = hurricane.after.2010, aes(group = ID, colour = Wind.WMO), alpha = 0.5, size = 0.8) + xlim(x.range) + ylim(y.range) + labs(x = "", y = "")+ facet_wrap(~Month)+ theme(panel.background = element_rect(fill = "gray10", colour = "gray30"), axis.text.x = element_blank(), axis.text.y = element_blank(), axis.ticks = element_blank(), panel.grid.major = element_blank(), panel.grid.minor = element_blank())+ ggtitle(label = "飓风 2010 - 2014/月")hurricane.map.month]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[San Francisco过去3月的犯罪案件可视化分析]]></title>
      <url>%2F2014%2F10%2F07%2F2014-10-07-SF-crime%2F</url>
      <content type="text"><![CDATA[目标分析San Francisco的犯罪案件的模式。数据来自:https://data.sfgov.org/Public-Safety/SFPD-Incidents-Previous-Three-Months/tmnf-yvry? 希望回答以下问题: 在哪里停车最危险? SF最安全的地方是哪里? 每周的哪天/哪个时间最危险? 某种特定偷窃案件是否在某个区域更普遍? 准备分析用的包我们用dplyr来整理数据，ggplot2以及ggmap来进行数据可视化 1require(dplyr) 1require(ggplot2) 1## Loading required package: ggplot2 1require(ggmap) 1## Loading required package: ggmap 1require(ggthemes) 1## Loading required package: ggthemes 准备数据先设置工作目录，读入数据。然后呢把日期格式化，同时呢我们把时间按照小时来分，不考虑分钟。 1234567891011setwd("d:/project/datascience/teamleada")#read in the datacrime=read.csv("./SFPD_Incidents_-_Previous_Three_Months.csv")# format the datacrime$Location=NULLcrime$IncidntNum=NULLcrime$Date=as.Date(crime$Date,format="%m/%d/%Y")crime$Hour=as.factor(substr(as.character(crime$Time),1,2))# we will use dplyr package to do the workcrime.df=tbl_df(crime)crime.df 12345678910111213141516## Source: local data frame [30,760 x 11]## ## Category Descript DayOfWeek## 1 LARCENY/THEFT GRAND THEFT FROM UNLOCKED AUTO Sunday## 2 LARCENY/THEFT GRAND THEFT FROM LOCKED AUTO Sunday## 3 LARCENY/THEFT GRAND THEFT FROM LOCKED AUTO Sunday## 4 DRUG/NARCOTIC POSSESSION OF METH-AMPHETAMINE Sunday## 5 DRUG/NARCOTIC POSSESSION OF COCAINE Sunday## 6 LARCENY/THEFT GRAND THEFT FROM LOCKED AUTO Sunday## 7 WARRANTS WARRANT ARREST Sunday## 8 VEHICLE THEFT STOLEN AUTOMOBILE Sunday## 9 LARCENY/THEFT PETTY THEFT OF PROPERTY Sunday## 10 ROBBERY ROBBERY OF A CHAIN STORE WITH BODILY FORCE Sunday## .. ... ... ...## Variables not shown: Date (date), Time (fctr), PdDistrict (fctr),## Resolution (fctr), Address (fctr), X (dbl), Y (dbl), Hour (fctr) 辅助函数这里主要有两个，因为在画直方图的时候，我们希望按count大小来排序，而不是按照数据中的factor变量的level来排序，所以写了一个辅助函数来对factor的lvel重新排序 1234order.level=function(level.var,count) &#123; level.var=factor(level.var,levels = levels(level.var)[order(count,decreasing=T)])&#125; 另一个函数就是控制多个图形绘制时的排版，这个直接使用的R cookbook提供了函数 12345678910111213141516171819202122232425262728293031323334353637383940414243444546## Multiple plot function## ggplot objects can be passed in ..., or to plotlist (as a list of ggplot objects)# - cols: Number of columns in layout# - layout: A matrix specifying the layout. If present, 'cols' is ignored.## If the layout is something like matrix(c(1,2,3,3), nrow=2, byrow=TRUE),# then plot 1 will go in the upper left, 2 will go in the upper right, and# 3 will go all the way across the bottom.#multiplot &lt;- function(..., plotlist=NULL, file, cols=1, layout=NULL) &#123; require(grid) # Make a list from the ... arguments and plotlist plots &lt;- c(list(...), plotlist) numPlots = length(plots) # If layout is NULL, then use 'cols' to determine layout if (is.null(layout)) &#123; # Make the panel # ncol: Number of columns of plots # nrow: Number of rows needed, calculated from # of cols layout &lt;- matrix(seq(1, cols * ceiling(numPlots/cols)), ncol = cols, nrow = ceiling(numPlots/cols)) &#125; if (numPlots==1) &#123; print(plots[[1]]) &#125; else &#123; # Set up the page grid.newpage() pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout)))) # Make each plot, in the correct location for (i in 1:numPlots) &#123; # Get the i,j matrix positions of the regions that contain this subplot matchidx &lt;- as.data.frame(which(layout == i, arr.ind = TRUE)) print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row, layout.pos.col = matchidx$col)) &#125; &#125;&#125; 在哪里停车?这里我们先只取VEHICLE THEFT进行分析，之后按照案件发生的区域分组统计 12345678910111213141516171819crime.df.vehicle=filter(crime.df,Category=="VEHICLE THEFT")crime.vehicle.district=crime.df.vehicle %&gt;% group_by(PdDistrict)%&gt;% summarise(count=n())# PdDistrict变量按照count进行排序crime.vehicle.district$PdDistrict=order.level(crime.vehicle.district$PdDistrict, crime.vehicle.district$count)p1=qplot(crime.vehicle.district$PdDistrict,data=crime.vehicle.district, weight=crime.vehicle.district$count,geom="histogram", xlab="District",ylab="Vehicle Theft Count")# 经纬度crime.df$lon=crime.df$Xcrime.df$lat=crime.df$Y# 获得San Francisco地图SFMap &lt;- qmap('San Francisco', zoom = 13, color = 'bw', legend = 'topleft') 1234## Map from URL : http://maps.googleapis.com/maps/api/staticmap?center=San+Francisco&amp;zoom=13&amp;size=%20640x640&amp;scale=%202&amp;maptype=terrain&amp;sensor=false## Google Maps API Terms of Service : http://developers.google.com/maps/terms## Information from URL : http://maps.googleapis.com/maps/api/geocode/json?address=San+Francisco&amp;sensor=false## Google Maps API Terms of Service : http://developers.google.com/maps/terms 1234567891011121314# 绘制密度图p2=SFMap+ geom_density2d(data=crime.df.vehicle,aes(group=1))+ stat_density2d(data=crime.df.vehicle,aes(group=1,fill=..level.., alpha=..level..), size=0.01,bins=16,geom='polygon')+ scale_fill_gradient(low="green",high="red")+ scale_alpha(range=c(0.00,0.25),guide=FALSE)+ theme(legend.position="none",axis.title=element_blank(), text=element_text(size=12))+ ggtitle("Vehicle Theft")multiplot(p1, p2, cols=1) 1## Loading required package: grid 1## Error: object &apos;lon&apos; not found 显然Ingleside,Mission,Bayview去案件更多，密度图也反映了相同信息 SF最安全的地方是哪里? 每周的哪天/哪个时间最危险?先按照每个区计算案件数目 123456crime.by.district=crime.df %&gt;% group_by(PdDistrict)%&gt;%summarise(count=n())crime.by.district$PdDistrict=order.level(crime.by.district$PdDistrict, crime.by.district$count)p1=qplot(crime.by.district$PdDistrict,data=crime.by.district, weight=crime.by.district$count,geom="histogram", xlab="District",ylab="# of Crimes",main="# of Crimes by District") 根据每天来统计 123456crime.by.day=crime.df %&gt;% group_by(DayOfWeek)%&gt;%summarise(count=n())crime.by.day$DayOfWeek=order.level(crime.by.day$DayOfWeek, crime.by.day$count)p2=qplot(crime.by.day$DayOfWeek,data=crime.by.day, weight=crime.by.day$count,geom="histogram", xlab="Day of Week",ylab="# of Crimes",main="# of Crimes by Day") 按照犯罪时间统计 12345678crime.by.time=crime.df %&gt;% group_by(Hour)%&gt;%summarise(count=n())crime.by.time$Hour=order.level(crime.by.time$Hour, crime.by.time$count)p3=qplot(crime.by.time$Hour,data=crime.by.time, weight=crime.by.time$count,geom="histogram", xlab="Time",ylab="# of Crimes",main="# of Crimes by Time")multiplot(p1, p2,p3 ,cols=2) Richmond和Park区域是最安全的地方，案件什么少于了200.而案件发生的日期显然没有太明显规律，周一到周日基本都在4000左右。相反案件发生时间有高峰期和低点，18:00pm~19:00pm(2087),17:00~18:00pm(2022),19:00pm~20:00pm(1838)是典型的高峰，而04:00am~06:00am(&lt;400)则是低点. 某种特定偷窃案件是否在某个区域更普遍?12345678910111213theft.filter=grep("THEFT",as.character(crime.df$Category))crime.by.theft=crime.df[theft.filter,]# get the crime by district and categorycrime.by.group=crime.by.theft %&gt;% group_by(PdDistrict,Category)%&gt;%summarise(count=n())%&gt;%arrange(Category,desc(count))# sum the count by crime categorycrime.by.number=crime.by.theft%&gt;%group_by(Category)%&gt;%summarise(count=n())%&gt;%arrange(desc(count))ggplot(crime.by.group, aes(x=Category,fill=PdDistrict,weight=count))+ geom_histogram(position="fill",col=gray(0.2))+ ylab(label="Ratio by District")+ theme_tufte() larcen/theft 在Southern和 Central 区域更普遍，而vehicle theft则在Ingleside,Misson 和 Tenderloin区域更普遍。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[R语言数据挖掘与机器学习实战培训]]></title>
      <url>%2F2014%2F09%2F28%2F2014-09-28-R%E8%AF%AD%E8%A8%80%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%2F</url>
      <content type="text"><![CDATA[计划整理一个R语言数据挖掘与机器学习实战的培训，大致规划内容如下： 机器学习简介 机器学习起源 什么是机器学习 机器学习的分类 R提供的机器学习工具 利用R管理与统计数据 R数据类型：Vector,Factor,List,Data Frame R数据管理：数据读入，存储，R访问数据库 探索与理解数据 数据结构 中心趋势，分布 可视化：boxplot,histogram 分组变量 变量关系 dplyr/tidyR 回归 线性回归简介 实战：预测房价，预测酒的质量 作业：UCI机器学习库中选择一个数据集利用回归方法进行预测，提交报告以及代码 kNN 最近邻居分类 kNN原理介绍 实战讲解：利用kNN诊断乳腺癌 作业：UCI机器学习库中选择一个分类数据集利用kNN进行分类，提交报告以及代码 朴素贝叶斯分类 贝叶斯算法简介 实战讲解：利用朴素贝叶斯进行垃圾短信判断 作业：UCI机器学习库中选择一个分类数据集利用朴素贝叶斯进行分类，提交报告以及代码 决策树 决策树原理介绍 实战讲解：信用卡坏账判断 作业：UCI机器学习库中选择一个数据集利用决策树进行分类，提交报告以及代码 Classification Rules Classification Rule原理 实战：判断哪些蘑菇是毒蘑菇 作业：UCI机器学习库中选择一个数据集利用classification rule进行分类，提交报告以及代码 逻辑回归 逻辑回归原理 实战：不同法官，人种死刑判断 作业：UCI机器学习库选择一个数据集利用逻辑回归进行预测，提交报告以及代码 分类模型评估 confusion矩阵 Kappa统计 Sensitivity/Specificity ROC曲线 CV以及Bootstrap 神经网络 神经网络原理简介 实战：利用神经网络判断水泥强度 作业：UCI机器学习库中选择一个数据集利用神经网络进行预测，提交报告以及代码 支持向量机（SVM） 支持向量机简介 实战：利用SVM识别手写字母 作业：UCI机器学习库中选择一个数据集利用SVM进行预测，提交报告以及代码 关联(association rule) Apriori简介 实战：利用商场销售记录来获得用户购买模式 作业：UCI机器学习库中选择一个数据集进行market basket分析，提交报告以及代码 聚类（k-means) k-means简介 实战：对社交用户进行聚类分析 作业：UCI机器学习库中选择一个数据集进行clustering分析，提交报告以及代码 文本分析 tm包介绍 实战：这是谁的发言，奥巴马还是罗姆尼 作业：微博文本分析 模型优化 模型参数调优 背包(Bagging) Boosting 随机森林(random forest) 作业: Kaggle上的自行车租用预测竞赛 几点说明问：什么时候开始？ 目前不确定，如果有兴趣的人能达到50人就考虑吧。 问：收费吗？ 或者您可以考虑来做一个免费的？ 目前的打算：也许会考虑预收点，39～49待定（已经够低了吧，呜呜），或者最后会退，如果你完成作业很好的话。原因： 之前做免费R数据分析培训的教训，大家报名的时候积极，讲了4～5次，交作业之类就不积极了，所以可能会考虑一个措施来解决这个问题，因为作业比较多，都自己动手才能真的学有所得 如果真准备这个培训，花的准备时间肯定不少，算是劳动的一点点回报吧。而且收这点费用&lt;&lt;&lt;我干别的事情获得的收入。 你可以先听2～3次后，再做决定是否继续听 问：MOOC上有很多免费课程，为何还要准备这个课程 这个与MOOC上的课有些地方有相似，不过目标稍有不同。主要目标是保证R的初学者能较快的掌握利用R来进行机器学习，会比Coursera上的机器学习课更偏向实际应用，同时比约翰霍普金斯的数据科学系列课程中的R编程，以及机器学习更详细，深入一些。如果你的自学能力还好，个人觉得可以直接听免费的MOOC课。 问：如何组织 打算考虑采用QQ群视频方式进行。计划每周1次课，每次课完了后有作业，安排一次作业讨论。 问：如果我有兴趣，怎么报名 目前还没有确定会进行，可以先加入群 397066090，如果感兴趣的人多再说。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Kaggle上的泰坦尼克生还数据分析]]></title>
      <url>%2F2014%2F09%2F17%2F2014-09-17-kaggle-titanic%2F</url>
      <content type="text"><![CDATA[数据准备先根据数据的codebook来给每列命名，同时预先设定类型 123456789101112131415161718setwd("d:/course/kaggle/titanic/")train.col.types &lt;- c('integer', # PassengerId 'factor', # Survived 'factor', # Pclass 'character', # Name 'factor', # Sex 'numeric', # Age 'integer', # SibSp 'integer', # Parch 'character', # Ticket 'numeric', # Fare 'character', # Cabin 'factor' # Embarked)test.col.types=train.col.types[-2]train.raw=read.csv("./train.csv",colClasses=train.col.types,na.strings=c("NA",""))test.raw=read.csv("./test.csv",colClasses=test.col.types,na.strings=c("NA",""))dim(train.raw) 1## [1] 891 12 1dim(test.raw) 1## [1] 418 11 这里预先设定了每列的class，之后读入数据。train数据集为891行12列大小矩阵，测试数据集418行，11列（没有了是否生还的列） 数据探索1summary(train.raw) 123456789101112131415161718192021222324## PassengerId Survived Pclass Name Sex ## Min. : 1 0:549 1:216 Length:891 female:314 ## 1st Qu.:224 1:342 2:184 Class :character male :577 ## Median :446 3:491 Mode :character ## Mean :446 ## 3rd Qu.:668 ## Max. :891 ## ## Age SibSp Parch Ticket ## Min. : 0.42 Min. :0.000 Min. :0.000 Length:891 ## 1st Qu.:20.12 1st Qu.:0.000 1st Qu.:0.000 Class :character ## Median :28.00 Median :0.000 Median :0.000 Mode :character ## Mean :29.70 Mean :0.523 Mean :0.382 ## 3rd Qu.:38.00 3rd Qu.:1.000 3rd Qu.:0.000 ## Max. :80.00 Max. :8.000 Max. :6.000 ## NA&apos;s :177 ## Fare Cabin Embarked ## Min. : 0.0 Length:891 C :168 ## 1st Qu.: 7.9 Class :character Q : 77 ## Median : 14.5 Mode :character S :644 ## Mean : 32.2 NA&apos;s: 2 ## 3rd Qu.: 31.0 ## Max. :512.3 ## 1summary(test.raw) 123456789101112131415161718192021222324## PassengerId Pclass Name Sex Age ## Min. : 892 1:107 Length:418 female:152 Min. : 0.17 ## 1st Qu.: 996 2: 93 Class :character male :266 1st Qu.:21.00 ## Median :1100 3:218 Mode :character Median :27.00 ## Mean :1100 Mean :30.27 ## 3rd Qu.:1205 3rd Qu.:39.00 ## Max. :1309 Max. :76.00 ## NA&apos;s :86 ## SibSp Parch Ticket Fare ## Min. :0.000 Min. :0.000 Length:418 Min. : 0.0 ## 1st Qu.:0.000 1st Qu.:0.000 Class :character 1st Qu.: 7.9 ## Median :0.000 Median :0.000 Mode :character Median : 14.5 ## Mean :0.447 Mean :0.392 Mean : 35.6 ## 3rd Qu.:1.000 3rd Qu.:0.000 3rd Qu.: 31.5 ## Max. :8.000 Max. :9.000 Max. :512.3 ## NA&apos;s :1 ## Cabin Embarked## Length:418 C:102 ## Class :character Q: 46 ## Mode :character S:270 ## ## ## ## 这里看出Age有177个缺失，占比比较大，而对于character变量Cabin，Name等没有相关信息，一个简单的查看缺失数据的方法就是采用missamp()函数 123require(Amelia)missmap(train.raw, main="Titanic缺失数据图", col=c("yellow", "black"), legend=FALSE) 这里看出Cabin，Age有较多缺失数据，而Embark只有两个缺失数据。 接下来先简单分析一下数据，比如生还与死亡的比例，不同等级(Pclass)对生还有无影响，我们知道老人，妇女，小孩会优先所以看看性别，年龄是否有影响，此外票的价钱有关吗（fare）？当然可能还有其它的变量，这里我们就不一一列举… 生还与死亡对比12barplot(table(train.raw$Survived),names.arg=c("死亡","生还"), main="生还 vs 死亡") 不同舱位等级的影响12345survive.rate.class=table(train.raw$Survived,train.raw$Pclass)barplot(survive.rate.class,names.arg=c("一等","二等","三等"), main="不同舱位生还 vs 死亡", legend.text=c("死亡","生还"), args.legend=list(x="topleft")) 1round((survive.rate.class[2,]/colSums(survive.rate.class))*100,2) 12## 1 2 3 ## 62.96 47.28 24.24 看起来一等舱，二等舱显然比三等舱要高。如果按照比例来看各舱位生还的比例依次为：62.96%,47.28,24.24%。除了舱位等级好的在越上面，跑得快，下面的先被淹外，估计这个也和当时发生事故的时间在睡觉有关系。 不同性别的生还率对比12345survive.rate.sex=table(train.raw$Survived,train.raw$Sex)barplot(survive.rate.sex,names.arg=c("女","男"), main="不同性别生还 vs 死亡", legend.text=c("死亡","生还"), args.legend=list(x="topleft")) 1round((survive.rate.sex[2,]/colSums(survive.rate.sex))*100,2) 12## female male ## 74.20 18.89 看来在生与死的选择时男人们还是发扬了高风亮节！女性生还率达到了74.2%，男性只有18.89% 不同年龄的生还率12345678age.breaker=c(0,18,50,100)age.cut= cut(train.raw$Age,breaks=age.breaker,,labels=c("小孩","成年人","老人"))train.raw$age.cut=age.cutsurvive.rate.age=table(train.raw$Survived,train.raw$age.cut)barplot(survive.rate.age, main="不同年龄生还 vs 死亡", legend.text=c("死亡","生还"), args.legend=list(x="topleft")) 1round((survive.rate.age[2,]/colSums(survive.rate.age))*100,2) 12## 小孩 成年人 老人 ## 50.36 38.75 34.38 12345678age.breaker=c(0,15,55,100)age.cut= cut(train.raw$Age,breaks=age.breaker,,labels=c("小孩","成年人","老人"))train.raw$age.cut=age.cutsurvive.rate.age=table(train.raw$Survived,train.raw$age.cut)barplot(survive.rate.age, main="不同年龄生还 vs 死亡", legend.text=c("死亡","生还"), args.legend=list(x="topleft")) 1round((survive.rate.age[2,]/colSums(survive.rate.age))*100,2) 12## 小孩 成年人 老人 ## 59.04 38.75 30.00 这里分别用15和18来作为小孩的判断标准，50，55分别作为老人判断标准。可以看出小孩的生还率分别为59.04%与50.36%，生还率最高。而老人的生还率却比成年人低，也许是年龄大了的影响？ 马赛克图还有一种更好的方法就是利用mosaicplot图 123mosaicplot(train.raw$Pclass ~ train.raw$Survived, main="不同舱位等级生还 vs 死亡 ", shade=FALSE, color=TRUE, xlab="舱位", ylab="生还") 123mosaicplot(train.raw$Sex ~ train.raw$Survived, main="不同性别生还 vs 死亡 ", shade=FALSE, color=TRUE, xlab="性别", ylab="生还") 相关性分析前面大致可以得出性别，年龄，舱位对生还率有很大影响，那么其他的票价，上船的港口，在那个cabin等等变量对生还率有影响吗？这里用相关性分析来观察一下： 12345678train.corrgram = train.raw# 相关性分析要求全部为数字train.corrgram$Survived &lt;- as.numeric(train.corrgram$Survived)train.corrgram$Pclass &lt;- as.numeric(train.corrgram$Pclass)train.corrgram$Embarked &lt;- as.numeric(train.corrgram$Embarked)train.corrgram$Sex &lt;- as.numeric(train.corrgram$Sex)train.corrgram[which(is.na(train.corrgram$Embarked)),]$Embarked=3cor(train.corrgram[,corrgram.vars]) 1## Error: object &apos;corrgram.vars&apos; not found 这里大致可以看出除了Pclass,Sex,Age以为，Fare还和生还率有关，而Embarked似乎也有一定关系（虽然理论上来讲登船地点应该与这个无关？）而Age由于有大量缺失值这里相关性为NA,下一步的目标似乎应该是在于如何处理这些缺失的数据上面。 图形分析也许更简单快捷，直接使用corrgram来分析。 1require(corrgram) 12## Loading required package: corrgram## Loading required package: seriation 12345# 字符变量这里不考虑corrgram.vars = c("Survived", "Pclass", "Sex", "Age", "SibSp", "Parch", "Fare", "Embarked")corrgram(train.corrgram[,corrgram.vars], lower.panel=panel.ellipse, upper.panel=panel.pie,text.panel=panel.txt, main="泰坦尼克生还率相关性分析") 初次建模有了前面的探索性分析，大致我们对数据有了一定了解，考虑先建立一个模型来进行一次初步预测。显然我们需要处理的问题是一个分类为题，首先想到的方法有逻辑回归，决策树，classification rule，更高级的可以考虑采用boost，随机森林。这里我们先考虑从简单的逻辑回归开始。 构建训练集与测试集1require(caret) 12345678910## Loading required package: caret## Loading required package: lattice## ## Attaching package: &apos;lattice&apos;## ## The following object is masked from &apos;package:seriation&apos;:## ## panel.lines## ## Loading required package: ggplot2 12345set.seed(201409)inTrain = createDataPartition(train.raw$Survived, p = 0.8, list = FALSE)training = train.raw[inTrain, ]test = train.raw[-inTrain, ] 第一个模型互联网时代来临之前，人们的收入通常和年龄相关，因此对缺失数据较多的年龄我们先根据不同等级来填充一个估计值。这里直接使用median。 另外这里还计算了3等舱的价格的median，因为我们发现测试集里面有一个记录的Fare值缺失。 1234567891011first.class.age=median(training[training$Pclass=="1",]$Age,na.rm=T)second.class.age=median(training[training$Pclass=="2",]$Age,na.rm=T)third.class.age=median(training[training$Pclass=="1",]$Age,na.rm=T)training[is.na(training$Age)&amp;training$Pclass=="1",]$Age=first.class.agetraining[is.na(training$Age)&amp;training$Pclass=="2",]$Age=second.class.agetraining[is.na(training$Age)&amp;training$Pclass=="3",]$Age=third.class.agethird.class.fare=median(training[training$Pclass=="3",]$Fare,na.rm=T)model.logit.1 &lt;- train(Survived ~ Sex + Pclass + Age + Embarked + Fare, data = training, method="glm")model.logit.1 1234567891011121314151617## Generalized Linear Model ## ## 714 samples## 12 predictors## 2 classes: &apos;0&apos;, &apos;1&apos; ## ## No pre-processing## Resampling: Bootstrapped (25 reps) ## ## Summary of sample sizes: 712, 712, 712, 712, 712, 712, ... ## ## Resampling results## ## Accuracy Kappa Accuracy SD Kappa SD## 0.8 0.6 0.02 0.05 ## ## 模型accuracy为0.8,kappa 为0.6，采用了bootstrap 采样方式。作为我们的第一个模型看起来还不错 123456789first.class.age=median(test[test$Pclass=="1",]$Age,na.rm=T)second.class.age=median(test[test$Pclass=="2",]$Age,na.rm=T)third.class.age=median(test[test$Pclass=="1",]$Age,na.rm=T)test[is.na(test$Age)&amp;test$Pclass=="1",]$Age=first.class.agetest[is.na(test$Age)&amp;test$Pclass=="2",]$Age=second.class.agetest[is.na(test$Age)&amp;test$Pclass=="3",]$Age=third.class.agepredict.model.1=predict(model.logit.1,test)table(test$Survived,predict.model.1) 1234## predict.model.1## 0 1## 0 91 18## 1 22 46 1sensitivity(test$Survived,predict.model.1) 1## [1] 0.8053 1specificity(test$Survived,predict.model.1) 1## [1] 0.7188 12#require(gmodels)#CrossTable(test$Survived,predict.model.1) 相对我们自己构建的测试集sensitivity,specificity分别为0.8,0.71也还行。 1require(ROCR) 12345678910## Loading required package: ROCR## Loading required package: gplots## KernSmooth 2.23 loaded## Copyright M. P. Wand 1997-2009## ## Attaching package: &apos;gplots&apos;## ## The following object is masked from &apos;package:stats&apos;:## ## lowess 1234predictions.model.1=prediction(c(predict.model.1),labels=test$Survived)perf = performance(predictions.model.1, measure = "tpr", x.measure = "fpr")plot(perf, main = "ROC curve",col = "blue", lwd = 2)abline(a = 0, b = 1, lwd = 2, lty = 2) 预测12345678910first.class.age=median(test.raw[test.raw$Pclass=="1",]$Age,na.rm=T)second.class.age=median(test.raw[test.raw$Pclass=="2",]$Age,na.rm=T)third.class.age=median(test.raw[test.raw$Pclass=="1",]$Age,na.rm=T)test.raw[is.na(test.raw$Age)&amp;test.raw$Pclass=="1",]$Age=first.class.agetest.raw[is.na(test.raw$Age)&amp;test.raw$Pclass=="2",]$Age=second.class.agetest.raw[is.na(test.raw$Age)&amp;test.raw$Pclass=="3",]$Age=third.class.agetest.raw[153,]$Fare=third.class.farepredict.final.model.1=predict(model.logit.1,newdata=test.raw)predictions=data.frame(PassengerId=test.raw$PassengerId, Survived=predict.final.model.1)write.csv(predictions,file="Titanic_predictions_1.csv", row.names=FALSE, quote=FALSE) 提交到Kaggle上，模型accuracy显示为0.77左右，排在了2000名左右的位置，显然这不是一个好的结果。不过到此为止我们还有很多事情没有做： 有4个特征没有使用分别是sibsp,parch,Cabin,names。 也许家人的数目会影响生还率，比如需要照顾家人或者上救生船的时候会让别的家庭成员上等等 Cabin这个变量虽然有很多缺失但是理论上应该也是一个关键特征，因为不同舱位对逃生影响很大 names看似没有用但是似乎它们都满足了 xx Title, xx xx这样一种模式，而西方人的title可以反映年龄。因此可能我们需要利用names来构建一个新变量Title 对Age的插值之前我们采用了直接取不同等级舱位年龄median的方式完成，这是否是最佳，可否有更好方法？比如可以考虑根据姓名的Title来判断 前面的模型参数我们没有进行任何的优化，也许优化参数可以取得更好结果？ 除了逻辑回归，我们还可以采用决策树，随机森林，boosting方法，是否这些模型会带来更好结果？ 这些就是下一步我们可以考虑的地方了。 模型优化先重新读入一次数据： 12345678910111213141516train.col.types &lt;- c('integer', # PassengerId 'factor', # Survived 'factor', # Pclass 'character', # Name 'factor', # Sex 'numeric', # Age 'integer', # SibSp 'integer', # Parch 'character', # Ticket 'numeric', # Fare 'character', # Cabin 'factor' # Embarked)test.col.types=train.col.types[-2]train.raw=read.csv("./train.csv",colClasses=train.col.types,na.strings=c("NA",""))test.raw=read.csv("./test.csv",colClasses=test.col.types,na.strings=c("NA","")) 数据整理与清洗首先考虑前面提到的Names中的title.西方人的命名都有一定规则，首先关于Title： Mr. 更个年龄的先生 Miss. 未婚女士（不过女权解放的现代，好像也有已婚的用？） Mrs. (已婚女士) Master. (男孩，早期使用，现在应该不用了。不过泰坦尼克灾难发生时适用) Rev., Col. Sir. Dr. etc… 等大多与职业相关，应该都是男士 而对西方的名字，比如： Baclini, Mrs. Solomon (Latifa Qurban) Mrs. 已婚 Solomon ：丈夫的名字,比如Jane Smith与John Smith结婚后，她就是Mrs. John Smith. Latifa : 他自己的名字 Qurban ：”maiden” ，结婚前的姓 Baclini:她丈夫的姓，也就是他丈夫Solomon的姓. 有了这些基础那么我们可以想法把中间的那个Title给取出来： 123456789#这里格式是xx, Title . xx xxgetTitle = function(data) &#123; title.start = regexpr("\\,[A-Z ]&#123;1,20&#125;\\.", data$Name, TRUE) title.end = title.start+attr(title.start, "match.length")-1 data$Title = substr(data$Name, title.start+2, title.end-1) return (data$Title)&#125;train.raw$Title=getTitle(train.raw)require(dplyr) 1234567891011## Loading required package: dplyr## ## Attaching package: &apos;dplyr&apos;## ## The following objects are masked from &apos;package:stats&apos;:## ## filter, lag## ## The following objects are masked from &apos;package:base&apos;:## ## intersect, setdiff, setequal, union 1head(train.raw %&gt;% group_by(Title) %&gt;% summarise(count=n())%&gt;%arrange(desc(count))) 123456789## Source: local data frame [6 x 2]## ## Title count## 1 Mr 517## 2 Miss 182## 3 Mrs 125## 4 Master 40## 5 Dr 7## 6 Rev 6 主要的称呼为Mr,Miss,Mrs,Master，剩余的类型都比较少，可以归入professional一类。因此我们增加一列Title，分为Mr,Miss,Mrs,Master,Professional 5种类型。 1234567title.filter=c("Mr","Mrs","Miss","Master","Professional")recodeTitle = function(data,title.filter) &#123; if(!(data %in% title.filter)) data = "Professional" return (data)&#125;train.raw$Title=sapply(train.raw$Title,recodeTitle,title.filter) 简单利用median来对年龄进行插值,Embarked的NA直接使用“S”,fare根据class的median来计算 12345678910111213141516171819202122232425imputeAge = function(Age,Title,title.filter) &#123; for(v in title.filter) &#123; Age[is.na(Age)]=median(Age[Title==v],na.rm=T) &#125; return(Age)&#125;title.filter=c("Mr","Mrs","Miss","Master","Professional")train.raw$Age=imputeAge(train.raw$Age,train.raw$Title,title.filter)imputeEmbarked=function(Embarked) &#123; Embarked[is.na(Embarked)]="S" return(Embarked)&#125;train.raw$Embarked=imputeEmbarked(train.raw$Embarked)imputeFare = function(fare,pclass,pclass.filter) &#123; for(v in pclass.filter) &#123; fare[is.na(fare)]=median(fare[pclass==v],na.rm=T) &#125; return(fare)&#125;pclass.filter=c(1,2,3)train.raw$fare=imputeFare(train.raw$Fare,train.raw$Pclass,pclass.filter) 第二个模型现在使用了新的计算确实年龄的方法，同时加上SibSp以及Parch变量建立一个新模型 12345678require(caret)set.seed(201409)inTrain = createDataPartition(train.raw$Survived, p = 0.8, list = FALSE)training = train.raw[inTrain, ]test = train.raw[-inTrain, ]model.logit.2 &lt;- train(Survived ~ Sex + Pclass + Age + Embarked + Fare+Title+SibSp+Parch,data = training, method="glm")model.logit.2 1234567891011121314151617## Generalized Linear Model ## ## 714 samples## 13 predictors## 2 classes: &apos;0&apos;, &apos;1&apos; ## ## No pre-processing## Resampling: Bootstrapped (25 reps) ## ## Summary of sample sizes: 714, 714, 714, 714, 714, 714, ... ## ## Resampling results## ## Accuracy Kappa Accuracy SD Kappa SD## 0.8 0.6 0.02 0.05 ## ## 12predict.model.2=predict(model.logit.2,test)table(test$Survived,predict.model.2) 1234## predict.model.2## 0 1## 0 93 16## 1 17 51 1sensitivity(test$Survived,predict.model.2) 1## [1] 0.8455 1specificity(test$Survived,predict.model.2) 1## [1] 0.7612 1predict(model.logit.2,test) 1234567## [1] 0 1 1 0 1 0 0 0 0 1 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 1 0 1 0 0## [36] 1 0 0 0 1 1 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1 1 0 1 0 1 1 1 0 1 1 0## [71] 0 1 0 1 0 1 1 1 0 0 1 0 0 0 1 1 0 0 0 0 1 0 0 0 1 0 0 0 1 1 1 0 0 0 1## [106] 1 0 0 1 1 0 0 0 0 0 0 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0## [141] 1 0 0 1 0 1 1 0 1 1 0 0 0 1 0 0 1 1 1 0 1 0 1 1 0 0 1 0 0 1 0 1 1 1 0## [176] 0 0## Levels: 0 1 训练集accuracy达到0.82，对于测试集sensitivity,specificity都达到了有提高 预测123456789title.filter=c("Mr","Mrs","Miss","Master","Professional")test.raw$Title=getTitle(test.raw)test.raw$Title=sapply(test.raw$Title,recodeTitle,title.filter)test.raw$Age=imputeAge(test.raw$Age,test.raw$Title,title.filter)test.raw$Embarked=imputeEmbarked(test.raw$Embarked)test.raw$Fare=imputeFare(test.raw$Fare,test.raw$Pclass,pclass.filter)predict.final.model.2=predict(model.logit.2,newdata=test.raw)predictions=data.frame(PassengerId=test.raw$PassengerId, Survived=predict.final.model.2)write.csv(predictions,file="Titanic_predictions_2.csv", row.names=FALSE, quote=FALSE) 提交到kaggle，新的排名一下提高的了800多位，一下子比之前提高了1600多位。回到之前我们提到的可能优化方向： 有4个特征没有使用分别是sibsp,parch,Cabin,names。 也许家人的数目会影响生还率，比如需要照顾家人或者上救生船的时候会让别的家庭成员上等等 Cabin这个变量虽然有很多缺失但是理论上应该也是一个关键特征，因为不同舱位对逃生影响很大 names看似没有用但是似乎它们都满足了 xx Title, xx xx这样一种模式，而西方人的title可以反映年龄。因此可能我们需要利用names来构建一个新变量Title 对Age的插值之前我们采用了直接取不同等级舱位年龄median的方式完成，这是否是最佳，可否有更好方法？比如可以考虑根据姓名的Title来判断 前面的模型参数我们没有进行任何的优化，也许优化参数可以取得更好结果？ 除了逻辑回归，我们还可以采用决策树，随机森林，boosting方法，是否这些模型会带来更好结果？ 现在还可以做的有: Cabin这个变量虽然有很多缺失但是理论上应该也是一个关键特征，因为不同舱位对逃生影响很大 前面的模型参数我们没有进行任何的优化，也许优化参数可以取得更好结果？ 除了逻辑回归，我们还可以采用决策树，随机森林，boosting方法，是否这些模型会带来更好结果？ 此外前面的代码中的插值函数显然可以重构成一个通用函数就不用每个特征写一个函数了，不过今天先到这里下一步的优化我们就下次再写了。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[数据科学(9)-第九章：Rstudio]]></title>
      <url>%2F2014%2F09%2F08%2F2014-9-data-science-chp9-Rstudio%2F</url>
      <content type="text"><![CDATA[本文为Introduction to Data Science一书的翻译，由网友义务完成，了解参加翻译的网友，请点击这里，如果要加入我们请加入qq群171546473,了解翻译规则点击这里 第九章 开始R-Studio 作为一个拥有活跃的用户社区的开源软件，R之所以能不断创新发展离不开社区志愿者无私贡献。其中一个非常重要的创新就能完美支持R语言副本的R-Studio框架。本章从R-Studio的安装开始，然后再详细介绍了“包”——扩展R的关键。 Joseph J. Allaire 作为一个连续创业家和软件工程师，开发了一系列著名软件，包括ColdFusion和微软博客工具Windows Live Writer，其中ColdFusion后来被卖给了网络媒体工具巨头Macromedia。从2009年开始，Allaire和一个小团队开始开发一款开源软件，用来提高R的可用性和效率。 如同前面章节所述，作为一门开源语言，R可以创建Mac，Windows，或者Linux平台下的副本，并且允许所有人检查和修改。同其它开源项目一样，社区里有大量活跃的志愿者致力于R语言的改善，包括语言本身，以及为R添加新的内容。其中一个新添加的内容就是R-Studio。R-Studio是一种集成开发环境（IDE，Integrated Development Environment）。每一个软件工程师都知道IDE在软件开发过程中的重要性。你可以把R想象成地板上卷起的一块帆布，而R-Studio则像一个优美的相框。当你把R放在R-Studio中后，就像任何好的相框一样，R-Studio能让R显得更加优美，具有鉴赏性。 你可以在任何时候登陆R-Studio官网 http://www.rstudio.org 来查看有关信息。在本章剩下的部分中，你需要一台装有Mac、或Windows、或Linux系统的电脑来跟着学习R-Studio的安装和使用。 在开始学习R-Studio之前，我们先来了解为什么我们在使用R的时候需要一个IDE。在前面的章节中，我们都是通过总所周知的“R控制台”来输入各种各样的R命令。控制台是一个非常古老的技术术语，可以追溯到电脑大得还需要单独的空调室的时代。当时控制台作为“主控台”，接受各种操作命令，完成对整个巨大的计算机的控制。如今，控制台这个术语在很多场合都是指可以直接输入命令的界面。我们之前学习R语言以及一些数据结构和统计学基本原理的时候，就是通过在R控制台中输入命令来完成的。 而要想真正搞数据科学，我们也不可能整天坐在电脑面前敲命令。首先，这样很快就会变得非常枯燥乏味。其次，如果老板发现我们在重复输入昨天输过的命令，他就会开始怀疑我们的工作。最后，也是非常重要的一点，如果实现所有的小任务必须通过手动输入一些命令来完成的话，这会非常容易出错，产生计算机科学家所谓的“bug”。考虑到这些原因，我们这本书的一个目标就是要创造一些可以重复使用的东西，这样我们就可以简单点几下鼠标或是输入一些简单的东西就可以完成需要很多处理步骤的任务。使用IDE，我们就可以轻松建立这样的可以重复使用的部分。IDE整合了各个功能，用的时候打开对应部分，不用的时候将其隐藏或者关闭即可。由于我们和数据打交道，我们需要非常细致的检测这些数据，包括数据内容及其结构。你可能也已经注意到了，通过R控制台来完成这些内容非常无聊的，其输出的一大段的文字并且有的文字在你看清之前就被挤出了屏幕。作为R语言的IDE，R-Studio就允许我们控制和监视代码和文字，这样在某种程度上可以支持我们创造可重复使用的内容。 在做这些之前，首先必须在电脑上安装R-Studio。安装R-Studio之前，必须先安装R。如果你已经在第二章中完成了R的安装，剩下的R-Studio的安装就是小菜一碟。在安装R-Studio之前，你必须确保安装的R是最新的版本。R-Studio官网 http://www.rstudio.org/ 上有丰富的文档，只要跟着这些文档的说明来安装，问题应该不大。如果安装过程中要求你选择安装“R-Studio服务器版”还是“桌面应用版”，选择后者即可。稍后我们会对R-Studio服务器版本进行介绍，但是现在选择桌面版本即可。如果你还想了解额外的关于R-Studio的信息，可以参考John Verzani（2011，Sebastopol，CA：O’Reilly Media)写的书：Getting Started with R-studio。该书第一章就对R和R-Studio进行了大体介绍，也介绍了如何安装和更新R-studio。YouTube的一段视频 http://www.youtube.com/watch?v=7sAmqkZ3Be8 也对R-Studio进行了详细介绍。需要注意的是，YouTube上有一个磁盘恢复程序和一个音乐组织也都叫R-Studio，因此如果你仅仅用“R-Studio”搜索而不加其他关键词的话，你将会搜到大量和这两者有关的视频。 安装好R-Studio后，你就可以直接运行它来进行本章后面的内容。本书不会像其他学习材料一样把R-Studio界面上的每个部分都介绍一遍，而是在使用到一个特性的时候就将其突出显示出来。当你运行R-Studio时，你将会看到三到四个子窗口。点击“File”菜单下的“New”项，并在“New”的子菜单中选择“R Script”，这样你就能看到类似如下的界面： 1234MyMode &lt;- function(myVector)&#123; return(myVector)&#125; 恭喜你刚刚用R写了第一个函数。一个函数是一组可以重复使用的R代码，每次使用时就不必重新输入相同的代码，其它编程语言里也都有函数这个概念。函数也有“过程”和“子程序”的意思，这在不同编程语言里有轻微的不同。我们给刚定义的函数命名为“MyMode”。从前面多个章节可以看出，除了基本的函数mean()和median（），R的基本安装里面并没有统计学模块的函数。这需要我们自己创建统计模块函数来解决这个问题。回想那个统计一个列表里每个值出现的次数并返回出现次数最多的值的函数，那就是一个统计模块的定义：列表中出现频率最高的项。 关于函数，有几点需要注意。首先、第一行中圆括号括起来的（myVector）表示函数的“参数”，或者说输入。之前当我们调用类似mean（）和median（）等函数时就遇到过“参数”这个概念。其次、注意第二行和最后一行的大括号，在函数中，这些大括号都是成对出现的。最后、注意函数接近末尾的return（）语句，这是将函数完成结果返回的地方。一会当我们在R控制台中调用这个函数时就能看到，返回结果就是return（）语句中括号里的东西。 基于这些说明，你现在知道前面的这个MyMode（）函数做了什么了吗？这个函数其实对我们传入的参数myVector未做任何改动。顺便说一下，写代码时一种常用的方式就是这么一点一点的实现代码。这样我们可以测试每一步都能得到什么样的结果。我们现在就可以测试一下目前的函数能干什么。首先，我们创建一个小的向量。在R-studio的左下角可以看到一个常规的R控制台正在运行。像前面几章一样，在里面输入R命令： 12tinyData &lt;- c(1,2,1,2,3,3,3,4,5,4,5)tinyData 1## [1] 1 2 1 2 3 3 3 4 5 4 5 然后我们就可以试试我们写的MyMode（）函数： > MyMode(tinyData) Error: could not find function “MyMode” 噢，出错了！R还不知道我们的新函数呢。我们虽然在代码窗口输入了MyMode（）函数，但是还没有告诉R呢。查看左上角的窗口，你可以看到MyMode（）函数的代码，以及在代码上方的工具栏中有一些小的按钮，其中有一个旁边写着“Run”的向右的箭头。选中MyMode()函数的所有代码（从字面“M”到最后的大括号），然后点击Run按钮，你就会立即看到下方的R控制台中出现了相同的代码。如果所有的代码都是正确的，控制台就不会报错或者警告。点击Run按钮后，R编译器就知道了MyMode（）函数，并且准备使用了。再在控制台中输入： 1MyMode(tinyData) 1## [1] 1 2 1 2 3 3 3 4 5 4 5 程序将返回我们所期望的tinyData的内容。这个例子中，tinyData就是函数的输入参数，从这个例子我们就可以看出函数的参数是如何工作的。当函数运行时，其将tinyData的内容复制到myVector中，以方便函数中使用。现在，我们在函数中新添加一条语句： 12345MyMode &lt;- function(myVector)&#123; uniqueValues &lt;- unique(myVector) return(uniqueValues)&#125; 我们将原来的MyMode函数进行了轻微的改动，结果如上所述。我们只在原函数中加了一行代码，避免函数变得过于复杂。我们先来看看这段代码都干些什么。首先不要忘了选中代码然后点击“Run”按钮，然后在R控制台中再次输入MyMode（）命令： 1MyMode(tinyData) 1## [1] 1 2 3 4 5 代码的作用非常明显，我们调用了unique()函数，其返回在tinyData中出现的数值的列表，这个列表中每个数值只出现一次。简单的说，unique()函数就是将输入向量中多余的数值去掉。接下来我们让代码更复杂一点： 123456MyMode &lt;- function(myVector)&#123; uniqueValues &lt;- unique(myVector) uniqueCounts &lt;- tabulate(myVector) return(uniqueCounts)&#125; 仍然别忘了在使用这个函数前要选中代码然后点击“Run”。这次我们将tinyData作为输入参数传给函数，得到的返回值也是一个有5个元素的列表，但是列表的元素表示的是输入参数中每个数值出现的次数： 1MyMode(tinyData) 1## [1] 2 2 3 2 2 现在差不多该结束我们的MyMode()函数了，但是我们得确保理解了uniqueValues和uniqueCounts中的内容： 下面的表中列出了uniqueValues中的元素以及其对应的个数。 首行显示“index”的标号行仅仅是出于说明的目的。这些索引数字用来定位列表中对应的元素的。比如，uniqueValues列表中下标为4的位置值为4，其对应的uniqueCounts的值为2。这样一来，如果我们要寻找出现次数最多的数字，我们只需在最后一行中找到最大的数的下标，然后在uniqueValues列表中找到该下标对应的值即可。在R语言中，我们只需一行代码就可以实现前面描述的过程： uniqueValues[which.max(uniqueCounts)] 其中，whicn.max()函数用来找到uniqueCounts中最大数在数组中的下标，然后在uniqueValues中通过一对方括号就能访问方括号中下标对应的元素。例如uniqueValues[5]就能得到uniqueValues中的第5个数。我们把这条语句加到return语句中，来完成我们的函数： 123456MyMode &lt;- function(myVector)&#123; uniqueValues &lt;- unique(myVector) uniqueCounts &lt;- tabulate(myVector) return(uniqueValues[which.max(uniqueCounts)])&#125; 现在我们准备再次测试这个函数，注意不要忘了选中所有代码然后运行一下，否则编译器记住的只是旧的那个函数。我们先回忆一下tinyData中都有些什么元素，然后再把tinyData作为参赛传给函数： 1tinyData 1## [1] 1 2 1 2 3 3 3 4 5 4 5 1MyMode(tinyData) 1## [1] 3 可以看出，函数工作正常。tinyData数组中出现最多的数是3。我们继续测试看看函数还会发生什么： 12tinyData &lt;- c(tinyData,5,5,5)tinyData 1## [1] 1 2 1 2 3 3 3 4 5 4 5 5 5 5 1MyMode(tinyData) 1## [1] 5 我们向tinyData数组末尾中添加了三个5，现在tinyData中有五个5，MyMode()函数也执行成功。我们在继续测试： 12tinyData &lt;- c(tinyData,1,1,1)tinyData 1## [1] 1 2 1 2 3 3 3 4 5 4 5 5 5 5 1 1 1 1MyMode(tinyData) 1## [1] 1 你会发现这很奇怪，现在tinyData中明明是1和5都有五个，但是函数返回的的确是1。其实你完全不必惊讶，which.max()函数的说明文档里已经写清楚了，这个函数会返回其找到的第一个最大值。所以出现上面的结果是情理之中的事。事实上，这也总是统计学中常见的一个问题：数据集中不止一个模式。我们的MyMode()函数还无法实现这样的功能，也不能给出任何警告说明数据中出现次数最多的数不止一个，而只是简单的返回其中找到的第一个。这函数中还有另一个问题： 12tinyData &lt;- c(tinyData,9,9,9,9,9,9,9)MyMode(tinyData) 1## [1] NA 1tabulate(tinyData) 1## [1] 5 2 3 2 5 0 0 0 7 第一行代码中，我们在tinyData后面加了一连串“9”，注意tinyData中没有6,7,8。而我们运行MyMode()却返回“NA”，这在R中表示某处出错了，你得到了一个空的值。如果我们不输入最后的“tabulate(tinyData)”语句，很难看出程序为什么会出现这么怪异的行为。我们先来看看都发生了什么：在MyMode()函数中运行的时候，tabulate()产生的数组的长度不是我们想的那样———有多少个不同的数，数组就多长。这是因为tabulate()函数会将6,7,8等这些不存在的数都置为0。tabulate()返回的结果中，最大数7（代表tinyData中9出现的次数）出现在末尾；而再查看unique()函数的返回结果： 1unique(tinyData) 1## [1] 1 2 3 4 5 9 可以看到unique()函数的返回结果中只有6个元素，这和tabulate()的返回结果不匹配（你可以想象一下前面一页那个表中最后两行长度不一样的情况）。我们可以添加额外的match()函数来解决这个问题： 123456MyMode &lt;- function(myVector)&#123; uniqueValues &lt;- unique(myVector) uniqueCounts &lt;- tabulate(match(myVector,uniqueValues)) return(uniqueValues[which.max(uniqueCounts)])&#125; 新添加match(myVector,uniqueValues)保证了传入tabulate()的参数是myVector和uniquValues数组“match”的结果，而不再是uniqueValues中所有可能的，即使myVector中没有的结果。现在我们在调用MyMode()函数就能得到正确的结果： 1MyMode(tinyData) 1## [1] 9 可以看到，程序输出了我们期望的结果。当我们在tinyData数组后面加了七个9之后，函数输出也输出正确结果：9。 在退出R-Studio之前千万不要忘了将你所做的工作保存下来，点击“File”菜单中的“Save”选项，根据提示选择合适的储存位置，然后输入任何你喜欢的文件名即可，比如MyMode。程序会自动为保存的文件加上“R”后缀，因此最终的文件名应为“MyMode.R”。你可以随时在你的当前工作中打开此文件，然后重复使用MyMode()函数。 有几点需要特别注意。第一，当创建自己的函数时，一定要做测试和修改，保证函数始终能正确运行。这在处理和计算机相关的工作的时候通常遇到的情况，诸如电子数据表，宏命令和其他很多要求准确性和精度的地方都必须保证其正确运行。第二，在前面的例子中我们介绍了四个函数：unique(),tabulate(),match()和which.max()。R中还有其他很多类似的函数，要想全部知道或者记住这些函数是非常困难的。而完成某个目的的方法往往不止一种。因此，当你不知道这些基本函数的时候，你也往往很难创建新的函数来解决特定的问题。这就是为什么社区兴起的原因，在网上随便一搜，你就会发现很多人都试图解决你所面临的类似的问题，而你也会发现很多人也把他们解决方法的R代码发布出来了。这些代码片段都是免费拿来使用和测试的。事实上，通过别人的例子来学习是非常好的拓展见识、学习新技术的方式。 最后一点需要注意的也是我们接下来要讨论的关键主题。为了创建这个MyMode函数，我们做了非常多的工作，但是仍然不能保证其对于所有可能遇到的输入参数都能完美运行。也行别人已经解决了我们遇到的问题，这样我们就可以直接将别人写的“包（package）”添加到R编译器中，来拓展R的函数。事实上，对于统计模式，现有的一个包几乎可以完成你能想到的所有工作。这个包叫“modees”，是“mode-estimator”的一个不太好的缩写。查看R-studio的右下角窗口，上面有很多标签，其中一个标签是“Package”，点击这个标签你就能看到所有已经安装的包的列表，选中包前面的勾选框就可以在你的R程序中使用这个包了。“modeest”包应该不在这个列表中，因此你需要点击“Install”按钮来安装包。点击“Install”后，会弹出一个如下图的界面： 在“Packages”编辑框中输入想安装的包的名字就可以开始安装这个包了。或许你记不清包的完整的名称，你只需输入前几个字母，R-studio就会自动列出所有匹配的包，从列表中选择你需要的即可。安装过程中可能会弹出“Install Dependencies”的勾选框，如果弹出来了，选上即可。这是因为在有些情况下，安装R的包需要依赖其他包，R-studio会自动按照正确的顺序将所有必需的包都安装上。点击“Install”后，你会发现R控制台（左下角窗口）中运行了一些命令。通常情况下，这个安装过程不会出现问题，你也看不到任何警告信息。安装完成后，再次查看右下角的窗口（注意选择“Packages”标签），你就会发现modeest包已经在列表中了。然后选中旁边的勾选框，这会自动调用library()函数，然后你就可以使用modeest这个包了。 我们先来试试mfv()函数，这个函数返回一个向量中出现次数最多的数，这个功能也是我们在函数中经常用到的： 1mfv(tinyData) 1## [1] 9 到目前为止一切都很顺利，这个函数似乎和我们的MyMode()函数做了一样的事，尽管其方法可能不一样。事实上，想知道这个函数是怎么实现的也非常简单，只需在R命令行中输入函数名字即可： 1mfv 1234567## function (x, ...) ## &#123;## f &lt;- factor(x)## tf &lt;- tabulate(f)## return(as.numeric(levels(f)[tf == max(tf)]))## &#125;## &lt;environment: namespace:modeest&gt; 这是开源程序最棒的一件事：你可以轻松的查看底层的具体实现。注意到尽管这个函数也是用了tabulate()函数，但其方法和和我们的MyMode()函数完全不同。最后一行以“environment”开头的信息在更加复杂的编程中非常重要，因为其指明了函数mfv()是来自哪里。这个函数的其他方面就不是特别明显了，其中还有一个特点是当你输入的参数列表中出现次数最多的数不止一个时，函数会正确返回所有出现次数最多的数，比如输入以下向量： 12multiData &lt;- c(1,5,7,7,9,9,10)mfv(multiData) 1## [1] 7 9 1MyMode(multiData) 1## [1] 7 上面第一语句中，我们输入了一个新的向量，其中数字7和9出现次数最多，都为两次，而其他数则只出现了一次。当我们调用mfv()函数来处理这个向量是，mfv()函数能正确返回7和9这两个数，而是用MyMode()函数，则只能返回第一次找到的出现最多的数，也就是7。 回顾一下，这一章介绍了R的一个集成开发环境(IDE)——R-studio。在处理数据和进行数据分析的时候，IDE能非常方便的构造可重复使用的模块。从这一点考虑，我们就会选择使用R-studio和不是旧的R，这样我们就可以保存和重复使用我们所做的工作。另一方面，使用R-studio也会非常便于管理R的包，这对于拓展R非常关键。这之后的章节中我们仍然会使用R包来实现专门的功能。 这些具有专门功能的函数是由R社区的贡献者们创造的，以额外函数的形式出现。通过创建自己的函数，我们知道了函数接受“参数”作为其输入，然后给出一个返回值。返回值是一个数据对象，因此可以是一个单一的数字（从技术上将就是一个元素个数为1的向量），或者返回一串值（即一个向量），或者更负责的数据对象。在本书后面的章节中，我们会经常自己写函数，然后重复使用。也会经常使用别人的函数，这时只需按照他们的包然后用library()函数载入即可。将包载入后，在R命令行中输入函数的名字，就可以查看函数是如何实现的。（注意对一小部分用其他编程语言，比如C，编写的函数就不能用这种方法查看函数的实现了） 本章挑战 编写并测试一个MySamplingDistribution(),根据输入的数值向量，创建其均值构成的抽样分布。你需要将这章所学的创建函数的知识和前面所学的创造抽样分布的知识结合到一起，来写出一个可用的函数。写之前一定好仔细考虑函数的输入参数和返回的数据对象。 资源 http://en.wikipedia.org/wiki/R_(programming_language) http://en.wikipedia.org/wiki/Joseph_J._Allaire http://stats.lse.ac.uk/penzer/ST419materials/CSchpt3.pdf http://www.use-r.org/downloads/Getting_Started_with_RStudio.pdf http://www.statmethods.net/interface/packages.html http://www.youtube.com/watch?v=7sAmqkZ3Be8 本章用到的R命令 function()——创建一个新的函数 return()——返回一个值来结束函数 tabulate()——统计一个向量中每个数出现的次数 unique()——取输入向量中的不重复元素构成新的向量 match()——返回两个向量中都存在的元素 mfv()——出现次数最多的元素（来自modeest包）]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[数据科学(8)-第八章：大数据]]></title>
      <url>%2F2014%2F09%2F08%2F2014-9-8-data-science-chp8-big-data%2F</url>
      <content type="text"><![CDATA[本文为Introduction to Data Science一书的翻译，由网友义务完成，了解参加翻译的网友，请点击这里，如果要加入我们请加入qq群171546473,了解翻译规则点击这里 第八章 大数据？了不起！在2012年，许多科技媒体的头条关注了大数据。是什么让数据变大，为什么“大”重要？在这一章，我们讨论这些问题后面的争议。掌握了前一章的知识后，对于数据的规模如何影响我们的数据工作，我们可以有更多的思考。 市场观察（一个华尔街期刊）最近发表了一篇文章标题为“谷歌IT和商务专业人士指出大数据等于大回报”，副标题“根据新的全球调查，70%的组织正在考虑、计划或者运行大数据项目”。相似的技术新闻在这几年屡见不鲜。如此大量这样的文章让人不由得认为“大数据”是一种给这个世界掀起信息和技术浪潮的革命。但这是真的的？“大数据”真的改变了一切吗？ 商业分析师Doug Laney指出三个特征让大数据跟以往的技术革命不同：海量，速度、多样性。海量指的是庞大的数据量。速度则是数据的快速生成和更新。最后，多样性指的是会包含多种不同类型的数据。这三个特性经常指代大数据的“3V”模型。但是，即使在计算机时代的破晓，我们就已经拥有多样的数据，有的数据产生非常快，而且经过时间的推移，这些产生的数据能装满大量的存储器（试想，从18世纪开始，美国国会图书馆每年产生的多种的大量的数据）。所以，光凭某人说他们面临海量的、高速的、多样性的数据问题，是不足以证明大数据是一个崭新的事物。 有这样的说法，和几年前相比，许多进行中的改变让今天的数据问题有了质的不同。让我们列下几个准确的例子： 传感器（比如条形码阅读器）价格的下降和最近几年是它价格下降并且更容易收集更多数据的技术 相似的，存储器价格的下降使不管质量和使用地保存大量无关紧要的数据变得现实了。 许多人对隐私的态度不再像以前那样严格，他们似乎已经适应了使用Facebook和其他会透露许多个人信息的平台。 研究者已经在机器学习算法中取得了重大进步，它是构建许多数据挖掘的基础。 当一个数据集达到一定规模（几千行），传统的统计显著性检验已经没有意义了，因为即使最小最微不足道的结果（或者效应量，按照统计学家的说法）也是统计上显著的 出了以上这几点，我们能然有许多需要做出的改变： A.垃圾进，垃圾出：数据的用处非常依赖它是如何收集的。数据收集后，它的质量非常依赖我们如何对它做预处理：数据清理和数据筛选。B.大等于怪异：如果你寻找异常现象——违反规则的罕见事件——那么数据量越大越好。低频率事件经常不会出现直到数据收集持续很长一段时间，或者包含一个足够大的样例组来专门记录一个奇特案例。C.连接增加可能：单个数据集不管提供什么变量都有内在的限制。但是如果这些数据能连接到别的数据，新的分析途径也许会突然出现。虽然不能保证，但是你连接越多的数据，你就越有可能有新的发现。 以上两个列表的观点都是老生常谈毫无争议的。但是他们进一步解释了大数据可能会有多重要的问题。历史上已经有许多成功应用传统统计检验适当规（比如1000行或更少）模数据集的统计规律。每个人都喜欢的基础统计，学生t检验，是检验两组数据中心趋势异同的工具。如果数据包含某种规律，一组数据显著不同于另一组数据，一个t检验会告诉我们同样的结果。 大数据没办法用这样的检验帮助我们。我们甚至不需要一千条数据来做传统统计比较，而且上百万上千万的数据并不会让我们的工作更简单（它只会占用更多的电脑内存和磁盘）。想想我们前一张读到的：我们可以开始用基本的统计推断处理有51条记录的数据。实际上，大部分经常使用的统计技术，比如学生t检验，是专门设计处理小样本的。 另一方面，如果我们是在草堆里找一根针，尽量在最大的草堆里找就讲得通了。因为大的草堆更有可能包含至少一根针。随着机器学习近几年的进步，我们开始意识到好的工具、大量数据和有趣的问题确实能为我们提供新的更深刻的见解。 让我们结合这个这样的乐观主义和三个非常重要的注意事项。第一条是当这个数据越复杂，就越难确定数据是否“干净”和适合我们的要求。一个脏数据集在某方面比没有数据还糟，因为我们也许会花很多时间精力在上面而得不到任何结果。更严重的是我们花时间精力最后只能得到错误的结果！很多分析师认为清理数据——让数据适合分析，清楚异常数据，组织适合的数据结构——事实上是数据分析过程中最耗时间精力的。 第二条注意事项是稀少和异常事件或模式一直伴随这他们不可预测的本质。甚至面对我们能想象的最好的数据和大量变量，我们几乎一直都会有准确性的问题。数据挖掘工具也许能给我们呈现数据的模式，我们甚至可以在新数据中重现这种模式，但是我们永远不敢保证我们已经对这种模式理解到能分离，控制或明白它的原因。预测飓风的路径就是一个很好的例子：尽管气象仪器，预测和数字捣弄已经有了几十年的发展，气象学家在预测飓风是否会登陆或风速会多大都面对非常大的困难。工作中的复杂性和不可预测性使得任务异常艰难。 第三条注意事项是有关连接数据集的。上文的C条目提出连接能提供格外的价值。但是每一条到新数据集的连接也增加了数据的复杂性，同时也增加了出现脏数据和假结果的可能性。而且，虽然很多公司都越来越不在乎这一点，我们连接越多个人的数据（如消费者，病人，投票者等），我们就越可能造成隐私泄露的灾难。即使你不在乎隐私问题，但是你不得不承认安全和隐私丑闻会给公司带来巨大的金钱和名誉损失。现在有价值和可接受的数据革新领域也许就是犯罪和欺诈数据。越多数据集的连接就越容易暴露那些人的恶行。 总的来说，我们可以认为丰富的大量的数据结合适合的分析工具在有保障的环境下可以为商业部门、教育部门、政府部门和其他领域带来益处。但是，数据科学家关注的不应该是取得尽可能多的可用数据，而是根据目的使用对的数据和对的规模。拥有大量不适当的数据并不会有任何好处。就好像，简单快速取得的数据并不能保证与要研究的问题相关。虽然说多样性是生活的调味品，但是过于复杂经常会损害可靠性和可信性：我们连接的数据集越复杂，我们越可能在使用和保存它们的时候出现意外问题。 数据科学的工具经过前面几章，我们对于上千数据分析师使用的分析工具——数据分析和数据可视化的开源软件R——已经有了一个快速的了解。尽管R功能十分强大，但是数据分析师根据研究的不同领域仍然使用了上百种其他分析工具。 出了R之外，一个非常流行和强大的工具是SAS（读“sass”），它是一个一个有专利的统计系统。SAS包含了强大的编程语言，覆盖了许多数据类型，功能和语言特点。跟R相比，学习SAS可以说难（或简单，根据你的预期），但是许多大企业使用SAS，因为不像R，SAS提供了大量的技术和产品服务。当然，这样的服务价格不便宜，所以大部分SAS使用者都是有能力购买许可证书和售后支持的大企业。 另一个在统计领域的工具是SPSS，一个许多研究者使用的工具包（全名是统计产品与服务解决方案）。对许多分析师来说，SPSS比SAS更容易上手，但是没有那么灵活和强大。 R、SPSS、SAS作为统计工具包，但数据分析师在某些方面仍然使用了其他一般编程语言做数据分析。一个令人激动的发展中的语言有一个奇怪的名字“Processing”。Processing是一个专门用来做数据可视化的编程语言。像R一样，Processing是一个开源项目，它免费提供在http://processing.org/。而且跟R一样，processing是一个跨平台语言，它可以在Mac、Windows、Linux上完美运行。有许多书提供processing的学习（不幸的是还没有开源书），许多网站为初学者提供了大量实例。在R之外，processing也许是数据科学家工具箱中最重要的工具之一。 本章练习查看各种与“Data.gov”相关的网站，尽可能找到最大或者最复杂的数据集。试思考（或写下来）一种或多种这些数据可能在分析中被误用的情况。下载一个你感兴趣的数据集，将它载入R中看你能做点什么。 下面是一个格外挑战，打开下面这个网页：heep://teamwpc.co.uk/products/wps下载试用版的“World Programming System”（WPS）。WPS能读取SAS代码，你能很方便的找到你要的代码用来读取Data.gov数据集。 参考资料http://aqua.nasa.gov/doc/pubs/Wx_Forecasting.pdfhttp://en.wikipedia.org/wiki/Big_datahttp://en.wikipedia.org/wiki/Data.govhttp://www.marketwatch.com/story/big-data-equals-big-business-opportunity-say-global-it-and-business-professionals-2012-05-14]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[机器学习与R(10)-朴素贝叶斯]]></title>
      <url>%2F2014%2F08%2F22%2F2014-8-22-naive-bayes%2F</url>
      <content type="text"><![CDATA[Naive Bayes关于贝叶斯定理阮一峰的博客已经写得相当好了，我就不献丑了，要了解看这里： 贝叶斯推断及其互联网应用（一）：定理简介贝叶斯推断及其互联网应用（二）：过滤垃圾邮件 朴素贝叶斯则是利用贝叶斯定理来进行分类的方法。它具有以下优点： 简单，快速，有效 能很好处理噪声，缺失数据 对小和大样本都可以 比较容易获得预测的概率估计 不过它也存在： 要求特征间平等及重要性相同，特征相互独立（通常不会存在，这也是为什么要叫naive的原因） 对主要是大量数字特征的数据集不太适合 估计概率不如分类可靠（所以一般用它来分类） 拉普拉斯估计值朴素贝叶斯用来进行文本分类通常存在一个问题，如果训练的数据中某个词没有出现过，那么该词的概率就变成了0，从而如公式所示，整个计算的垃圾邮件概率就变成了0，这显然不合理，于是就有了拉普拉斯估计的应用，我们给它预估一个值这样就可以避免问题的出现了。 数字特征的处理贝叶斯是通过计算频数来进行学习，这显然只能用于分类数据，对于连续数据这时可以考虑把它进行分区（bin）处理，比如利用cut函数。不过需要注意的是这样会导致信息的丢失。 利用朴素贝叶斯来判断垃圾短信这里我们以判断垃圾短信为例，数据来自sms spam数据集 数据准备把数据下载后读入： 1234sms_raw &lt;- read.table("SMSSpamCollection", stringsAsFactors = FALSE, sep = "\t", header = F, comment = "", quote = NULL)names(sms_raw) = c("type", "text")str(sms_raw) 123## &apos;data.frame&apos;: 5574 obs. of 2 variables:## $ type: chr &quot;ham&quot; &quot;ham&quot; &quot;spam&quot; &quot;ham&quot; ...## $ text: chr &quot;Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...&quot; &quot;Ok lar... Joking wif u oni...&quot; &quot;Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&amp;C&quot;| __truncated__ &quot;U dun say so early hor... U c already then say...&quot; ... 注意如果在read.table里面不指定quote=NULL那么会遇到如下问题 Warning message:In scan(file, what, nmax, sep, dec, quote, skip, nlines, na.strings, : EOF within quoted string 实际上你如果仔细研究一下数据，你可以发现这是因为数据里面的5082行开始有&quot;&quot;导致。 接下来将type转换为factor变量，因为贝叶斯分类要求目标变量为factor类型。 12sms_raw$type = factor(sms_raw$type)table(sms_raw$type) 123## ## ham spam ## 4827 747 数据集里面有4827条正常短信，747条垃圾短信 数据预处理对于文本的分析通常我们会用到tm包 1require(tm) 1## Loading required package: tm 1sms_corpus &lt;- Corpus(VectorSource(sms_raw$text)) 这里将原始数据中的短消息都作为向量输入构建语料库 1print(sms_corpus) 1## A corpus with 5574 text documents 1inspect(sms_corpus[1:3]) 12345678910111213141516## A corpus with 3 text documents## ## The metadata consists of 2 tag-value pairs and a data frame## Available tags are:## create_date creator ## Available variables in the data frame are:## MetaID ## ## [[1]]## Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...## ## [[2]]## Ok lar... Joking wif u oni...## ## [[3]]## Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&amp;C&apos;s apply 08452810075over18&apos;s 这里可以看出语料库有5574个文档，实际与我们的数据集样本数一样。每个文档对应的就是一条短信。从前3条短信我们看出，文档的里面有标题，数字，还有标点符号，以及大小写，为了方便分析我们进行如下处理： 123456corpus_clean &lt;- tm_map(sms_corpus, tolower)corpus_clean &lt;- tm_map(corpus_clean, removeNumbers)corpus_clean &lt;- tm_map(corpus_clean, removeWords, stopwords())corpus_clean &lt;- tm_map(corpus_clean, removePunctuation)corpus_clean &lt;- tm_map(corpus_clean, stripWhitespace)inspect(corpus_clean[1:3]) 12345678910111213141516## A corpus with 3 text documents## ## The metadata consists of 2 tag-value pairs and a data frame## Available tags are:## create_date creator ## Available variables in the data frame are:## MetaID ## ## [[1]]## go jurong point crazy available bugis n great world la e buffet cine got amore wat## ## [[2]]## ok lar joking wif u oni## ## [[3]]## free entry wkly comp win fa cup final tkts st may text fa receive entry questionstd txt ratetcs apply s 以上依次把所有词转换为小写，去掉数字，去掉停止词（就是类似and，or，the之类，也就是冠词、介词、副词或连词），去掉标点，最后去掉所有空格。 完成了上述步骤，我们就需要统计每个词在文档中出现的频率了，这可以通过构建document term稀疏矩阵完成，这个稀疏矩阵的行对应一个文档，列则对应了每个词。term document则反过来。 1sms_dtm &lt;- DocumentTermMatrix(corpus_clean) 准备训练与测试数据有了上面的矩阵，我们就可以开始准备训练数据与测试数据了，还是用caret包的createDataPartition来完成，可以看出训练与测试数据中的垃圾短信比例都相似。 1require(caret) 1## Loading required package: caret 1## Warning: package &apos;caret&apos; was built under R version 3.0.3 12## Loading required package: lattice## Loading required package: ggplot2 123456789101112set.seed(2014)inTrain = createDataPartition(y = sms_raw$type, p = 0.75, list = FALSE)sms_raw_train = sms_raw[inTrain, ]sms_raw_test = sms_raw[-inTrain, ]sms_dtm_train = sms_dtm[inTrain, ]sms_dtm_test = sms_dtm[-inTrain, ]sms_corpus_train = corpus_clean[inTrain]sms_corpus_test = corpus_clean[-inTrain]prop.table(table(sms_raw_train$type)) 123## ## ham spam ## 0.8659 0.1341 1prop.table(table(sms_raw_test$type)) 123## ## ham spam ## 0.8664 0.1336 wordcloud最简单的文本分析方法就是市场词云了，我们用wordcloud包 1require(wordcloud) 12## Loading required package: wordcloud## Loading required package: Rcpp 1## Warning: package &apos;Rcpp&apos; was built under R version 3.0.3 1## Loading required package: RColorBrewer 1wordcloud(sms_corpus_train, min.freq = 40, random.order = FALSE) 这里的min.freq是词出现的最小频率，通常我们用语料库的10%来开始(训练语料库有4182个文档)。上面那个词云只是给出了一个总体印象，对我们的分析没有太大帮助，所有我们考虑分布看看垃圾邮件与正常邮件的区别 123spam &lt;- subset(sms_raw_train, type == "spam")ham &lt;- subset(sms_raw_train, type == "ham")wordcloud(spam$text, max.words = 40, scale = c(3, 0.5)) 1wordcloud(ham$text, max.words = 40, scale = c(3, 0.5)) 很显然可以看出垃圾邮件里面free,now,prize,text claim比较多。 词频把所有的词都考虑进来显然不是很好的方法，我们的矩阵有7986个特征，因此我们需要考虑缩小范围，于是采用findFreqTerms的方法取大于5的特征（具体取多少根据数据的数据情况）： 1findFreqTerms(sms_dtm_train, 5)[10:20] 123## [1] &quot;actually&quot; &quot;add&quot; &quot;address&quot; &quot;admirer&quot; &quot;advance&quot; ## [6] &quot;aft&quot; &quot;afternoon&quot; &quot;age&quot; &quot;ago&quot; &quot;ahead&quot; ## [11] &quot;aight&quot; 1sms_dict &lt;- Dictionary(findFreqTerms(sms_dtm_train, 5)) 获得了频数大于5的词后，我们再利用它来生成一个字典，这样可以在文档矩阵中指出，我只取字典中有的词，新的矩阵就只有1252个特征了。 12sms_train &lt;- DocumentTermMatrix(sms_corpus_train, list(dictionary = sms_dict))sms_test &lt;- DocumentTermMatrix(sms_corpus_test, list(dictionary = sms_dict)) 我们的目标是想通过短信里面有或者是没有某个词来判断是否是垃圾短信，那么我们很显然应该使用的矩阵是标记某个词在某个短信中出现了还是没有出现。因此写个函数来完成这一个功能： 12345convert_counts &lt;- function(x) &#123; x &lt;- ifelse(x &gt; 0, 1, 0) x &lt;- factor(x, levels = c(0, 1), labels = c("No", "Yes")) return(x)&#125; 对矩阵每一列进行这样的处理： 12sms_train &lt;- apply(sms_train, MARGIN = 2, convert_counts)sms_test &lt;- apply(sms_test, MARGIN = 2, convert_counts) 于是我们可以得到最终用来构建模型的数据集。 模型训练在R里面有多个包都提供朴素贝叶斯分类，比如e1071包，还有klaR包的NaiveBayes()。这里使用e1071 1require(e1071) 12## Loading required package: e1071## Loading required package: class 1sms_classifier &lt;- naiveBayes(sms_train, sms_raw_train$type) 于是我们得到了分类器sms_classifier 模型评估有了模型就可以对测试数据进行预测: `pred(m,test,type=”class”) 这里的type如果为class代表是分类，如果是raw则代表概率的计算 12sms_test_pred &lt;- predict(sms_classifier, sms_test)require(gmodels) 1## Loading required package: gmodels 1## Warning: package &apos;gmodels&apos; was built under R version 3.0.3 12CrossTable(sms_test_pred, sms_raw_test$type, prop.chisq = FALSE, prop.t = FALSE, dnn = c("predicted", "actual")) 1234567891011121314151617181920212223242526272829## ## ## Cell Contents## |-------------------------|## | N |## | N / Row Total |## | N / Col Total |## |-------------------------|## ## ## Total Observations in Table: 1392 ## ## ## | actual ## predicted | ham | spam | Row Total | ## -------------|-----------|-----------|-----------|## ham | 1202 | 29 | 1231 | ## | 0.976 | 0.024 | 0.884 | ## | 0.997 | 0.156 | | ## -------------|-----------|-----------|-----------|## spam | 4 | 157 | 161 | ## | 0.025 | 0.975 | 0.116 | ## | 0.003 | 0.844 | | ## -------------|-----------|-----------|-----------|## Column Total | 1206 | 186 | 1392 | ## | 0.866 | 0.134 | | ## -------------|-----------|-----------|-----------|## ## 我们可以看出简单的贝叶斯模型的效果却很好，97.6%的正确率，186封垃圾邮件中29封误判为了正常邮件。而1206封正常邮件中4封误判为垃圾邮件。把正常邮件误判为垃圾邮件的影响显然更大，这是需要考虑的地方。 模型改进前面说过了拉普拉斯估计的问题，那么如果我们假设拉普拉斯估计会怎么样呢？ 1234sms_classifier2 &lt;- naiveBayes(sms_train, sms_raw_train$type, laplace = 1)sms_test_pred2 &lt;- predict(sms_classifier2, sms_test)CrossTable(sms_test_pred2, sms_raw_test$type, prop.chisq = FALSE, prop.t = FALSE, prop.r = FALSE, dnn = c("predicted", "actual")) 1234567891011121314151617181920212223242526## ## ## Cell Contents## |-------------------------|## | N |## | N / Col Total |## |-------------------------|## ## ## Total Observations in Table: 1392 ## ## ## | actual ## predicted | ham | spam | Row Total | ## -------------|-----------|-----------|-----------|## ham | 1204 | 30 | 1234 | ## | 0.998 | 0.161 | | ## -------------|-----------|-----------|-----------|## spam | 2 | 156 | 158 | ## | 0.002 | 0.839 | | ## -------------|-----------|-----------|-----------|## Column Total | 1206 | 186 | 1392 | ## | 0.866 | 0.134 | | ## -------------|-----------|-----------|-----------|## ## 加了拉普拉斯估计后，正常邮件误判为垃圾邮件减少了2封，而垃圾邮件误判为正常邮件的增加了1封。似乎新的模型要好些。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[机器学习与R(8)-k nearest neighbors-1]]></title>
      <url>%2F2014%2F08%2F22%2F2014-8-22-kNN-1%2F</url>
      <content type="text"><![CDATA[今天我们介绍最简单但是却很实用的kNN分类算法的R实现。 kNN简介基于Nearest Neighbors的分类可以用于： 比如文字识别，面部识别 预测某人是否喜欢推荐的电影（Netflix） 基因模式识别，比如用于检测某种疾病 通常最近邻居分类器适用于特征与目标类之间的关系为比较复杂的数字类型，或者说二者关系难以理解，但是相似类间特征总是相似。kNN算法： 简单有效，对数据分布没有假设，数据训练也很快 但是它没有模型输出，因此限制了对特征间关系的理解 分类阶段也比较慢，耗费内存 对nominal特征以及缺少数据需要预先处理 算法kNN分类的思想很简单就是物以类聚，人以群分，根据与待分类数据集中的最近的k个训练集中的分类标签来对决定其类别。比如说我们已经知道如下数据： 食物 甜度 脆度 食物类别 apple 10 9 水果 bacon 1 4 蛋白质 banana 10 1 水果 carrot 7 10 蔬菜 celery 3 10 蔬菜 cheese 1 1 蛋白质 kNN算法会所有特征作为多维空间的坐标，通过计算这些点之间的距离来计算每一个样本间的相似性。 显然从这里我们可以看出组内数据间的相似性。此时如果我有一个西红柿需要分类，那么根据它的k个最近邻居我们就可以做出判断，k=1时分类与最近那个相同，k=3那么就是投票决定。 前面讲到了距离那么必然涉及距离如何计算的。常用的有欧几里德距离： $$ dist(p,q)=\sqrt{(p_1-q_1)^2+(p_2-q_2)^2+(p_3-q_3)^2+…} $$ 因此我们可以计算： 食物 甜度 脆度 食物类别 与tomato的距离 grape 8 5 水果 sqrt((6 - 8)^2 + (4 - 5)^2) = 2.2 green bean 3 7 蔬菜 sqrt((6 - 3)^2 + (4 - 7)^2) = 4.2 nuts 3 6 蛋白质 sqrt((6 - 3)^2 + (4 - 6)^2) = 3.6 orange 7 3 水果 sqrt((6 - 7)^2 + (4 - 3)^2) = 1.4 如果k=1，那么西红柿与orange最近，就应该是水果，如果k=3，就是投票，橘子和葡萄都说它是水果，那它就是水果了。 k的选择很明显k的选择对最终结果大有影响，这就是机器学习中几点的bias与variance取舍问题，鱼和熊掌不可兼得。如果k很大，那么可以减少干扰数据的影响，但是此时就导致了系统性偏差，比如如果去k为总的训练数据数，那么每次投票肯定都是训练数据中多的类别胜利。显然训练数据的系统性偏差会影响结果。而如果k=1，那么某个干扰数据或者异常数据会影响最终结果的准确性，所以我们始终是在bias与variance直接取舍。 k通常会在3～10直接取值，或者是k等于训练数据的平方根。比如15个数据，可能会取k=4 另一个不常用的是k取较大值，但是我们在投票时权重不同 数据的预处理有了计算距离的方法，也有了k的取值，是否我们就可以开始分类了？不要急，还有重要的一步就是，数据的预处理。简单考虑一下，比如我们度量各个特征的时候刻度单位不同，那么会带来什么问题。特征A取值是从0～1，另一个特征B则是0～10000，这里特征B的1000，不代表是特征A 的1的1000倍，因此我们要对数据进行标准化。传统的是采用最小最大值标准化方法： $$ X_new=\frac{X-min(X)}{max(X)-min(X)} $$ 这样$X_new$的取值就在0～1之间了。另一种方法则是z-score: $$ X_new=\frac{X-\mu}{\sigma}=\frac{x-mean(x)}{sd(x)} $$ 数据已经标准化了是否就ok了？不是的，欧几里德距离只能用于数字，对于nominal变量（就是分类变量啦），无法处理。这个时候我们可以通过dummy（哑元）来处理。比如： 性别为male就取1 其它取0 那如果不止2类，是多个呢？一种方法就是我创建（n-1)个哑元变量，还有一种就是如果你的分类变量是有顺序的，而且每一级间的间隔是固定的那你可以用比如1，2，3来代表冷，温暖，热。不过这时一定要注意这个间隔的问题，比如穷人，中产，富人，显然中产和穷人，富人和中产的差距就不是一样的。 懒惰的kNN前面我们在什么是机器学习时说过机器学习有一个抽象的过程，而kNN实际没有抽象与泛化（generalization）的过程。kNN因此是懒惰的学习或者叫instance-based learning / rote learning。 kNN也是非参数化的，而后面我们要说的回归则是参数化的。尽管kNN分类器很懒惰，但是却很有用，后面我们用UCI的机器学习数据来看R里面的实现。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[机器学习与R(9)-k nearest neighbors-2]]></title>
      <url>%2F2014%2F08%2F22%2F2014-8-22-kNN-2%2F</url>
      <content type="text"><![CDATA[利用kNN算法来诊断乳腺癌 背景乳腺癌的早期检测来源对乳腺异常包块的检测。如果存在包块，那么会继续乳腺穿刺取样，之后利用显微镜下的分析来判断是良性还是恶性。如果我们可以用机器学习来完成癌症细胞的自动识别则我们可以为健康系统带来大大好处，比如检测效率的提升。同时自动的筛选系统可以大大减少人工的主观性。 数据准备这里我们使用Breast Cancer Wisconsin Diagnostic数据集，你可以从这里下载它，数据说明点击这里。 数据包含了569个样本，32个特征的数据，其中关键特征为: Radius Texture Perimeter Area Smoothness Compactness Concavity Concave points Symmetry Fractal dimension 数据分别度量了这些特征的均值，标准差，以及最大值，在数据集中依次为3～12，13～22，23～32列。第1列为id，第2列为诊断结果，B为良性，M为恶性。 123456wdbc = read.csv("wdbc.data", header = F)wdbc.names = c("Radius", "Texture", "Perimeter", "Area", "Smoothness", "Compactness", "Concavity", "Concave points", "Symmetry", "Fractal dimension")wdbc.names = c(wdbc.names, paste(wdbc.names, "_mean", sep = ""), paste(wdbc.names, "_worst", sep = ""))names(wdbc) = c("id", "diagnosis", wdbc.names) 最终的数据为： 1str(wdbc) 123456789101112131415161718192021222324252627282930313233## &apos;data.frame&apos;: 569 obs. of 32 variables:## $ id : int 842302 842517 84300903 84348301 84358402 843786 844359 84458202 844981 84501001 ...## $ diagnosis : Factor w/ 2 levels &quot;B&quot;,&quot;M&quot;: 2 2 2 2 2 2 2 2 2 2 ...## $ Radius : num 18 20.6 19.7 11.4 20.3 ...## $ Texture : num 10.4 17.8 21.2 20.4 14.3 ...## $ Perimeter : num 122.8 132.9 130 77.6 135.1 ...## $ Area : num 1001 1326 1203 386 1297 ...## $ Smoothness : num 0.1184 0.0847 0.1096 0.1425 0.1003 ...## $ Compactness : num 0.2776 0.0786 0.1599 0.2839 0.1328 ...## $ Concavity : num 0.3001 0.0869 0.1974 0.2414 0.198 ...## $ Concave points : num 0.1471 0.0702 0.1279 0.1052 0.1043 ...## $ Symmetry : num 0.242 0.181 0.207 0.26 0.181 ...## $ Fractal dimension : num 0.0787 0.0567 0.06 0.0974 0.0588 ...## $ Radius_mean : num 1.095 0.543 0.746 0.496 0.757 ...## $ Texture_mean : num 0.905 0.734 0.787 1.156 0.781 ...## $ Perimeter_mean : num 8.59 3.4 4.58 3.44 5.44 ...## $ Area_mean : num 153.4 74.1 94 27.2 94.4 ...## $ Smoothness_mean : num 0.0064 0.00522 0.00615 0.00911 0.01149 ...## $ Compactness_mean : num 0.049 0.0131 0.0401 0.0746 0.0246 ...## $ Concavity_mean : num 0.0537 0.0186 0.0383 0.0566 0.0569 ...## $ Concave points_mean : num 0.0159 0.0134 0.0206 0.0187 0.0188 ...## $ Symmetry_mean : num 0.03 0.0139 0.0225 0.0596 0.0176 ...## $ Fractal dimension_mean : num 0.00619 0.00353 0.00457 0.00921 0.00511 ...## $ Radius_worst : num 25.4 25 23.6 14.9 22.5 ...## $ Texture_worst : num 17.3 23.4 25.5 26.5 16.7 ...## $ Perimeter_worst : num 184.6 158.8 152.5 98.9 152.2 ...## $ Area_worst : num 2019 1956 1709 568 1575 ...## $ Smoothness_worst : num 0.162 0.124 0.144 0.21 0.137 ...## $ Compactness_worst : num 0.666 0.187 0.424 0.866 0.205 ...## $ Concavity_worst : num 0.712 0.242 0.45 0.687 0.4 ...## $ Concave points_worst : num 0.265 0.186 0.243 0.258 0.163 ...## $ Symmetry_worst : num 0.46 0.275 0.361 0.664 0.236 ...## $ Fractal dimension_worst: num 0.1189 0.089 0.0876 0.173 0.0768 ... 从这里我们知道数据集中357个样本为良性，212个为恶性肿瘤 1table(wdbc$diagnosis) 123## ## B M ## 357 212 这里我们再修改一下数据,同时去掉id，因为id对预测没有意义： 123wdbc$diagnosis = factor(wdbc$diagnosis, levels = c("B", "M"), labels = c("Benign", "Malignant"))round(prop.table(table(wdbc$diagnosis)) * 100, digits = 1) 123## ## Benign Malignant ## 62.7 37.3 1wdbc = wdbc[-1] 通过summary，我们很明显看出不同的特征的度量值差别太大： 1summary(wdbc[c("Radius_mean", "Area_mean", "Smoothness_mean")]) 1234567## Radius_mean Area_mean Smoothness_mean ## Min. :0.112 Min. : 6.8 Min. :0.00171 ## 1st Qu.:0.232 1st Qu.: 17.9 1st Qu.:0.00517 ## Median :0.324 Median : 24.5 Median :0.00638 ## Mean :0.405 Mean : 40.3 Mean :0.00704 ## 3rd Qu.:0.479 3rd Qu.: 45.2 3rd Qu.:0.00815 ## Max. :2.873 Max. :542.2 Max. :0.03113 数据转换显然数据需要转换,我们定义转换函数为： 123normalize &lt;- function(x) &#123; return((x - min(x))/(max(x) - min(x)))&#125; 接下来对数据进行转换后，执行summary可以看出特征的区间分布已经统一了： 12wdbc_n &lt;- as.data.frame(lapply(wdbc[2:31], normalize))summary(wdbc_n[c("Radius_mean", "Area_mean", "Smoothness_mean")]) 1234567## Radius_mean Area_mean Smoothness_mean## Min. :0.0000 Min. :0.0000 Min. :0.000 ## 1st Qu.:0.0438 1st Qu.:0.0206 1st Qu.:0.117 ## Median :0.0770 Median :0.0331 Median :0.159 ## Mean :0.1063 Mean :0.0626 Mean :0.181 ## 3rd Qu.:0.1330 3rd Qu.:0.0717 3rd Qu.:0.219 ## Max. :1.0000 Max. :1.0000 Max. :1.000 接下来我们需要构造训练数据与测试数据，实际通常的做法是training,validation,test三个数据集，validataion用来校正提高模型准确性。这里简单起见我们只用train和test数据集，最简单的方法是如下： 123456wdbc_train = wdbc_n[1:469, ]wdbc_test = wdbc_n[470:569, ]wdbc_train_label = wdbc[1:469, 1]wdbc_test_label = wdbc[470:569, 1]mal_rate = table(wdbc_train_label)round(mal_rate[2]/sum(mal_rate), digits = 2) 12## Malignant ## 0.4 这个方法虽然简单，但需要注意这里我们不是随机采样，如果样本中的恶性肿瘤大部分布在1：469显然就有很大问题。当然另一种方法就是用sample函数，例如： 12345678set.seed(2014)inTrain = sample(1:dim(wdbc_n)[1], 469, replace = F)wdbc_train = wdbc_n[inTrain, ]wdbc_test = wdbc_n[-inTrain, ]wdbc_train_label = wdbc[inTrain, 1]wdbc_test_label = wdbc[-inTrain, 1]mal_rate = table(wdbc_train_label)round(mal_rate[2]/sum(mal_rate), digits = 2) 12## Malignant ## 0.37 除此之外，个人常用caret包的createDataPartition来完成这一工作： 1require(caret) 1## Loading required package: caret 1## Warning: package &apos;caret&apos; was built under R version 3.0.3 12## Loading required package: lattice## Loading required package: ggplot2 12345678set.seed(2014)inTrain = createDataPartition(y = wdbc$diagnosis, p = 0.8, list = FALSE)wdbc_train = wdbc_n[inTrain, ]wdbc_test = wdbc_n[-inTrain, ]wdbc_train_label = wdbc[inTrain, 1]wdbc_test_label = wdbc[-inTrain, 1]mal_rate = table(wdbc_train_label)round(mal_rate[2]/sum(mal_rate), digits = 2) 12## Malignant ## 0.37 构建模型这里kNN实现，我们采用class包的实现，当然其他实现你可以参考CRAN。 1require(class) 1## Loading required package: class 12wdbc_test_pred &lt;- knn(train = wdbc_train, test = wdbc_test, cl = wdbc_train_label, k = 21) 这里k=21，基于是采用length(wdbc_train_label)的平方根。knn的使用说明如下： train: 训练数据 test: 测试数据 cl: factor,训练数据的对应分类 模型评估这里采用gmodels的CrossTable函数 1require(gmodels) 1## Loading required package: gmodels 1## Warning: package &apos;gmodels&apos; was built under R version 3.0.3 1CrossTable(x = wdbc_test_label, y = wdbc_test_pred, prop.chisq = FALSE) 1234567891011121314151617181920212223242526272829303132## ## ## Cell Contents## |-------------------------|## | N |## | N / Row Total |## | N / Col Total |## | N / Table Total |## |-------------------------|## ## ## Total Observations in Table: 113 ## ## ## | wdbc_test_pred ## wdbc_test_label | Benign | Malignant | Row Total | ## ----------------|-----------|-----------|-----------|## Benign | 71 | 0 | 71 | ## | 1.000 | 0.000 | 0.628 | ## | 0.959 | 0.000 | | ## | 0.628 | 0.000 | | ## ----------------|-----------|-----------|-----------|## Malignant | 3 | 39 | 42 | ## | 0.071 | 0.929 | 0.372 | ## | 0.041 | 1.000 | | ## | 0.027 | 0.345 | | ## ----------------|-----------|-----------|-----------|## Column Total | 74 | 39 | 113 | ## | 0.655 | 0.345 | | ## ----------------|-----------|-----------|-----------|## ## 分别得到TN = 71,TP = 39, FN=1,FP=0。因此： accuracy = (TN+TP)/100=97.345%sensitivity=TP/(TP+FN)= 92.86%Specificity=TN/(TN+FP)= 100% 详细解释可以看wikipedia Sensitivity and specificity。简单说sensitivity是检查正确识别恶性肿瘤的比例，Specificity检查正确排除恶性肿瘤的比例。 模型改进前面我们采用的是最大最小标准化，还可以用z-score来实验一次。最大最小值标准化强制把数据压缩在了0～1之间，也许减小了极值的影响，不过可能极值正好是恶性的标志呢？以下采用z-score重复前面的步骤： 12wdbc_z = as.data.frame(scale(wdbc[-1]))summary(wdbc_z$Area_mean) 12## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -0.737 -0.494 -0.347 0.000 0.107 11.000 123456789set.seed(2014)inTrain = createDataPartition(y = wdbc$diagnosis, p = 0.8, list = FALSE)wdbc_train = wdbc_z[inTrain, ]wdbc_test = wdbc_z[-inTrain, ]wdbc_train_label = wdbc[inTrain, 1]wdbc_test_label = wdbc[-inTrain, 1]wdbc_test_pred = knn(train = wdbc_train, test = wdbc_test, cl = wdbc_train_label, k = 21)CrossTable(x = wdbc_test_label, y = wdbc_test_pred, prop.chisq = FALSE) 1234567891011121314151617181920212223242526272829303132## ## ## Cell Contents## |-------------------------|## | N |## | N / Row Total |## | N / Col Total |## | N / Table Total |## |-------------------------|## ## ## Total Observations in Table: 113 ## ## ## | wdbc_test_pred ## wdbc_test_label | Benign | Malignant | Row Total | ## ----------------|-----------|-----------|-----------|## Benign | 71 | 0 | 71 | ## | 1.000 | 0.000 | 0.628 | ## | 0.947 | 0.000 | | ## | 0.628 | 0.000 | | ## ----------------|-----------|-----------|-----------|## Malignant | 4 | 38 | 42 | ## | 0.095 | 0.905 | 0.372 | ## | 0.053 | 1.000 | | ## | 0.035 | 0.336 | | ## ----------------|-----------|-----------|-----------|## Column Total | 75 | 38 | 113 | ## | 0.664 | 0.336 | | ## ----------------|-----------|-----------|-----------|## ## 我们发现z-score的效果比之前的差: accuracy = (TN+TP)/113=96.46%sensitivity=TP/(TP+FN)= 90.47%Specificity=TN/(TN+FP)= 100% 此外我们还可以调整k，这里就不重复了。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[机器学习与R(7)-数据探索-2]]></title>
      <url>%2F2014%2F08%2F22%2F2014-8-22-exploring-data-2%2F</url>
      <content type="text"><![CDATA[盒子图boxplot是用图形表示Q1,Q3,median,min,max的方法 1boxplot(contribution$Class.Year,main="Boxplot for Class Year",ylab="Class Year") 1boxplot(contribution$FY00Giving,main="Boxplot for donation in FY00",ylab="donation $") 通过图形，很容易我们可以看出给学校捐款的学生大多是85年前毕业的，而大部分人没有捐款，少数人在贡献。 直方图histogram是另一种描述数据分布的方式 1hist(contribution$Class.Year) 1hist(contribution$FY00Giving) 1hist(subset(contribution,FY00Giving&gt;0&amp;FY00Giving&lt;500)$FY00Giving) 从Class.Year我们看出数据集中1997毕业的最多，而FY00Giving大部分为0，进一步还看出大部分人捐款都在100以下。同时我们还了解到了数据的偏度(skew)。 偏度：偏度（Skewness）是描述某变量取值分布对称性的统计量。http://www.r-tutor.com/elementary-statistics/numerical-measures/skewness Skewness=0 分布形态与正态分布偏度相同 Skewness&gt;0 正偏差数值较大，为正偏或右偏。长尾巴拖在右边。 Skewness&lt;0 负偏差数值较大，为负偏或左偏。长尾巴拖在左边。 利用来自e1071包的函数skewness我们可以验证到Class.Year分布是左偏的，FY00Giving是严重右偏的。 1require(e1071) 1## Loading required package: e1071 1skewness(contribution$Class.Year) 1## [1] -0.3364 1skewness(contribution$FY00Giving) 1## [1] 13.38 标准差与方差1sd(contribution$FY00Giving) 1## [1] 1171 1var(sd(contribution$FY00Giving)) 1## [1] NA 标准差是描述数据离mean的距离。对正态分布68%的数据在1个标准差内，95%的在2个标准差内，99.7在3个标准差内，这就是我们常说的$/sigma$ 分类（categorical） 变量此时我们可以通过频数表来统计 1234table(contribution$Major)major_table=table(contribution$Major)prop.table(major_table)round(prop.table(major_table)*100) table统计频数，prop.table则可以计算比例。 CrossTable1require(gmodels) 1## Loading required package: gmodels 1## Warning: there is no package called &apos;gmodels&apos; 1?CrossTable 12## No documentation for &apos;CrossTable&apos; in specified packages and libraries:## you could try &apos;??CrossTable&apos; gmodels包的CrossTable函数提供了更丰富的功能，可以通过?CrossTable查看 散点图12data(trees)plot(trees$Height,trees$Volume)]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[机器学习与R(6)-数据探索-1]]></title>
      <url>%2F2014%2F08%2F22%2F2014-8-22-exploring-data-1%2F</url>
      <content type="text"><![CDATA[探索数据是对数据进行初步研究,以便更好的理解数据，帮助选择合适的数据预处理和数据分析技术。通过数据探索我们将识别原始数据中的问题，缺失值，异常，分布等等。通常我们会考察： 频数，均值，方差，偏度… 对数据可视化，以快速找到数据的特征 数据读入12contribution=read.csv("contribution.csv",stringsAsFactors=F)str(contribution) 123456789101112## &apos;data.frame&apos;: 1230 obs. of 11 variables:## $ Gender : chr &quot;M&quot; &quot;M&quot; &quot;F&quot; &quot;M&quot; ...## $ Class.Year : int 1957 1957 1957 1957 1957 1957 1957 1957 1957 1957 ...## $ Marital.Status : chr &quot;M&quot; &quot;M&quot; &quot;M&quot; &quot;M&quot; ...## $ Major : chr &quot;History&quot; &quot;Physics&quot; &quot;Music&quot; &quot;History&quot; ...## $ Next.Degree : chr &quot;LLB&quot; &quot;MS&quot; &quot;NONE&quot; &quot;NONE&quot; ...## $ FY04Giving : num 2500 5000 5000 0 1000 0 0 100 100 0 ...## $ FY03Giving : num 2500 5000 5000 5100 1000 0 0 100 100 0 ...## $ FY02Giving : num 1400 5000 5000 200 1000 0 0 100 100 0 ...## $ FY01Giving : num 12060 5000 5000 200 1005 ...## $ FY00Giving : num 12000 10000 10000 0 1000 0 0 100 100 0 ...## $ AttendenceEvent: int 1 1 1 1 1 0 0 0 0 1 ... 1230条观察记录，11个变量，其中的文本变量都是char类型，没有转换成factor，这里可以根据需求来转换。 数字变量探索1summary(contribution$FY00Giving) 12## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0 0 0 169 60 21000 1summary(contribution$FY01Giving) 12## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0 0 0 277 75 161000 1summary(contribution[c("FY00Giving","FY01Giving")]) 1234567## FY00Giving FY01Giving ## Min. : 0 Min. : 0 ## 1st Qu.: 0 1st Qu.: 0 ## Median : 0 Median : 0 ## Mean : 169 Mean : 277 ## 3rd Qu.: 60 3rd Qu.: 75 ## Max. :21000 Max. :161370 summary列出了最小，最大，第1，3个四分位值以及mean，median。通过这些数据我们大致对数据的分布区间有了了解。而其它的来自Hmisc包的describe提供了更详细的数据描述。 1describe(contribution$FY01Giving) 1## Error: could not find function &quot;describe&quot; 数据的中心趋势对于一组数据的描述，我们常用均值来进行描述，而均值有mean和median两种，其中mean很容易受到异常值的影响(outlier) 12x=c(10:20,80,100,120)mean(x) 1## [1] 33.21 1median(x) 1## [1] 16.5 显然在这是用median是更好描述方法，通过summary，我们可以同时获得mean,median，通过比较二者，我们也可以知道数据的分布情况。比如这里mean远大于median，虽然数据中大部分点位于10到20，但是少数几个较大的outlier使得mean为33.2，而此时median则为16.5 1range(contribution$FY00Giving) 1## [1] 0 21000 1diff(range(contribution$FY00Giving)) 1## [1] 21000 利用range与diff我们可以获得数据的分布区间，而 1IQR(contribution$FY00Giving) 1## [1] 60 则直接给出中间50%的数据的区间即Q1与Q3间的差。直接使用 1quantile(contribution$FY00Giving) 12## 0% 25% 50% 75% 100% ## 0 0 0 60 21000 1quantile(contribution$FY00Giving,prob=c(0.75,0.85,0.95)) 12## 75% 85% 95% ## 60 154 350 12#试试这个#quantile(contribution$FY00Giving,prob=seq(from=0,to=1,by=0.1)) 可以给出更详细的值，比如我们可以由此知道85%的人的捐款都少于154$。通过这些数据你应该可以理解前面的summary(contribution$FY00Giving)为什么median为0了，数据分布的非正态。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[数据科学(7)-第七章：罐子中的样本]]></title>
      <url>%2F2014%2F08%2F17%2F2014-8-data-science-chp7-sample%2F</url>
      <content type="text"><![CDATA[本文为Introduction to Data Science一书的翻译，由网友义务完成，了解参加翻译的网友，请点击这里，如果要加入我们请加入qq群171546473,了解翻译规则点击这里 第七章：罐子中的样本 抽样分布是理解统计推断的关键概念。为了理解随机性在抽样时所产生的影响，很多讲解抽样分布的方法都提到了从罐子中抽取弹球或糖果这样的经典例子。通过前面提到的美国人口的例子，我们可以清楚的看到尽管总体的分布不是正态的，但其样本均值的分布却服从正态分布。 想象你拥有一个装满了红色和蓝色糖果的罐子。罐子中放入的红色糖果和蓝色糖果各有100颗，但这些糖果在放入罐中时是混合在一起的。如果你从罐中随机抽取8颗糖果，你会得到什么颜色的糖果？如果抽取如想象中般顺利的话（虽然通常都没那么走运），你将会抽到4颗红糖果和4颗蓝糖果。红的和蓝的各占一半，这个比例和罐中的红蓝比例是一样的。当然了，你很难抽到这个结果，是不？除了4红4蓝的组合，你也可能会抽到3颗红糖果和5颗蓝糖果，或者其他各种你能想到的组合。事实上，尽管概率很小，但你还是有可能抽中8颗红色的。虽然有多种可能，但仅就一次抽取结果来看，我们是无法预测会抽到红和蓝的哪种组合的。而这种不确定性，就是能够以无法预知的方式来影响你抽取结果的随机性的力量。 我们现在提出一个有趣的想法，这虽然对预测单次抽取结果没什么帮助，但却能很好的展示多次抽取的长期趋势。从罐子中抽取8颗糖果，数一数其中有多少红色的，然后将所有的糖果都放回罐中。我们并不需要数蓝色糖果的数量，这是因为 8 - 红色糖果的数量 = 蓝色糖果的数量。将罐中的糖果晃动一下，这样能混合的更好一些，然后再抽取8颗糖果，同时数一下红色糖果的数量。将这种操作重复多次，我们可以对每次记录的结果用下面的表格来表示： 抽取次数 红色糖果的数量 1 5 2 3 3 6 4 2 注意到表格左边的列就是对抽取次数的计数而已。表格右边的列是我们感兴趣的内容，它列出了每次抽样中的红色糖果数量。在本例中，红色糖果的数量真是天南海北。在第4次抽样时，我们只抽到了两颗红色糖果，但是在第3次抽样时，我们却抽到了足足6颗红色糖果。但是本例最有趣的地方就是，如果你将这4次抽取的红色糖果数量进行算术平均，你会得到平均每次抽到正好4颗红色糖果的结果，而这正是我们期望的红蓝各半的罐子中应该抽出的结果。以上只是一个人为制造的例子，而我们在平时抽取时，很难在仅仅4次抽取之后就遇到这么好的平均的结果。但是如果你将随机抽取这一操作，进行了4000次，你将会肯定得到一个接近 平均每次抽到4颗红色糖果 的完美结果。 从总体中不断的抽取子集的过程称为“抽样”，而经过大量的抽样之后，我们会最终得到抽样分布。注意到，在刚才这句话中，我们用到了“总体”这个词汇，用来代表我们从什么地方做的抽样，这也是它的统计含义。在前面章节的例子中，我们的数据集包含美国各个州的人口数量，这个“人口”与“总体”同样使用英文单次 population 可以说是一个巧合。下面，我们使用 R 来帮助我们从美国各州人口数据集中抽取大量样本。 R 中有一个很便捷的函数 sample()，可以用来从数据集中简便的抽取样本。我们可以用它来从我们的数据集中抽取样本： 12set.seed(31415)sample(USstatePops$V1, size = 16, replace = TRUE) 123## [1] 1852994 4339367 3574097 2967297 2915918 4533372 1360301## [8] 5773552 6724540 6346105 3574097 3751351 1052567 37253956## [15] 814180 3046355 (译者注：原著中并未使用set.seed()来设置随机的种子，这会导致读者的可重复性研究非常困难。在本章中我们统一将种子设置为31415。读者也如此设置，便可实现和我们相同的抽样结果。)在本例中，注意到我们调用的sample()函数含有三个参数。第一个参数是数据源。对于第二个和第三个参数，为了让 R 能够正确识别，我们采用了“命名参数”的方式来调用，而不是单靠这些参数在参数列中的位置。参数 size = 16 可以让 R 从总体中抽取16个州的人口作为样本。参数 replace = TRUE 则规定了一种统计学家经常用来简化数学证明的抽样方法。对我们来说，采取放回抽样还是不放回抽样通常没有实际的影响，所以我们直接使用统计学家的常用方法。 相较于给抽取的各色糖果计数，我们在处理类似于各州人口的这类数值时，对其算数平均数，或者称为均值，更感兴趣。于是我们可以让 R 使用mean()来计算所抽取的样本均值： 12set.seed(31415)mean(sample(USstatePops$V1, size = 16, replace = TRUE)) 1## [1] 5617503 从上面的mean()函数可以看出，我们又遇到了函数的嵌套调用了。我们在mean()函数的输出中并未看到从51个州中随机抽取的16个州人口数值，而是看到了mean()函数直接用这16个数来为我们计算出来的均值。如果你的记忆力很好，或者往前面的章节翻一下，你会看到51个数值的均值是 6053834. 于是你会发现我们从这16个州的样本获得的均值真的是离总体的均值相差的有点远。我们是不是应该感到焦虑？当然不是！我们知道，当抽取样本时，无论是抽取糖果，还是抽取各个州，我们的样本均值永远都不会和总体均值正好一样。其实我们并非对某一次抽样的结果感兴趣，而是要专注于长期抽样的结果。于是，我们现在要让 R 来帮我们做重复抽样了，不是抽取1次、4次，而是400次甚至4000次。和其他的编程语言一样，R 有很多方法可以重复某个操作。其中最简单的一个方法非replicate()函数莫属。让我们先尝试一下重复4次： 12set.seed(31415)replicate(4, mean(sample(USstatePops$V1, size = 16, replace = TRUE)), simplify = TRUE) 1## [1] 5617503 3945129 6510150 4407374 简单的不可思议吧！我们仍然使用了刚才用过的命令，即用来计算16个州的均值的mean()函数。只不过这一次，我们将mean()函数作为一个参数放到replicate()函数里面，于是我们才可以一次又一次的求均值。其中的 simplify = TRUE 参数是让 R 将函数运行的结果以一个均值构成的向量形式返回，而这正是我们想要的形式。由于我们只将mean()函数运行了4次，所以我们不会看到满屏的数字。从刚才的尝试来看，我们可以很轻易地将求均值的过程重复400次。你可以自己试一下，看看结果。至于在本书中，我们要将replicate()函数整个塞进另一个mean()函数，于是我们可以得到400个样本均值的平均数。我们来看一下： 123set.seed(31415)mean(replicate(400, mean(sample(USstatePops$V1, size = 16, replace = TRUE)), simplify = TRUE)) 1## [1] 5983180 在上述命令中，最外层的mean()函数地位与之前命令的不同。将这个多层嵌套的命令的效果，用语言来描述一下就是：a)从51个州的总体中抽取400个样本容量为16的样本; b)对每一个样本计算其样本均值，并将均值保存到一个列表中; c)当计算完这400个均值后，计算包含有400个均值的列表的均值。从刚才的计算结果可以看到400个样本的均值是5983180. 虽然这个值和总体均值仍然不相等，但是已经接近了。我们距离总体均值的真值大概70000多，大概是1.16%(更精确的数值为70654/6053834 = 1.167%)。你可能也注意到了，即使你有一个速度较快的电脑，运行刚才的命令也得花一些时间。因为刚才的命令的确包含了很多操作！让我们更进一步，看看我们是否能更加接近总体均值： 123set.seed(31415)mean(replicate(4000, mean(sample(USstatePops$V1, size = 16, replace = TRUE)), simplify = TRUE)) 1## [1] 6052941 现在我们更加接近真值了！我们得到的均值距离总体均值的真值只差不到千分之一！你可能注意到你在运行上述几个命令时得到的结果与我们有少许不同，这是因为你在运行sample()函数时，随机抽取的400或4000个样本与上述命令得到的不同，但是最终均值结论的精确性却相差不多。(译者注：这是由于原著中没有使用set.seed()来明确生成的随机序列，读者如果按照译文中的命令来执行，是会得到与我们完全相同的结论的。我们也建议读者在任何涉及到随机抽样的模拟中，添加set.seed()，以便将来的可重复性研究。) 我们准备进行下一步了。这次我们不用一个简单的算术平均来概括抽样分布的均值了，我们使用更复杂的直方图来观察抽样分布的均值序列。 这个直方图展示了4000个均值的频数分布。对这张图详细观察可以训练你阅读频数直方图的能力。这个直方图具有典型的近乎于钟形的但仍然有点右偏的形状。最高的，也就是频数最大的区间正好和均值真值6053834不远。 另外，你能否自己找出生成这张直方图的命令？你只需要将前面的命令之中的最外层mean()函数替换为hist()函数即可。在本例中，代码如下： 123set.seed(31415)hist(replicate(4000, mean(sample(USstatePops$V1, size = 16, replace = TRUE)), simplify = TRUE), main = "Frequency of 4000 sample mean", xlab = "Sample Mean") 这是一个伟大的时刻，请让我们做一下深呼吸。我们刚刚仅用短短几页就概括了统计思想在数百年间的成果。事实上，我们提到的有两个重要思想，“大数定律”和“中心极限定理”。这两个重要理论的确花费了像Gerolamo Cardano(1501-1576)和Jacob Bernoulli(1645-1705)这些数学家们数个世纪才最终得到。如果你去查阅这些理论，你会发现有很多令人迷惑的数学论证细节，但对我们来说，在这之中是两个浅显的道理。首先，如果你运行了同一个统计过程很多次数，则最后通常会收敛到一个稳定的结果。对本例来说，我们知道50个州加上哥伦比亚特区（即华盛顿首府）的平均人口是多少。这51个观测值就是我们的总体，此时我们想知道要像得到这个均值真值的较好的近似，我们需要抽取多少个容量为16的样本。我们了解到只抽取一个样本会得到较差的结果。抽取400个样本能够得到距离真值只差1.16%的较小的误差。而抽取4000个样本得到的均值距离真值的差距都不超过千分之一。如果我们将抽样过程重复40000次乃至400000次，我们会得到和真值6053384极端接近的所有样本的均值。 其次，当我们将大数定律纳入考虑，并将其运用到样本均值时，我们发现样本均值形成的分布开始向钟形或正态分布靠拢，所有样本均值的均值也和总体均值真值越来越近。样本容量越大，则均值接近真值的速度也越快，相反，当样本容量小的时候，你就得抽取非常多的样本来使得样本均值的均值和总体均值足够接近。为了说明这一点，我们举一个有趣的例子，该例中样本容量大于16. 我们抽取的样本容量为 n = 51(与总体的单位数量相同)，共重复抽取100次： 123set.seed(31415)mean(replicate(100, mean(sample(USstatePops$V1, size = 51, replace = TRUE)), simplify = TRUE)) 1## [1] 6016650 现在我们的计算结果与总体均值已经相差不超过1%了。你可能会觉得有点摸不到头脑，“等一等，难道样本容量为51不就是把总体中所有的单位都抽取了吗？”这的确让人迷惑，但这个问题可以追溯到前几页里我们提到的可放回抽样（当时我们在抽样命令里也是使用了 replace = TRUE 这个参数）。可放回抽样的意思是随着你从总体中抽取了一个单位来放到你的样本中，紧接着你就把它又放回了总体的单位里去，这也意味着你有潜在的可能性在接下来的抽取过程中再次抽中它。如前文所说，我们已经省略掉了相应的证明部分，而这除了会让你迷惑以外，并不产生任何问题。事实上，我们可以让样本容量更大，而不会带来任何麻烦： 123set.seed(31415)mean(replicate(100, mean(sample(USstatePops$V1, size = 120, replace = TRUE)), simplify = TRUE)) 1## [1] 6028504 这个命令对容量为 n = 120 的样本重复抽取了100次。让我们来看看抽样分布的均值和总体均值有多么的接近了！回顾之前我们提到过的，每次你运行这个程序，你都会得到稍微不同的结果，这是因为每次抽取的都是不同的样本（译者注：如果使用了set.seed()就不会遇到这种情况）。但是按照以往的经验，随着统计学家们用 n 来表示的样本容量越来越大，你对总体均值的估计也会越来越接近真值。相应的，如果你抽取更多的样本量，你的估计也是越来越准的。 现在，如果你已经从刚才的状况缓了过来，那么就让我们来进一步的利用抽样分布来做更多的事情吧。首先，让我们将所有样本均值保存下来，以便我们可以对这列数进行下一步的计算： 123set.seed(31415)SampleMeans &lt;- replicate(10000, mean(sample(USstatePops$V1, size = 5, replace = TRUE)), simplify = TRUE) 我们是将所有的抽样均值保存到一个称为“SampleMeans”的新向量中。我们应该有10000个样本均值： 1length(SampleMeans) 1## [1] 10000 这些抽样均值的算术平均数应该和我们的总体均值6053384相当接近： 1mean(SampleMeans) 1## [1] 6062372 你可能还想要对 SampleMeans 生成一个直方图，来看看频数分布是如何的。目前，我们所需要查看的只是这列抽样均值数列的一个总括： 1summary(SampleMeans) 12## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 822000 3880000 5380000 6060000 7650000 24500000 如果你需要回顾中位数（median）和分位数（quantile）的概念，可以回头看看第3章-行和列。 这个总括里包含了很多信息。首先让我们来看看最大值（max）和最小值（min）。这列样本均值的最小值是822000。让我们思考一下，当我们知道总体均值是那么大的时候，怎么会抽取到均值这么小的样本呢？怀俄明州（Wyoming）在那个样本中一定被抽到过好几次吧！答案和伴随着抽样过程的随机性有关。如果你运行某个随机抽样程序10000次，那么你肯定会遇到几个很怪异的样本的。这很像是买彩票。你买的大多数彩票都是正常的——中不了奖的。但是就在某个时刻，你也会遇到那个极其特殊的大奖。抽样过程和此类似：极端特殊的情况是很不寻常的，但是如果你抽样的次数足够多，你也会遇到这种情况的。对于最大值我们也有类似的结论：我们抽到的最大的样本均值24500000，比总体均值大得多。 SampleMeans的中位数5380000和均值6060000比较接近，但并不完全相同。这是因为我们的样本均值分布有点右偏（数值较大的右侧尾巴比对称情况下要长，这是由于原始分布是反向J形的）。中位数是很有用的，这是因为它把所有的样本等分为两半：50%，或者5000个样本均值是比5380000大的，另外一半是比5380000要小的。因此，如果我们此时要从总体中再随机抽取一个样本的话，那么它就有一半的可能性大于中位数。分位数能够帮助我们更好的对数据进行分割。第三分位数将数据较小的75%和较大的25%分开。因此只有25%的样本均值大于7650000。这意味着如果我们再从总体中随机抽取一个样本的话，那么这个样本均值只有25%的可能性是大于7650000的。类似的，第一分位数告诉我们，如果随机抽取一个新的样本的话，那么样本均值只有25%的可能性是比3880000要小的。 有另外一种更加灵活的从R中获取相同信息的方法. quantile()函数可以展示中位数以及分位数, 例如 1quantile(SampleMeans, probs = c(0.25, 0.5, 0.75)) 12## 25% 50% 75% ## 3875658 5379840 7649252 你会注意到这些数值与上文summary()函数展示的数值的少了千分之一(one tenth of one percent). quantile()函数的结果比较精准, 尽管summary()函数的结果可以满足绝大多数的需求. 使用quantile()函数的另一个原因是我们可以精准的控制分割点. 为了得到分位数, 我们在25%, 50%, 75%进行分割. 但是如果我们希望在2.5%和97.5%进行分割该怎么办? 这对于quantile()函数来说很容易: 1quantile(SampleMeans, probs = c(0.025, 0.975)) 12## 2.5% 97.5% ## 2019463 13318627 在这个例子里, 如果我们选取新的样本, 样本均值仅有2.5%的几率低于2019463. 类似的, 样本均值仅有2.5%的可能性会大于13318627(因为有97.5%的数值低于样本均值的分布). 我们来做个练习. 这里的人在一定区域内，其中每一个领域是某种与美国相关联的单元的数目的样本 3,706,690159,358106,40555,51953,883 我们可以很容易的在R中计算样本均值 12MysterySample &lt;- c(3706690, 159358, 106405, 55519, 53883)mean(MysterySample) 1## [1] 816371 我们的神秘样本的均值是 816371. 但问题是, 这是一个美国各州的样本还是其他? 仅有这一个样本很难回答这个问题. 我们首先会观察到样本中含有比Kansas, Utah, Nebraska 和其他几个州都要多的人数. 从上文的人口数据的分布我们也可以看出很多州的人数都很小. 多亏了我们在上文中举的例子使得我们可以更好的进行比较. 我们有样本均值的分布并且我们可以很公平的拿新的样本均值与之对比, 新的均值超出了样本分布的极限区域, 也就是低于2.5%或高于97.5%. 故我们的神秘样本看起来不像是美国各州的取样. 既然这样, 我们可以清楚的看到816371处在样本分布的极低区域. 回想我们使用quantile()函数时我们发现仅2.5%的样本均值低于2019463. 事实上, 我们可以使用更为严格的标准: 1quantile(SampleMeans, probs = c(0.005, 0.995)) 12## 0.5% 99.5% ## 1414671 17084662 quantile()函数显示仅有0.5%的样本均值低于1414671. 因此我们的神秘样本的均值816371如果是美国各州的采样, 那么必然是个小概率事件. 从这一点我们可以基于有力的统计证据来暂时推测, 我们的神秘样本不是美国各州的采样. 神秘样本的均值比美国各州样本的均值小太多. 事实上我们的推测是正确的: 神秘样本包含五个不同的美国领土, 其中包含在太平洋加勒比海地区的Puerto Rico. 那些领土是由一些大陆块和与美国有关系的一群人组成, 但是他们与美国各州不同. 也就是说他们都是岛屿, 因此他们的人口数都限制在大陆块中. 尽管在美国各州中仅夏威夷是岛屿, 但事实上夏威夷岛是美国大陆上各州面积的10倍以上. 这个例子中我们最值得学习的就是这组数据的属性, 尤其是与已知样本均值的分布不同的均值, 可以让我们推断出这组数据并非从原始数据中采样. 这是所有统计推断的基础. 你构建一个比较分布并且划分出一些极限值所在的区域,然后你用新获取到的数据与已知分布进行比较看新样本是否会落入极限区域. 如果新样本符合上述规律, 那么你可以暂时认为新样本是从其他数据源获取到的. 如果你有一些困惑，那么不要气馁. 上一段话代表了四五百年的数学发展. 在没有像R这样的可以创造和分析真实样本分布的工具之前, 上述的绝大多数材料仅是一系列的公式和证明. 在后续的章节里我们会回到用户证明上述描述的特定统计规程. 但是现在我们仅需要记下其他三条信息. 第一, 我们使用mean()函数查看样本均值的分布和使用hist()函数查看它的形状, 但是我们从不量化分布的离散程度. 1sd(SampleMeans) 1## [1] 3016920 这里显示样本分布的标准差. 统计学家称之为”均值的标准误差”. 这个拗口的术语有另外一个清晰但较长的解释: 人口采样的样本分布均值的标准偏差. 不幸的是, 统计学家并非全都知道. 当我们看一个分布并且分布中的每个点都代表一个样本(例如, 一个均值), 那么标准差就可以认为是标准误差. 第二, 发现标准误差其实不需要构造一个10000个（或其他任意个数的）样本均值构成的经验分布. 事实上, 原始数据的标准差和标准误差具有一定的关系: 1sd(USstatePops$V1)/sqrt(5) 1## [1] 3051779 上述公式表示用原始数据的标准差除以样本大小的平方根. 在上文中当我们使用replicate()和sample()命令创建SampleMeans向量时, 我们使用的样本大小n=5. 这就是为什么你在上文中看到sqrt(5)的原因. 在R或者其他软件中sqrt()是”square root”的简写, 而不是你预期的”squirt”. 因此如果你有一系列数据并且你想计算他们的标准差, 你可以计算样本均值分布的标准误差(每一个都有相同的样本大小), 除以样本大小的平方根. 你可能发现通过使用这种方法得出的结果会比直接从样本分布中得到的结果大一些, 但是这种差异没有任何意义(仅是因为分布的随机性导致). 另一个你会注意到的事情是样本空间越大, 标准差越小. 这对于样本的选择有很重要的法则: 样本越大越好. 最后是另外一个简便方法. 我们创建样本分布的97.5%的分割点并且使用quantile()函数计算真实的分割. 我们同样可以使用均值和标准差计算分割点. 均值减去2倍标准差就是2.5%的分割点, 均值加上2倍标准差就是97.5%的分割点. 123StdError &lt;- sd(USstatePops$V1)/sqrt(5)CutPoint975 &lt;- mean(USstatePops$V1) + (2 * StdError)CutPoint975 1## [1] 12157391 你会发现这个数值与我们使用quantile()函数计算全部分布的数值有差别. 这种差异由我们构建的分布的随机性导致. 上述结果仅是基于统计证明的估计值, 而我们创建的SampleMeans 数据集仅是我们能创建的最大的接近无限的集合. 我们可以很容易的通过使用更大的样本容量和添加更多的抽样分布的抽取次数来降低两种方法之间的矛盾. 总结一下, 我们使用了一个有51个美国各州人口数据的数据点和一些R的知识创建了一个样本均值分布, 我们学到了以下知识: 持续运行一个统计过程多次你就可以得到一致的规律. 计算大样本的均值和绘制直方图可以展示样本均值是正态分布并且分布中心与原始数据的均值接近. 样本均值形成的分布可以用来进行比较. 通过在极低的区域和极高的区域使用分割点, 例如2.5%和97.5%, 我们可以把它与新获取的样本进行比较. 如果我们获得新的样本均值, 并且我们发现它掉入我们设定的极限区域, 我们可以暂时认为新样本与之前的数据来自不同数据源. 另外一种更精确的计算标准差的方法是使用原始数据的标准误差. 目前为止我们仍不是统计学家, 但是建立在样本分布上的推理过程是统计推理的核心. 假如你理解了本章的内容, 你已经向成为一个应用统计人才的道路迈出了坚实的一步. 本章测试收集一个包含至少20个数据点的样本并且构造样本分布. 计算标准差和并且使用标准误差计算2.5%和97.5%的分布分割点. 你收集的数据点必须代表相同数量的现象. 例如, 你可以手机20本教科书的价格, 或者统计20段文字中的单词数量. 资源 中心极限定理 Gerolamo Cardano Jacob Bernoulli 大数定理 美国各州及领土人口 中心极限定理 本章使用的R命令 length(). 获得向量中的元素数量. mean(). 算术平均值或者一系列值的均值 quantile(). 基于百分比或比例计算分割点. replicate(). 多次运行一条语句/计算. sample(). 在向量中随机抽取元素. sd(). 计算标准差. sqrt(). 计算平方根. summary(). 返回向量的描述性信息.]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[数据科学(6)-啤酒，农场和豌豆]]></title>
      <url>%2F2014%2F08%2F17%2F2014-8-data-science-chp6-beer%2F</url>
      <content type="text"><![CDATA[本文为Introduction to Data Science一书的翻译，由网友义务完成，了解参加翻译的网友，请点击这里，如果要加入我们请加入qq群171546473,了解翻译规则点击这里 第六章：啤酒，农场和豌豆 很多最简单和最实用的统计方法都来自于出生于十九世纪工业革命时期的四个年轻人。他们大部分的工作都专注于使用收集到的数据来描述观察到的现象并且做出相应的推断，应用到工业和农业生产实际中去解决问题。十八世纪末到十九世纪初，数学和科学取得了突破性的进展。那个年代的科学家和数学家们认为只需要纸、笔和足够的时间，人类所面临的一切问题（包括人类无法突破自身局限的问题）都可以被测量、分解、分析和重建使之更为有效。法兰西斯·高尔顿，卡尔·皮尔逊，威廉·戈塞和罗纳德·费雪这四个英国人成为了当时科学发展观和理想主义信念的代表。 首先要提到的是法兰西斯·高尔顿，大名鼎鼎的查尔斯·达尔文的表兄，虽然名气没有表兄大，但他也是智商超群。高尔顿是一位绅士，通晓拉丁文和希腊语，学过医学和数学，他还是一位著名的非洲探险家。他提出了“优生学”的概念，也因极力倡导优生学而为人所知。优生学是指通过有选择的生育来改善人种。高尔顿研究了豌豆、兔子和人类的遗传特征，推断称我们应该奖励部分人结婚生育，因为他们的后代可以改善人种。这些观念在二战时被纳粹德国错误利用，他们认为自己是优等种族，这也成为他们惨绝人寰的屠杀理由。当然除了优生学之外，高尔顿对数学和统计学的发展也做出了重要和珍贵的贡献，特别是建立了两种如今广泛使用的基础方法：相关性分析和回归分析。 细看高尔顿的研究和理论，客观来讲他本人并不是个出色的数学家，但是他有个年轻的搭档，也就是常被认为是数理统计奠基人的卡尔·皮尔逊。皮尔逊重新处理了相关性和回归分析理论背后的数学推导，并且在数据处理方面做了很多影响至今的工作。和高尔顿一样，皮尔逊也支持优生学的理论，他也因为启发了爱因斯坦的相对论而获得赞誉，同时，他还是一名早期的女权主义的拥护者。 下一位要介绍的是威廉·戈塞，一个数学和化学的奇才。也许因为他在化学方面的专业知识，他大学刚毕业就受聘于爱尔兰都柏林的健力士精酿公司。作为一个前瞻性的公司，健力士酿酒厂此时正想办法解决大批量生产精酿啤酒时品质波动的问题。戈塞加入公司之后提出了一种现在被称为小样本抽样的统计方法，也就是分析容量相对较小的样本，得到对整体的估计结果。当然，大批量精酿啤酒是个耗时且昂贵的工艺流程，所以要想从小样本抽样分析得到对整体质量的估计，戈塞只好研究概率对在决定一批啤酒质量所起到的作用。健力士公司并不赞成他把这项研究的结果对学术界公开，无奈的戈塞用了“学生”这一谦虚的化名将他的方法公之于众。“学生t检验”中的“学生”就是由此而来的。 最后要介绍的这位十八世纪的数学家是罗纳德·费雪，他同时也研究生物和遗传学。和高尔顿不同，费雪并不是一名富有的绅士，事实上,他和夫人刚结婚时是两个靠干农活维持生计农民。在费雪的众多论文中，有一篇是在洛桑试验站投稿发表的。在洛桑试验站，他研究粮食产量波动的数据，提出了一个非常重要的统计分析方法–方差分析。费雪还是实验设计的先驱，提出了我们在前几章提到过的要素、水平、实验组和对照组等方面内容。 当然，十九、二十世纪期间对实用统计做出贡献的数学家肯定不止以上提到的四位，但是这四位在应用数理和统计到其他科学研究方面确实颇有地位（所以“啤酒、农场和豌豆”也是一个好的章节名）。 纵观这四位的研究，问题的关键在于他们手头上能拿到的数据“样本”和可能或者的确存在的大的数据总量在数量上有很大区别。当戈塞在啤酒厂批量精酿啤酒时，他知道穷尽所有种原料配比和制备方法来获取数据是不现实的。戈塞明白他只能小批次取样分析，描述数据然后来推测在将来的批次可能发生的情况。这也是处理各种类型、各种容量数据时我们面临的窘境：不论你有什么样的数据，总会有更多没有被取样的数据。你可以通过改变测量和处理方法来获取数据，但未来还会有更多没有被收集到的数据，也许永远不会被收集。甚至于我们使用了完全相同的数据收集方法，但是数据却仍然会因随机性而不同。不管你有什么数据，它只是总体的一个快照，或者说一个样本。这让我们明白我们无法完全信任我们手上的数据；我们必须有所保留，记住在数据中总有不确定性。统计学提供了很多强大的统计工具，这些费雪们开发的实用工具帮助人们描述并量化数据中的不确定性，帮助我们了解对单一样本的信任程度。所以请记住：尽管我们可以描述我们手上的样本数据，但最大的挑战在于如何推测我们无法获取的更大的总量数据的意义。这正是描述统计学和推断统计学之间最重要的差别。 在前面几章我们已经学习过几种描述统计学概念，下面从实际应用角度重新定义了这几种概念： 平均数（严格说是算术平均数）是表征数据集中趋势的一个指标。 它是由观测数据之和除以这组数据的观测个数得到的。 中位数也可以表征数据集中的趋势，但它不能直接计算得出。把所有观察值由高到低排序后，位于正中间的那个数就是中位数。 全距，又称极差，是表征离散程度的一个指标，通过计算样本中最大值与最小值之间的差得到。 在这个列表中我们应该再加上三个在不同情况下会遇到的概念： 众数，另一个表征集中趋势的指标，指一组数据中出现次数最多的值。和中位数一样，众数也无法被直接计算得出。你得数出每个变量出现的个数，然后那个出现次数最多的数就是众数。 方差，测量样本数据的离散程度。和全距类似，方差也描述样本的展开程度，然而与全距仅仅由最大最小数之差来描述不同，方差是各个数据与平均数之差的平方和的平均数。前面章节提到过家庭成员年龄的数据，他们的平均年龄是22岁。如果你还能想起细节，那你就能读懂如下的表格： 这个表格演示了如何计算方差：先计算各个数据与平均数之间的偏离量，然后将差值平方（乘以自己）来消除负数偏离量的影响（比如弟弟的偏离量为-14）。然后我们求出偏离量的平方和，最后除以观察值的个数来得到“平均的偏移量平方”。注意，将平方和除以4而不是5并不是一个错误，当我们在后面谈到自由度这个概念的时候你就会明白这么做的原因。这样我们就得到了方差这个应用广泛的统计概念。虽然在数学上很有用，但联系实际方差看起来并不能描述所有问题。比如从家庭成员年龄的数据，得到对平均年龄有356.5“平方年”的偏离量，看起来怪怪的。平方米或许容易理解，但那是另一码事情了。所以为了应付这种尴尬，统计学家又提供了一种更容易理解的概念： 标准差，另一个描述离散程度的特征量，且和方差联系紧密。标准差其实就是对方差进行开平方，消除了“平方年”这种尴尬的单位带来的困惑。在上面的例子里，标准差约是18.88年（保留两位小数，在这种情况已经足够）。 下面我们来看看在R中如何计算这些数值： 12&gt; var(myFamily$myFamilyAges)[1] 365.5 12&gt; sd(myFamily$myFamilyAges)[1] 18.88121 注意这里使用了前面章节用到的数据，通过使用“$”来获取数据框的变量。如果你没有前面章节的数据，你也可以这样做： 12&gt; var(c(43, 42, 12, 8, 5))[1] 365.5 12&gt; sd(c(43, 42, 12, 8, 5))[1] 18.88121 这里例子真是很无聊，而且对后面章节的内容也没有太多用，所以下面我们换一组数据来分析。我们要使用Windows或者Mac的剪切板程序来复制粘贴一组大一点的数据到R里。进入美国统计局网站，那里存储了许多人口数据： http://www.census.gov/popest/data/national/totals/2011/index.html 如果你有电子表格程序，点击XLS链接（“Annual Estimates of the Resident Population for the United States”）。当表格打开之后，选择五十个州的人口估算数据。前几行数据应该像下面这样（2011年数据）： 为了使用下面介绍的R命令，请只选择数字而不是文本。在你复制数字之前，把单元格类型改为“常规”来去除逗号。通常可以在格式菜单里选择修改单元格，当然你也可以在工具栏里很容易找到相关按钮。把数字复制到剪切板中（Windows用户使用快捷键Ctrl + C，Mac用户使用快捷键command + C）。在Windows上使用如下命令： 1read.DIF("clipboard", transpose=TRUE) 在Mac电脑上使用下面代码： 1read.table(pipe("pbpaste")) 在不同类型电脑上使用不同命令很烦人，谁让微软和苹果的工程师使用不同的方法来设置剪切板呢，这种副作用无法避免，况且R又是跨平台设计，只能屈从于不同的平台采用不同的代码了。不管怎么说，你现在应该能在R的控制台看到很长一串人口数量数据输出了。光是这串数字其实没什么用，下面我们把他们放到一个向量里面： 在Windows上使用read.DIF: 123456789&gt; USstatePops &lt;- +read.DIF("clipboard", transpose=TRUE)&gt; USstatePops V11 47797362 7102311 6392017... 在Mac电脑上使用read.table: 12345678&gt; USstatePops &lt;- read.table(pipe("pbpaste"))&gt; USstatePops V11 47797362 7102311 6392017... 这里因篇幅所限只显示前三行数据。你的R的输出应该是全部列表。注意其实这里和之前章节的不同不过是我们使用了read.DIF()或者read.table()函数来获取更大一点的数据，而不需要手动输入。像read.table()这样的函数在今后使用R的过程中显得十分重要，因为这些函数让我们能把从外部文件存储的数据读入R的存储空间来进行数据分析。如果在使用中出现问题，你可以复制粘贴本章最后的“如果失败了”部分的命令代码来把相同的数据输入到R里。 注意我们使用赋值符号&lt;-把read.DIF()或者read.table()的结果放到了一个R数据对象中。现在你可以使用前面章节学到的str()和summary()函数来对这个新的数据对象USstatePops进行练习了。你有没有发现这些函数返回的结果很有趣？你会发现这里有51个观测值而不是50个，你知道为什么吗？如果一时想不出，你可以看看网站或者是表格里的原始数据。另外你会发现USstatePops是一个数据框，而不是一个向量。你可以从上面的输出看出来：在第二行代码我们要求R返回USstatePops变量里存储的内容，R输出了一列，列名称为V1。因为我们没有给这个列命名，于是R缺省地将此列命名为V1，即Variable One的缩写。所以当我们想要获取这一列数据的时候，我们就可以使用USstatePop$V1。如果这听起来挺陌生的，那请在看一看前面“行与列”那章关于如何获取数据框中一列的内容。 现在我们已经准备好在这个稍微大一点的数据上小试牛刀了。首先是对各州人口的一些描述性统计量： 12345678910&gt; mean(USstatePops$V1)[1] 6053834&gt; median(USstatePops$V1)[1] 4339367&gt; mode(USstatePops$V1)[1] "numeric"&gt; var(USstatePops$V1)[1] 4.656676e+13&gt; sd(USstatePops$V1)[1] 6823984 上面的总结内容看起来很棒，但是等一下，有些返回值好像跟想象有所不同： mode()函数返回了数据向量的类型，而不是统计变量：众数。这挺怪的，但是确是对的：基础的R包其实并没有一个计算众数的函数！这是因为众数只少数情况下使用。我们在后面的章节会告诉你如何添加R的包来获取新的R函数，其中就包括计算众数的函数。 方差的返回值是4.656676e+13。这是在本书中我们第一次看到R中使用科学计数法。如果你以前没看过这种计数方法，没关系，你可以这么想：把4.656676乘以10000000000000（也就是10的13次方）。这个数太大了，而且写起来很累，所以使用科学记数法很省事。如果你不想在计算器上按照科学计数法来输入，还有一个窍门就是把小数点向右移13位。 除了这两点，我们现在知道美国各州人口平均数为6053834，标准差为6823984。你可能会想标准差接近七百万意味着什么？平均数和标准差的数值都没问题，而且他们还十分精确，但是对我们大多数人来说，用一张图来展示这种数据集中趋势和离散程度会更为直观。那么我们就来画一张直方图。运行如下代码： 1hist(USstatePops$V1) 在图片输出那里你应该能看到如下图片 直方图是专门用来显示“频率”的条形统计图表。频率在这里指在一个数据集合中一个特定的数值或者数值空间出现的次数。上面的直方图很有趣，大约有30个州人口不足500万，另外有10个州人口不到1000万，剩下很少一部分州人口超过1000万。那么我们到底该如何从统计图表中提取这些信息呢？首先顺着Y轴（左面垂直的轴线）读取频率信息。最高的竖长条就挨着Y轴，最高点接近Y轴的标记30。顺着X轴（底部水平的轴线）可以读出这个竖长条所表示的内容了，注意到每两个竖长条之间就会有一个X轴的科学计数标记，第一个是1e+07，也就是1000万.所以每出现一个新的竖长条（或者没有数据）代表人口增加500万。知道X，Y轴标记代表的意义之后就很容易推断出大约有30个州人口不足500万这个结论了。 如果你考虑总统选举，或者学校或者公司的地址，或者拿美国的一个州和世界上其他国家的比较，你得知道美国有两个特别大的州，以及一群特别小的州。当你能熟练阅读直方图之后，这些信息就会一目了然了。 另一个角度来说，这张统计直方图还有点不尽如人意。超40个州都挤到最前面几个竖长条里了，这其中应该还有许多我们想知道的隐藏信息。我们可以通过改变竖长条的个数来揭示这些隐藏的内容。在刚的那直方图中，一共有8个直方长条，那么为什么R会选择8呢？ 答案就在于hist()函数有自己默认的计算长条数量的算法，观测值的数量、数据的分布以及空缺的区间都被这个算法考虑在内。幸运的是我们可以通过调整breaks这个参数让R调整直方长条的数量，就像下面的代码： 1hist(USstatePops$V1, breaks = 20) 这样X轴每两个标记之间就会有5个直方长条，或者说每个直方长条代表了200万的人口增长。那么新得到的这个直方图还是告诉我们以前一样的信息：有15个州的人口低于200万。这种图样表示了一种统计分布，从左开始有很高的频率，然后向右迅速跌落。你可以称这种分布为“倒J”分布，因为它的形状很像倒下来的英文字母“J”。统计学中，这种分布被称为帕累托分布（以意大利经济学家维弗雷多·帕雷托命名）。现在我们不必太过纠结为什么人口分布符合帕累托分布，但是我们可以猜测为什么会出现这样的分布图样。首先每个州的人口数量都是正数，不会出现零或者负人口，否则也不太合情理了。那么一个州总得有些人住，而且如果你回顾美国历史，每个州都起源于有人居住的殖民地或者领地。换个角度想，是什么因素导致某些州拥有巨大人口的呢？首先你得有足够的土地，然后有足够吸引人们搬到这个州或者在这个州出生的原因。同时也有很多制约人口增长的因素：比如罗德岛地盘小装不了太多人口，再比如阿拉斯加尽管地盘挺大，却因为太冷了很少有人愿意搬过去。所以每个州刚开始的时候人口都挺少的，随着时间推移人口数量有所上升，但很难增长太多。因此，在这个分布中大多数州都聚集在靠近Y轴的区域，只有少数州人口不断增长，成为人口大州。在这些州中，有只能有极少的几个州人口能接近4000万，事实上只有一个州实现了这个目标。顺带问一句，你知道或者你能猜出这个超级大州是哪个州吗？ 除了帕累托分布，还有许多其他的分布图样。最普通的也是大家最常听说的是“钟”形分布，因为分布的曲线长得像钟。统计学中，这种分布被称为正态（常态）分布。“常态”这个词是卡尔·弗里德里希·高斯（1777-1855）第一次提出的，也许他认为自然现象研究中，这种分布最为典型，在常态下最容易发生。下面的直方图就体现了这种典型的钟形正态分布图样。 如果你很好奇，你就会想到底R是怎么弄出上面这个直方图的呢？而且如果你仔细观察，你就会发现这张图里有好几处都出现了”rnorm”这个词。其实这是R另外一个很棒的功能：当解决问题或者做展示的时候，可以轻易利用R生成“假”数据来辅助说明。上面直方图所使用的数据是R中的rnorm()函数随机生成的，这些数据被用来拟合图示的正态分布（数据越多图样越接近正态分布）。更多对rnorm()命令的解释有助于理解这一过程，如果你还记得上面用到的美国人口数据，平均数是6043834，标准差是6823984，使用下面的命令就能得到刚才的正态分布直方图： 1hist(rnorm(51, 6043834, 6823984)) 这里要介绍两个很重要的概念。一是嵌套函数：rnorm()函数生成了用来绘制直方图的随机“假”数据，hist()函数利用数据生成对应的直方图（要加倍注意括号的位置）。R先运行里面的函数rnorm()，生成的数据直接并且立即传送给hist()函数绘图。 还有个重要概念是在“传送”到rnorm()函数里的“参数”。其实在刚才我们调用read.DIF()和read.table()的时候就已经在使用函数参数的概念了，只是我们没有提出来。计算机科学家常使用“参数”来告诉函数进行运算的附加信息。在这个例子里，我们传递了三个参数给rnorm()：生成假数据的观察个数、分布的平均值和标准差。rnorm()函数使用了这三个参数来生成51个随机数据，这些数据粗略来说可以拟合一个正态分布。因此直方图里面的数据其实是利用“假”数据来演示按照正态分布的51州人口数据的形态。 在应用统计学中，正态分布被广泛用于比较。如果你看一下刚才那张正态分布直方图中最右面的那个直方长条，它旁边的那个标签是3e+07，或者说3000万。我们从真实的人口数据中得到的结论是只有一个州的人口超过了3000万（如果你还没查，那个州是加利佛尼亚州）。所以如果有个人告诉你他（她）们州人口有3000多万，但他（她）不住加州，那么你自动联想：“这不太正常的啊，我不相信”。你不相信的原因就是你有个分布函数来作对比。分布函数不仅仅拥有一个特征形状，而且有一个中值点（平均值）和一个分布空间（标准差）。有了这三个信息，你有拥有了对比数据的武器。 下一张我们会做一些上述提到的数据对比分析，看看我们能基于现有数据的子集（统计学家称为样本）来推断整体的关系。 本章挑战这一章我们使用了rnorm()函数来生成一系列近似拟合正态分布的数据。我们也知道了人口数据是帕累托分布。查一查哪种R函数可以生成近似拟合帕累托分布的随机数据。然后传递正确的参数到该函数生成51个随机数据（提示：可以尝试不同的概率值）。利用这些数据绘制直方图，并描述分布的形状。 参考资料http://en.wikipedia.org/wiki/Carl_Friedrich_Gauss http://en.wikipedia.org/wiki/Francis_Galton http://en.wikipedia.org/wiki/Pareto_distribution http://en.wikipedia.org/wiki/Karl_Pearson http://en.wikipedia.org/wiki/Ronald_Fisher http://en.wikipedia.org/wiki/William_Sealy_Gosset http://en.wikipedia.org/wiki/Normal_distribution http://stat.ethz.ch/R-manual/R-devel/library/utils/html/read.table.html http://www.census.gov/popest/data/national/totals/2011/index.html http://www.r-tutor.com/elementary-statistics/numerical-measures/standard-deviation 复习 6.1 啤酒，农场和豌豆用来展示数值型变量频率的条形图称为： A.Histogram B.Pictogram C.Bar Graph D.Bar Chart 本章使用的R函数read.DIF() 将数据读入可交换模式read.table() 从外部资源将数据读入数据表格mean() 计算算术平均数median() 找到一组数据的中位数mode() 返回数据对象的数据类型 注：不返回统计众数var() 计算样本方差sd() 计算样本标准差hist() 绘制统计直方图 自我测试如果失败了如果你在使用read.DIF()或者read.table()时遇到困难，下面的代码可以被复制粘贴（如果不能就只好手动输入）到R的控制台，来生成本章需要使用的数据。 123V1 &lt;- c(4779736,710231,6392017,2915918,37253956,5029196,3574097,897934,601723,18801310,9687653, 1360301,1567582,12830632,6483802,3046355,2853118,4339367,4533372,1328361,5773552,6547629,9883640,5303925,2967297,5988927,989415,1826341,2700551,1316470,8791894,2059179,19378102,9535483,672591,11536504,3751351,3831074,12702379,1052567,4625364,814180,6346105,25145561,2763885,625741,8001024,6724540,1852994,5686986,563626)USstatePops &lt;- data.frame(V1)]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[数据科学(5)-行与列-1]]></title>
      <url>%2F2014%2F08%2F17%2F2014-8-data-science-chp5-row-col-1%2F</url>
      <content type="text"><![CDATA[本文为Introduction to Data Science一书的翻译，由网友义务完成，了解参加翻译的网友，请点击这里，如果要加入我们请加入qq群171546473,了解翻译规则点击这里 第五章 行与列 ###表现数据的最基础和应用最广的方法之一就是使用行与列，每一行是一个案例或是实例，每一列是一个变量或是属性。尽管电子表格并不常常提到案例或变量的概念，但是大部分电子表格依然用行与列的形式来组织数据。R语言中行与列的的形式可以通过数据框这种对象来表现出来。 我们生活在一个三维的世界中，在这个世界里，麦片盒有着高度，宽度，深度。然而一个可悲的事实是，现实生活中也存在着诸如纸张，黑板，白色写字板和电脑屏幕这些只有二维的物体。其结果是，大部分统计学家，会计，计算机科学家和与大量数据打交道的工程师倾向于用行与列的形式来组织数据。 这种形式除了使得用数字填写一张矩形纸片的过程变得容易之外，再无更好的使用理由。行与列可以通过你想要的任意方式来组织数据。但是最常见的方式是，行表示案例或实例，列则表示属性或是变量。我们来看下这种的行与列的二维呈现形式。 一目了然，对吧？最上面粗体字那行并不是数据的一部分，它包含了属性或变量的名称。注意，计算机科学家喜欢叫它们为属性，而统计学家则喜欢叫它们为变量，这两种术语都行。比如说，年龄是每个活物都具有的一个属性，你可以用分，时，日，月，年，或者其它的时间计量单位来度量它。在上表中，我们用年来度量年龄这个属性。从技术上讲，在顶行的变量名称是元数据，或者你也可以把它当作数据的数据。试想一下，如果没有元数据，凭空去理解上面这张表中的数据是很难的。元数据有很多种不同的类型，变量名称只是元数据类型中的一种简单形式而已。 如果你无视包含有变量名称的顶行，那么剩下的每一行则是一个案例或实例。再次提醒一下，计算机学家可能会称它为实例，统计学家则可能会称它为案例，但是两种术语都行。重要的是，每一行对应着一个实际事物。在这个例子中，我们的所有的事物都是来自于同一个家庭的生物。你可以把NAME列看成一个案例标签列，每个标签对应着且只对应着我们数据中的唯一一行。当你在处理大型数据集的时候，大部分情况下，案例标签都是数字，每个案例的数字都是独一无二的（换句话说，相同的数字不会在两行或者两行以上中出现）。计算机学家有时把这列独一无二的数字称为键，键是一个很有用的东西，尤其是在从不同的数据源中匹配事物时，我们稍后再讨论这个问题。现在，只要关注这个信息，即虽然他们都是男性,但”Dad”行明显不同于”BRO”行。即使我们再加一个”Uncle”行，与”Dad”有着相同的年龄，性别，体重，我们仍然能够能把这两行分开，因为一行有着”Dad”的名称，而另一行的名称则为”Uncle”。 另一个重要的提示：每一列都包含着相同类型的数据。举例来说，年龄这列都是数字。这列中并没有诸如“老”或者“年轻”这样的词汇。这是一种使事物有组织的一种真正有价值的方式。毕竟，如果这列有一些诸如老或年轻之类的小段文本的话，我们在年龄这列不能运行mean()这个函数。作一个相关的提醒，每个元胞（行与列的交集处，举例说，姐姐的年龄）包含着一条信息。尽管电子表或者字处理程序可能允许我们将两个或两个以上事物放到元胞中，但实数处理程序却不会允许我们这样的行为。最后，我们可以看到每列有着相同的数字输入，以致于整体形成了一个漂亮的矩形。统计学家和处理大型数据库的那些人在处理数据集的时候，他们期望能见到这种矩形的组织结构。 现在我们来看下，怎样把数据以行与列的形式输入R中。你在R中很快就可以学到的一件事是，实现一个目标往往有多种方法。有时，最快或者最有效的方法并不是最容易理解的方法。在此例中，我们将逐列建立每列，然后将它们合并于一个数据框中。这种方法有一些费工费力，同时并不是我们处理一个数据集的常规方法，然而，这种方法易于理解。首先，我们运行下列命令来创建列的名称: 1myFamilyNames &lt;- c("Dad","Mom","Sis","Bro","Dog") 你可能要注意的一件事是每个名字都被放到双引号之中。这个双引号的意思是你给了R一个信号，表示你想要把双引号中的内容看作一个字符串，而非存储地点的名字。如果我们要求R使用Dad而非“Dad”,它将会寻找一个叫Dad的存储位置（即一个数据对象）。需要注意的另一件事情是双引号之外不同的值之间用逗号分隔开。如果你写了一个普通的句子，那么这件事无关紧要，但是对于计算机编程来说，逗号只有不在引号之间的时候，它才能分开不同的值。一旦你键入上面那行代码，随后要记得输入下面这行命令来检查myFamilyNames的内容 &gt; myFamilyNames 输出应如下： 1## [1] &quot;Dad&quot; &quot;Mom&quot; &quot;Sis&quot; &quot;Bro&quot; &quot;Dog&quot; 下一步，你可以建立一个关于家庭成员年龄的向量，就像这样： 1myFamilyAges &lt;- c(43, 42, 12, 8, 5) 注意，这个命令同我们先前几章用到的命令是完全相同的。如果你一直在运行R的话，那么你现在无需再次输入这行命令。因为myFamilyAges一直都在。实际上，如果你关闭了R，当处理完先前几章的例子后，你关闭了R， 系统提示“保存工作区”。如果你这样做了，R会存储所有的你在先前几节使用的数据对象。你可以在一个空白的命令行上输入myFamilyAges来检查。输出应如下： 1myFamilyAges 1## [1] 43 42 12 8 5 嘿，现在你已经使用c()函数和赋值箭头来为myFamilyNames和myFamilyAges来赋值。如果你在这章的先前部分看到这张数据表。你应该能够想出创建myFamilyGenders和myFamilyWeights的命令语句。如果你遇到了困难，那么你也可以在下一页中找到相应的答案。但是你在翻到下一页之前，还是应该努力找出答案。在你键入命令来建立新的数据对象之后的每一个案例，你也应该在命令行中键入数据对象的名称以确保它看起来像那么回事。对于变量，每一个变量有五个值。两个变量是字符型数据，另两个变量则是整数型数据。以防你需要它们，下面给出这两个额外的命令语句： 123myFamilyGenders &lt;- c("Male","Female","Female","Male","Female")myFamilyWeights &lt;- c(188,136,83,61,44) 现在，我们来介绍一下数据框。在R中，数据框是一个列表(关于列的），列表中的每个元素都是一个向量。每个向量都具有相同的长度，这样我们才能让我们的漂亮矩形的行和列建立起来。一般每个向量也有它自己的名称。建立一个数据框的命令很简单： 12myFamily &lt;- data.frame(myFamilyNames, + myFamilyAges, myFamilyGenders, myFamilyWeights) 注意哦，上面这个命令有点略长，所以我们不得不把它断成两行。第一行行尾的加号告诉R在处理命令前要等下一行的输入。当然了，如果你想的话，你可以把所有的命令都塞到一行中，但是这样做的话呢，就得把“+”去掉。无论如何，data.frame()函数可以把我们先前输入的四个向量变成一个数据框。当然我们也可以使用赋值箭头来生成一个新的储存位置来放入数据框。这种新的数据对象就是我们所说的数据框，它的名称叫做myFamily。一旦你输入了上述命令，可以在命令行中输入myFamily来得到一个数据框内容的回馈。你看到的输出应如下： 1myFamily 123456## myFamilyNames X.myFamilyAges myFamilyGenders myFamilyWeights## 1 Dad 43 Male 188## 2 Mom 42 Female 136## 3 Sis 12 Female 83## 4 Bro 8 Male 61## 5 Dog 5 Female 44 看起来不错哈。注意哦，R把行数（也就是那1，2，3，4，5）放在了我们数据每一行的前面。这不同于我们先前在[]括号中看到的数字，因为这些数字是数据框中的实际“索引”。换句话来说，这些行号是R用来追踪某一特定的数据在哪一行。 像这样的小型数据集，只有五行，对所有的数据进行一览也很简单。但是当我们处理一个大型数据集的时候，这种方法也不实际。我们需要其他的方法来总结这些数据集。第一种方法是用来来显示结构的类型，结构就是是R用来存储数据对象的 1str(myFamily) 12345## &apos;data.frame&apos;: 5 obs. of 4 variables:## $ myFamilyNames : Factor w/ 5 levels &quot;Bro&quot;,&quot;Dad&quot;,&quot;Dog&quot;,..: 2 4 5 1 3## $ X.myFamilyAges : num 43 42 12 8 5## $ myFamilyGenders: Factor w/ 2 levels &quot;Female&quot;,&quot;Male&quot;: 2 1 1 2 1## $ myFamilyWeights: num 188 136 83 61 44 在此献上首次提醒，这个例子表明，命令提示”&gt;”是用来区分命令与其后的输出结果的。你无需键入这个符号，当R准备接受新的输入时，它会自动产生这个符号。这本书从现在开始的例子中，R命令将与输出结果混合在一起，所以哦，要时刻注意”&gt;”这个符号，因为在这个符号之后的命令才是你所要键入的。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[数据科学(5)-行与列-2]]></title>
      <url>%2F2014%2F08%2F17%2F2014-8-data-science-chp5-row-col-2%2F</url>
      <content type="text"><![CDATA[本文为Introduction to Data Science一书的翻译，由网友义务完成，了解参加翻译的网友，请点击这里，如果要加入我们请加入qq群171546473,了解翻译规则点击这里 str()函数展现了括号内数据对象的结构。在这个例子中，我们可以肯定myFamily是一个数据框，因为我们先前的命令建立了这个数据框。但是，未来会遇到各种各样的情况，我们无法确定R是怎样建立一个新的数据对象。因此，使用 str()函数随时了解数据对象的结构很重要。 在第一行输出结果中，我们可以确定myFamily是一个数据框，其中包含了5个观测值（统计师通常使用观测值而不是案例或实例）和4个变量。”$” 符号将后续的输出信息分为四个部分，分别对应4个变量。这四个部分分别描述了myFamily数据框对象的列信息。 根据R的输出格式，变量名紧跟着冒号和数据类型。 123456myFamilyNames&lt;-c(&quot;Dad&quot;,&quot;Mom&quot;,&quot;Sis&quot;,&quot;Bro&quot;,&quot;Dog&quot;)myFamilyAges&lt;-c(43,42,12,8,5)myFamilyGenders&lt;-c(&quot;Male&quot;,&quot;Female&quot;,&quot;Female&quot;,&quot;Male&quot;,&quot;Female&quot;)myFamilyWeights&lt;-c(188,136,83,61,44)myFamily&lt;-data.frame(myFamilyNames,myFamilyAges,myFamilyGenders,myFamilyWeights)str(myFamily) 1$ myFamilyGenders: Factor w/ 2 levels &quot;Female&quot;,&quot;Male&quot;: 2 1 1 2 1 例如，myFamilyGenders显示的结果是”factor”(因子)。”因子”是R的一个专业术语，是一类特殊类型的标签，用于识别和管理观测值。R按照字母顺序排列呈现开头部分少许的观测值（由于例子中的数据框比较小，现在呈现的是数据框内所有的观测值。）变量myFamilyGenders一共有两个水平，表示这个变量一共有两个选项：”female”和”male”。R会分配一个从1开始的数字给每个水平。所以，在这个例子中，1表示观测值”female”，2表示观测值”male”（因为female比male先出现在字母表，所以Female是第一个因子标签表示为1。）如果仔细思考的话，你会产生这个的疑问为什么我们输入一些字符串文本例如”male”但是R会将字符串文本转换成为数字并称之为”因子”。这是由于统计的起源。多年来，研究人员习惯将实验组称之为”Exp”,对照组称之为”Ctl”,不使用文本字符串以外的其他标签。因此，R假定，当你输入一些字符串的时候，例如”male”，你想表达的是一个组的标签，R会将”male”当做因子的一个水平，除非告诉R你不想将字符串转化为水平。如果你不想将字符串转化为水平，可以使用 data.frame()函数的一个选项： stringsAsFactors=FALSE。我们之后会深入研究选项和默认值。 这个真的是很复杂。相比之下，两个数值变量myFamilyAges和myFamilyWeights就简单地多了。我们可以看到冒号后面的数据类型是“num”(数值)和开头部分少许的观测值： 123456myFamilyNames&lt;-c(&quot;Dad&quot;,&quot;Mom&quot;,&quot;Sis&quot;,&quot;Bro&quot;,&quot;Dog&quot;)myFamilyAges&lt;-c(43,42,12,8,5)myFamilyGenders&lt;-c(&quot;Male&quot;,&quot;Female&quot;,&quot;Female&quot;,&quot;Male&quot;,&quot;Female&quot;)myFamilyWeights&lt;-c(188,136,83,61,44)myFamily&lt;-data.frame(myFamilyNames,myFamilyAges,myFamilyGenders,myFamilyWeights)str(myFamily) 1$ myFamilyAges : num 43 42 12 8 5 综合所有的输出结果，我们得到了数据框myFamily的详尽信息，同时可以使用这些信息做一些深入的分析。综合上述信息可知，R使用一些难懂的标签和转化规则。R是为专业人员设计的而不是为业余人员设计的，所以我们要好好学习，早日成为专业人员。 接下来，我们要研究另一个非常有用的函数 summary(). summary()函数和 str()所呈现的信息有共同点，但是summary()会提供更多的信息，特别对数值型变量。下面是我们所得的： 123456myFamilyNames&lt;-c(&quot;Dad&quot;,&quot;Mom&quot;,&quot;Sis&quot;,&quot;Bro&quot;,&quot;Dog&quot;)myFamilyAges&lt;-c(43,42,12,8,5)myFamilyGenders&lt;-c(&quot;Male&quot;,&quot;Female&quot;,&quot;Female&quot;,&quot;Male&quot;,&quot;Female&quot;)myFamilyWeights&lt;-c(188,136,83,61,44)myFamily&lt;-data.frame(myFamilyNames,myFamilyAges,myFamilyGenders,myFamilyWeights)summary(myFamily) 为了更好地展示输出结果，这些列做了一定的调整。列/变量的名字显示了关于这列的信息，每一格所涵盖的信息都是独立于其他的。（所以是没有任何意义的。例如，”Bro:1”和”Min.”在同一行输出）。值得注意的是，str()输出结果有很大不同，取决于我们所说的因子，例如myFamilyNames和myFamilyGenders，和数值变量如myFamilyAges和myFamilyWeights。因子所在的列列出了水平的名称和观测值出现的次数。例如，在myFamilyGenders列中有3个女性和2个男性。相反，在数值型变量，我们得到5个不同的分位数来总结这个变量。下面是这个五个分位数的介绍： “Min”(最小值)指的是观测值中的最小值。在这个数据框中，5是狗狗的年纪，也是家庭成员中年纪最小的成员。 “1st Qu.” (第一四分位数)指的是第一个四分之一组的上限值。如果我们观测所有的观测值，并将它们按年纪大小（或者体重轻重）排序分成四组，每一组都含有相同个数的观测值。 第一四分位数 第二四分位数 第三四分位数 第四四分位数 最小25%的观测值 小于中位数25%的观测值 大于中位数25%的观测值 大于中位数25%的观测值 就像数轴一样，最小的观测值在左边最大的观测值在最右边。我们看一下myFamilyAges 最左边这组（包含了四分之一观测值），它的下限是5（狗狗的年龄），它的上限是8（哥哥的年龄）。所以年龄或其他变量的”第一个四分位数”值是区分第一个四分之一组和其他四分之一组的值。值得注意的是，如果观测值的个数不能被四整除，这个值是一个近似值。 “Median”(中位数)指的是将所有观测值平均分为两部分的值，即有一半的观测值大于中位数，另一半观测值小于中位数。如果在深入思考一下，中位数也是区分第二个四分之一组和第三个四分之一组的值。 “Mean” (平均值)如之前所学的是所有观测值的平均值。例如，这个家庭的平均年龄是22岁。 “3rd Qu.” 是第三四分位数。如果你还记得第一四分位数和中位数，那第三四分位数是区分第三个四分之一组和第四个四分之一组的值。你或许会对这些分位数产生疑惑，他们为什么有用。统计学家很喜欢这些，因为他们可以初步展现数据的分布情况。大家都有排序或分割东西的经历，如分披萨，理牌，给队员分组。对大多数人来说，已知四个相等大小的组，知道需要多大年纪多少体重（或者其他变量）可以进入下一个组是很有用的信息。 最后，”Max” (最大值)是观测值中最大的值。例如，在这个数据框中父亲的体重是最大的，有188.看上去是个比较苗条的人。 本章结束前的最后一点：如何获取存储在新数据框内的变量。R使用向量列表存储在数据框内，我们可以使用数据框的名字和向量的名字和”$”符合将两者相关联起来，例如： myFamilyNames&lt;-c(&quot;Dad&quot;,&quot;Mom&quot;,&quot;Sis&quot;,&quot;Bro&quot;,&quot;Dog&quot;) myFamilyAges&lt;-c(43,42,12,8,5) myFamilyGenders&lt;-c(&quot;Male&quot;,&quot;Female&quot;,&quot;Female&quot;,&quot;Male&quot;,&quot;Female&quot;) myFamilyWeights&lt;-c(188,136,83,61,44) myFamily&lt;-data.frame(myFamilyNames,myFamilyAges,myFamilyGenders,myFamilyWeights) myFamily$myFamilyAges 如果你疑惑我们为什么需要输入如此长一大串，还需要$符号加在两个名字中间，我们可以像先前建立数据框一样直接输入”myFamilyAges”.接下来是值得指出的，当我们创建myFamily数据框时，我们从每个向量中都复制了所有信息存储到新的空间中。因此，现在我们创建了”myFamily”数据框，myFamily$myFamilyAges实际上指的是完全不同的向量值（虽然现在还是相同的）。你可以自己尝试一下，试着加上一些数据到原始的向量内，myFamilyAges： myFamilyAges&lt;-c(myFamilyAges,11) myFamilyAges myFamily$myFamilyAges 我们看一下上面五行信息。第一行中我们使用c()命令在原始的年龄数据中增加了一个数值11，存储在myFamilyAges中（我们或许领养了一只老猫到家中）。第二行，我们命令R输出myFamilyAges向量中的值。第三行，R输出了原先的五个数值和新加入的数值11。我们命令R输出myFamil$myFamilyAges,但是，R只输出了原先的五个数值。这个表明数据框内的列/向量数值已经与之前单独的向量数据完全不同了。所以我们要非常小心，如果我们建立新的数据框用于后续的分析，那在分析时避免因使用原始数据集而出错。 下面是继上面问题而引申的。我们现在有一个很好的数据框，包含了5个观测值和4个变量。这是一个矩形数据集，正如本章开头所述。如果我们在一个变量中新加入一个数值，如下： myFamily$myFamilyAges&lt;-c(myFamily$myFamilyAges,11) 如果运行这条命令，我会有一个很诡异的情况：数据框中的myFamilyAges变量会比其他变量多一个观测值，这不在是一个完美的矩形。试一试这条命令，看看会有什么发生。这个结果阐明了R是怎样处理类似情况的。 因此我们从这一章学到了哪些新技术新知识呢？下面是本章的总结： 参考资料http://en.wikipedia.org/wiki/Central_tendency http://en.wikipedia.org/wiki/Median http://en.wikipedia.org/wiki/Relational_model http://msenux.redwoods.edu/math/R/dataframe.php http://stat.ethz.ch/R-manual/R-devel/library/base/html/data.frame.html http://www.burns-stat.com/pages/Tutor/hints_R_begin.html http://msenux.redwoods.edu/math/R/dataframe.php]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[数据科学(4)-跟着数据走]]></title>
      <url>%2F2014%2F08%2F16%2F2014-8-data-science-chp4-follow-the-data%2F</url>
      <content type="text"><![CDATA[本文为Introduction to Data Science一书的翻译，由网友义务完成，了解参加翻译的网友，请点击这里，如果要加入我们请加入qq群171546473,了解翻译规则点击这里 第四章 跟着数据走侦探这行有句谚语“跟着钱走。”在数据科学，成功的一个关键是“跟着数据走。”在多数情形，一个数据科学家不会去从无到有的设计一个信息系统。反而，在一些或者许多已有的系统中，就有数据；数据科学家面临的很大一部分挑战就是如何整合这些系统。我讨厌唠叨，但是你最近做过体检吗？如果你去看过医生，不管是为什么原因，你可能会记得医生的办公室被数据湮没了。首先，医生有大量的数字传感器，从血压监测器到超声波机器应有尽有，所有这些都会产生大量的数据。也许，对于现在引起广泛关注的关于健康保险的争论，金融和保险数据的一个很重要的出发点就可以是医生的办公室。美国医疗保健体系有一个著名的“特点”，就是我们的医疗保健服务的普遍做法是：按照流程付钱。当你在医生的办公室经历一次“流程”时，不管是一次咨询，一次检查，一次化验，或者其它什么，这是一系列会产生深远结果的数据事件的开端。 如果你的医生是典型的医生，这些事件的起点是一张纸质的表格。你曾经详细的看过它们吗？这些表格上多半填的是由流程和代码组成的矩阵。虽然许多设备先进的地方可能会用平板电脑或者其它电脑将这些表格数字化，但是纸质表格仍然是普遍存在的。如果要进行保单赔付或者付款，纸质表格的数据会被输进一个系统，这个过程会在医生办公室或者在一个外包公司里进行。 这些流程数据去哪儿了呢？哪些其它种类的数据（比如病人账号信息）可以在接下来的流程中和它们连接？这些连在一起的数据形成了什么样的网络？或者它们的安全性如何？在这些数据到达保险公司之前，处理数据要多少步骤？保险公司在考虑理赔之前是怎样处理和分析数据的？当保险公司的系统同意理赔之后钱是怎么转账的？这些问题仅仅是从表面上考虑的：还有几十种上百种处理步骤我们连想都想不到。 我们很容易从这个例子中看到，根本不可能把已有的系统扔掉，然后去设计一个更好的或者至少更标准化的系统。但是如果你的工作是提高系统的效率，或者审查保险理赔以确保它们和保险记录相符，或者使用数据去探测和预测疾病的爆发和流行，或者给顾客提供反馈告诉他们一些流程的期望花费是多少，那么你该怎么办呢？ 你的项目的关键的出发点在于要跟着数据走。你应该像一个侦探一样，充分的弄清楚细节，包括内容，格式，发送人，接收人，转账方法，仓库，还有过程中每一步数据的使用者以及进行数据处理和存储的组织。 幸运的是，已经有一个广泛的研究和实践领域叫做“数据模型”，它给出理论，策略和工具以帮助实现数据科学家“跟着数据走”的目标。这些想法开始于70年代早期，是由计算机科学家Ed Yourdon提出的称之为数据流程图方法。一个更现代的方法叫做实体关系模型，它和建立关系型数据库的方法有很强的关系。专家们使用这个模型发明了实体关系图（ERDs）来描述一个系统的结构和数据的移动。 实体关系模型在不同层面出现，从抽象概念侧面到实际存储层面。在概念层面一个实体是一个对象或者东西，通常是真实世界中存在的东西。在医生办公室的例子中，一个重要的“对象”是病人。另一个实体是医生。病人和医生通过一个关系相连：在现代医疗保健术语中这是“供应”关系。如果病人是X先生，医生是Y医生，供应关系提供一个双向的连接： Y医生是X先生的提供者 X先生的提供者是Y医生 自然的，有一些数据可以描述X先生：名字，地址，年龄，等等。同样的，也有描述Y医生的数据：医龄，专业领域，资质证书，执照。重要的是，有一大块数据用来描述X和Y的联系，这就是关系。 建立一个ERD需要调查和枚举所有的实体，比如病人和医生，还有它们之间可能存在的所有的关系。就像本章的开始提到的，这可能发生在许多组织上（比如，医生办公室和保险公司），这取决于信息系统被设计出来的目标。最终，ERDs必须足够详细，这样它们才能被用作一个数据库的物理存储的说明书。 在医疗保健这样的应用领域中，如何设计数据有太多不同的选择了，以至于建立一个好用的系统要求一些经验和一些“艺术”。“艺术”一部分体现在理解用户的现有信息需求以及这些需求在未来预期的变化。如果一个组织在重新设计一个系统，补充一个系统，或者设计新系统，他们这样做是为了未来的利益。这个利益可能是更高的效益，错误/不精确的减少，或者增强的信息能力带来的做出新产品或新服务的可能性。 不管目标是什么，数据科学家现在在方法上面临重要且困难的挑战，包括纸质表格和手工数据录入，想象一下将来的方法。跟着数据走！ 在下一章，我们介绍组织数据的最普遍也是最有用的方法，叫做矩形结构，它包含行和列。这种把数据布置成矩形的做法出现在电子表格和数据库，被很多应用所使用。理解这些行和列如何组织的，这是数据科学中多数任务的关键。 参考资料http://en.wikipedia.org/wiki/Data_modeling http://en.wikipedia.org/wiki/Entity-relationship_diagram]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[数据科学(3)-R入门]]></title>
      <url>%2F2014%2F08%2F16%2F2014-8-data-science-chap3_GettingStartedWithR_full%2F</url>
      <content type="text"><![CDATA[本文为Introduction to Data Science一书的翻译，由网友义务完成，了解参加翻译的网友，请点击这里，如果要加入我们请加入qq群171546473,了解翻译规则点击这里 第三章 R入门“R”是一个开源软件程序，由一群正在使用它的科学家，研究员和数据分析师们自愿服务来对它进行维护。“R”是免费下载和使用的，而且在线提供大量的建议和指导资源帮助用户学习R，这一点是非常好的，因为R是一个非常有力的和复杂的程序，现实中，它是一个专注于数据分析的功能全面的编程语言。如果你对计算机，编程或者数据科学一无所知，那么欢迎来到这个令人兴奋的章节，这里能够帮你开启大门，认识到全宇宙中有史以来最有力的免费数据分析工具，不是说笑哦。反过来，如果你熟悉电子表格，统计分析或者会计软件，你可能会觉得这本书已经走向极端，永远不会恢复理智，而所有的这些对于用户接口设计来说都是有益的和正确的。这两个观点都是合理的。R这种开源的数据分析程序是非常有力的，灵活的和扩展性极强的（意味着人们可以很容易地为它创建新的性能）。同时，R是面向“命令行”的，就是说大量的工作和需求行为只需通过几个精心设计的命令便可以实现，其中的大部分命令是有技巧的（设计命令的标点和相关的规则是奏效的）。另外，当结果看起来有点奇怪时，R不能很正确地返回反馈或错误信息用以帮助用户修复错误或者找出问题所在。但是还是有方法来对付这种疯狂的行为的。R作为一个教学工具的最大的一个优点是它透明度比较高。因为成功的学习者可定能够充分理解到“数据情况”是什么或者哪些R命令不能够工作。在电子表格当中，我们很容易键入大量的数和公式，比如 =FORECAST()，然后结果就会像魔法一般跳进格子里，无论这些数字或者公式有无意义。使用R，你必须理解你的数据，知道自己怎么利用这些数据，清楚这些数据时怎么被转化的，明白怎么查找问题。因为R是一门编程语言，它也会要求用户通过数据对象和能够应用到这些对象的方法，还有能够应用到这些方法的过程来思考问题。在现代的编程语言当中有几个比较重要的隐喻，如果对软件编程，测试和集成到一个工作系统的过程没有一个基础的了解，没有数据科学家能够通往成功。R强大的扩展性能能够让志愿者们随时为它添加新的模块。R是首批集成直接从“推特”社交媒体平台提取数据的功能的分析程序。所以你可以确信下一代伟大的发展埋伏在数据的世界当中，在R社区当中的某个人开始为R所需要的模块开发新的“包”。最后，用R工作的人会慢慢体会到：R无所不能，它几乎能应用到任何其他语言和兼容其他平台。如果一个人掌握了R语言，那么他很容易就能掌握SAS(r);(统计编程语言)和SPSS(r);语法。（SAS和SPSS是两个目前最受欢迎的商业统计分析程序）。所以学校，老师和学生无需支付任何版权费便能学习到世上最强大的数据分析系统及其课程，无论你身处何地。但是它需要你有耐心，所以请你一定要坚持。让我们开始吧。显然你需要一台计算机。如果你在使用平板电脑或智能手机，你可能想跳转到R-studio的章节，因为旧版R还没有支持在平板电脑上工作（但是有一个适合平板电脑的工作区叫做R-studio）。有少数人开始使用基于Web界面的R，像这个 http://dssm.unipa.it/R-php/R-php-1/R/ -但是他们仍然在初始阶段。如果你的计算机装有Windows(r);, Mac-OSX(r);或者 Linux操作系统，你可以到这里下载R： http://cran.r-project.org/。下载并安装到你的电脑。如果在安装这个新软件的过程中你需要帮助，这里推荐一个非常有用的小册子，是Thomas P. Hogan著的，叫做：Bare Bones R: A Brief Introductory Guide，你可以自行购买或者从图书馆借阅。还有很多在线的网站能够给你提供帮助，虽然大多数都不是直接面向初学者的。我在谷歌输入“help installing R”，它返回了一些比较好的网站。在Windows下，一个对安装R有比较丰富的内容的网站是：&quot;readthedocs.org&quot;，你可以点击以下网址进入： http://tinyurl.com/872ngtt。 对于Mac用户，可以看一下Jeremy Taylor在Vimeo.com的视频介绍：http://vimeo.com/36697971，这个视频既介绍了R在Mac上的初始化安装，也介绍了一些可选的步骤助于你的入门。Youtube上也提供了4个相关的视频教程，你课可以在搜索框输入“install R”试试。本章接下来的内容我们假设你已成功安装了R并且能像以上的截图一样运行在你的计算机上。（由于这个截图是在Mac上截下来的，所以如果你使用的是Windows或者Linux，那么截图看起来会有点不同的。）来玩一下吧，运行R你可以做的第一件事就是按一下颜色轮子来自定义R的面板颜色。以上截图中的R是使用了鲜艳的橙色作为背景色。截图上同时也展示了一些常用的命令，用于调出与R交互的一下比较基本的方法。注意到截图的底部的大于符号（“&gt;”），其实它是一个命令提示符&ndash;当R真正运行时，你在“&gt;”后面输入一些命令然后按回车建或返回键，这个命令就会被传递给R执行。当执行完成，返回的结果就会出现在“&gt;”之后，同时接着下一行另外一个命令提示符(“&gt;”)就会做好准备，接受你的下一个R命令。在截图当中，用户输入了1+1然后回车。世界各地的小学生都在利用1+1这条公式相互嘲笑各自的数学技巧，但R返回的结果并不是简单的2。如果你细心观察，你会发现在2之前还有一个括在中括号里的“1”，就是：[1]。这个[1]有什么作用呢？它是一个行标号，可以帮助追踪R显示的结果。仅仅显示一行结果是毫无意义的，但R就是喜欢做的更好，所以当我们深入学习R的时候，我们可以看到很多这样的行标号。 还记得在数据概述一章中关于家庭成员年龄的数据嘛？下面回顾一下：43,42，12，8,5，分别是爸爸，妈妈，姐姐，弟弟，以及宠物狗的年龄。我们曾经提到过，这组数据是一组形式相同的数据列表，即整数列。你可以确定这是一个整数列的原因是这些数字小数点后位数为零。在R中，我们可以通过”c()”这一命令来建立一个整数向量，如下图所示： 上图是我们上次在R console中的操作截图。从现在开始,出于排版的考虑，我们仅列出相关命令及结果。上图中的第一行命令在前些章已经出现过： 1c(43, 42, 12, 8, 5) 你可能已经注意到了，在命令的下面，R输出了相同的向量。行编号“[1]”之后，我们可以看到数列43,42,8,5。因为没有相应的存储命令，所以R会输出相同的信息.相比之下，请看另一条命令： 1myFamilyAges &lt;- c(43, 42, 12, 8, 5) 但是，再次输入同一数列时，用“&lt;-”符号将其存储为“myFamilyAges”，R返回了一个空的命令提示符。这就是为什么第三行命令中要求显示“myFamilyAges”变量中包含哪些信息（黄色“&gt;”符号后面蓝色字体是需要键入的命令）。这是一个十分简单并且重要的命令。每当你想知道数据集的解构时，只要输入数据集名称，R便输出相应数据集。在下一条命令中，我们可以感受到R的用途： 1sum(myFamilyAges) 该命令使R对myFamilyAges变量中所有数字求和，即110（你可以用计算器检验）。虽然对家庭成员的年龄求和不是一个非常确切的例子，但是我们展示了你可以通过这样一条简短的命令减少计算量。在随后的一条命令中，我们想要知道所有年龄数据的均值（也就是非数据人员所说的平均数），结果显示为22岁。接下来的命令叫“range”，表示列表中的最低和最大年龄。最后，开个玩笑，我们来谈一下命令”fish(myFamilyAges).”几乎就像你期望的那样，R并没有fish()”函数，所以我们收到报错消息。这就提到了R的另一个重要原则：你可以随意进行尝试而不必担心造成破坏。如果R无法理解你想做的事情或者你没有写清楚自己想做的事情，R会提示出错并且在你输入新命令之前，它不会发生任何改变。本书在将来的章节会讨论的一些策略，使你可以解决这个问题并且懂得R如何明白你想做的事情 现在我们看一下存储单元。首先，你应该认真尝试一下电脑中记下的所有命令。你可以根据需要阅读相应的命令，但是全部尝试一下会学到更多。再者，如果你尝试了这些页面中的命令但是它们没有起作用，你应该找出原因。由于R对于命令的类型非常严格，所以从检查拼写开始。R语言是区分大小写的，“myFamily”与“myfamilyages”是有区别的。验证拼写后如果还是报错，尝试一下使用在线帮助： http://stackoverflow.com, https://stat.ethz.ch,以及http://www.statmethods.net/.检查出错误的原因往往会让你对R的学习更有帮助。第三，你应该花一点时间试验一下学到的新的命令集。比如输入： 1myRange &lt;- range(myFamilyAges) 输入这个命令之后会发生什么呢？然后在另一个命令行里输入”myRange”来输出存储了什么内容？然后思考一下原因并且用同样的方法尝试一下别的试验。尝试越多，学到的知识越多。有些好东西就是试验得到的。在这一点上，即使只尝试了一些命令，你也已经知道关于R的如下事情： -怎样在电脑上安装R并且运行。-怎样在R控制台输入指令。-“c()”函数的使用。记住”c()”代表连接，就是将事物联系到一起。可以在括号里输入一系列事情，用逗号隔开。-向量可以通过使用分配箭头存储在已经命名的存储单元中（左箭头和破折号组成，比如”&lt;-“）。-可以在命令行输入存储的数据对象的名字来生成报告。-可以对一组数据运行函数，比如mean()，将它们转换成别的东西（mean()计算均值，这是最基础的数值）-R中包括了sum(), mean(), 和range()，但是不包括fish() 本章将学习向前推进了一些，开始针对文本进行运算，学习如何将家庭年龄表与家庭成员名字以及他们的其他信息结合起来。 章节难点======================================================== 使用逻辑和在线资源帮助学习，学习使用c()函数将家庭成员年龄增加到“我的家庭年龄”向量后面。 资源======================================================== http://a-little-book-of-r-for-biomedical-statistics.readthedocs.org/en/latest/src/installr.html http://cran.r-project.org/ http://dssm.unipa.it/R-php/R-php-1/R/ (UNIPA experimental web interface to R) http://en.wikibooks.org/wiki/R_Programming https://plus.google.com/u/0/104922476697914343874/posts (Jeremy Taylor’s blog: Stats Make Me Cry) http://stackoverflow.com https://stat.ethz.ch http://www.statmethods.net/ 自我测评 回顾3.1 R入门 问题1 R的版权费是多少？ A R是免费的 √ B 在iTunes商店中售价99美分 C 10美元 D 100美元 本章节中用到的R函数c() 连接数据元素 &lt;- 赋值箭头 sum() 求和 range() 求最值 mean() 求平均值]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[数据科学(2)-识别数据问题]]></title>
      <url>%2F2014%2F08%2F16%2F2014-8-data-science-chap2-Identifying-Data-Problems%2F</url>
      <content type="text"><![CDATA[本文为Introduction to Data Science一书的翻译，由网友义务完成，了解参加翻译的网友，请点击这里，如果要加入我们请加入qq群171546473,了解翻译规则点击这里 2.第二章 识别数据问题 数据科学与其他领域诸如数学或统计学完全不同。数据科学是一种实用的活动，数据科学家提供需求，并且帮助数据使用者解决问题。在解决一个问题之前，首先需要明确该问题，而这个过程却并不总是看起来那么明显。在这一章中，我们将要讨论如何识别数据问题。 种植苹果的农民经常生活在担忧中：春天花开的时候，一场霜冻足以冻死这些美丽的花朵；夏天结果的时候，一场冰雹或是飓风则会让所有的果实覆灭。一般而言，农业耕种是物理世界中最重要的活动，受到诸如天气等极其复杂的、远远超出了人类控制范围的自然因素的影响。 在这物理世界中高度不可预测的自然力量面前，数据科学是否占有的一席之地呢？表面上看似乎没有，但能否就此盖棺定论？要做到非常敏锐地识别数据问题，你需要有开阔的思维，强烈的好奇心和蓬勃的创造力，并且总是乐意不断的提出问题。事实上，正如你在第一章中得到的印象那样，数据科学家们往往整天坐在电脑面前，处理一段让人抓狂的R编写的程序。每天，数据科学家们必须专注于他们要处理问题的领域。显然，你可不能让一个数据科学家去做农活，但是如果你想要认识到农民才会遇到的数据问题，那么你必须学会像农民一样去思考问题，至少在一定程度上如此。 你可以通过阅读和观看视频来获取一个领域的专业知识，但是最好的办法是询问“相关的专家”（比如上面例子中的农民）来了解他们都干什么。询问问题的流程应当因问题而异，但是通常都可以从以下三个方面考虑。首先，你可以让这些“相关的专家”把他们所做的事像故事一样告诉你；然后你可以询问他们一些反常现象：哪些异常的发生导致最终结果变得更好或是更差；最后，你可以询问风险和不确定因素：哪些情况是难以确定是否会发生的，哪些情况将会对结果产生极大的影响，无论是好的影响还是坏的影响。这三个方面的每一部分都体现了识别数据问题的方法，通过数据，信息以及在正确时间做出的正确决策，可以让事情变得更好。 为什么要像听故事一样来提问呢？这是因为人们往往习惯于讲故事的思维，无论是农民、教师还是经理人或是CEO，人们总是讲述或倾听他们领域中那些成功或失败的例子。故事是一种非常强力的交流智慧，无论是在同一专业领域的不同人之间交流的时候，或是不同领域的人要获得对别的领域的初步认识的时候。当然，唯一存在的问题就是故事有可能是错的。 如果你可以让一个专业人员像说故事一样讲述她如何管理她的工作，接下来你就应该考虑如何来核实这些故事。即使不直接向故事叙述者询问故事的真实性，你也可以想到很多方法并从不同方面来验证故事中发生的事情。最终证实（也可能是揭穿）这些故事。 举例说明，一个农民可能会告诉你，在5年前发生的那场非常严重的春季霜冻中，在山谷中的树木都幸免于难，而山脊上的树木则受到严重的毁坏。由于这个原因，在一个寒冷的夜晚，这个农民在山脊上布满了烟熏罐（一种存放燃料用以产生烟火的容器），他坚信这个策略能起到作用。但事实是否如此呢？我们可以从不同地理位置收集一系列温度数据，包括果园里寒冷夜晚和暖和夜晚的温度，放置烟熏罐和不放置烟熏罐的夜晚的温度等，这些都是可行的。有了这些数据，我们就可以建立一个果园里不同位置的温度变化模型，这个模型就可以支持、改善或揭穿这个故事。 另外一种识别问题的策略就是寻找异常的例子，不管是好的还是坏的。稍后我们将在这本书中了解到经典的统计推断方法的核心是如何来描述“中心”的。“中心”是指出现的大部分的典型情况，通过检查那些远离中心的例子，就可以获取到能帮助我们理解环境干预或异常结合等信息。识别异常例子对理解事情工作原理非常有用，但前提是要定义大部分典型情况发生的中心，这样才能对何为异常情况有个清楚的认识。 让我们再回到农民朋友的这个例子上来，在去年夏末的一场大雷雨来之前，果园中吹过了一阵强风，将一些水果从树上打落下来。大部分的树都只掉落了少量的水果，并且这些水果都掉落在树根附近。而有少部分的树掉落的水果数量却明显多很多，而且掉落的位置也离树较远。这是因为这风非常奇怪，故意在这个位置上造成这个结果的吗？或者这只是一个巧合而已？ 如果按照树的随机样本来对掉落的水果进行系统的统计，也许可以回答这个问题。从这个统计结果中应该能看到，大部分的树掉落的水果数量是大致相同的，更重要的是，这些正常情况可以给我们一个清晰的界限用以区分正常和不正常的情况。当我们找出了明显超出了正常范围的反常的例子时，我们就可以专注于这些反常的例子；通过这些异常，更好的理解事情发生的原理。 还有第三种识别数据问题的方法，就是明确事情存在的风险和不确定性。前面章节提到过，信息的一个基本功能就是用来降低不确定性。风险极大的影响着我们所做的事情，因此考虑降低不确定性就显得非常重要了。无论在单位，学校，或是家里，生活总是充满了风险：一个决定或是未能完成某件事情都可能引发一连串事情，从而导致某些事变好或变坏，这都很难说。一般情况下，我们都希望尽可能的获得好的结果，尽量避免结果变坏，缩小事情变化范围。要实现这样的目的，我们必须做出更好的决策；而想要做出更好的决策，我们就需要降低不确定性。数据科学家就是通过询问事情的风险和不确定性（以及做出的决策）从而能将注意力集中到关键问题上。回顾前两个策略——询问包含专业知识的故事和询问异常例子，其潜在的目的都是为了降低风险和不确定性。 还是农民这个例子，其中主要的风险都是来自于天气，通常情况下，解决天气不确定性的对策都是很划算的。在烟熏炉消耗大量昂贵的燃油来使夜晚暖和是一种非常浪费资源的表现，这在盈利的年份和亏损的年份将产生明显的不同。因此，想通过数据来解决这个问题，那么关于这个地区气候条件更加精确、合适的信息将是一个非常关键的部分。如果农民的手机上有一个应用能及时的播报当地的天气情况会怎样呢？让我们来建立一个这样的应用……]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[数据科学(1)-关于数据]]></title>
      <url>%2F2014%2F08%2F16%2F2014-8-data-science-chp1-about-data%2F</url>
      <content type="text"><![CDATA[本文为Introduction to Data Science一书的翻译，由网友义务完成，了解参加翻译的网友，请点击这里，如果要加入我们请加入qq群171546473,了解翻译规则点击这里 第一章 关于数据 数据这个词来源于拉丁文“datum”，意思是“已知的东西”。虽然“数据”这个术语从1500年代就被使用了，但是现代的用法是在1940年代和1950年代随着电子计算机开始输入，处理和输出数据时才出现的。这一章讨论数据的本质，并对没有计算机科学背景的初学者介绍一些关键概念。 万维网的发明者Tim Berners-Lee经常被引用的一句话是：“数据不是信息，信息不是知识，知识不是理解，理解不是智慧。”这有点像金字塔， 数据就是建筑下面用来做地基的原材料，而信息，知识，理解和智慧代表金字塔更高的层次。在某种意义下来说，数据科学家的主要目标是去帮助人们把数据转换成信息以及金字塔的更高层。但是在实现这个目标之前，我们应该清楚地知道数据究竟是什么。（注意到这本书把“数据”这个词处理成了名词复数——而在平常你可能经常听到人们把这个词作为单数。）如果你学过计算机科学或者数学，你可能会发现这一章的讨论有一点多余，所以跳过它们没问题。但如果你没有学过那些，可以接着读下面关于数据——数据科学家的工作中最基本的元素——的介绍。 关于数据，我们现在了解并且谈论的大部分内容来源于一个叫做Claude Shannon的美国数学家的工作。Shannon在第二次世界大战前后研究了很多关于数据和信息的数学和工程问题。Shannon说：“通信的最基本问题是如何在一点近似或完全的再现另一点的内容。”这句话概括了在本书中的至关重要的想法，即数据是从源传递到接收方的信息。想想最简单的，你可以用手机给别人发短信，甚至当面告知来传递信息。假如一个朋友问题你一个问题，你明天可不可以去他家吃晚饭。你可以回答去或者不去。你可以在电话里对她说去不去。但你可能信号不好，所以你的朋友可能听不到。你也可以发短信告诉他去不去，这样你就得指望她的电话是开着的，这样才能收到短信。你也可以当面告诉你的朋友，然后指望她没有把耳机声音开得很大以至于听不到你。在这三种情况中，你都有一个“比特”的信息想传达给你的朋友，去或者不去，而目标是减少她对于你明天去不去吃晚饭这件事的不确定性。假设信息没有混淆和丢失的传递过去了，那么你就成功的向她传递了一比特的信息。Claude Shannon发展了一些现在被称之为“信息论”的数学，用以量化从源到接收方传递的数据是如何通过提供信息而减少不确定性的。现在世界上大量的电脑网络设备和软件——还有互联网——都是在解决这个简单的工作：把信息的比特从源传输到目的地。 一旦我们熟悉“比特”作为信息的最基本单位——“是”或“不是”——之后，我们可以把比特组合到一起形成更复杂的结构。首先，让我们稍微转换一下标签。我们不用“不是”而改用0，再把“是”换成1。现在我们变成了一个数字，虽然只有一个数字但是有两个状态：0或者1（我们现在不允许任何更大的数字，比如3或者7）。这事是上是“比特”这个词的院士含义，也即“二进制数字”的简写。一个简单的二进制数字可以是0或1，但是这阻止不了在我们的信息中使用多个数字。看下表这个例子： 意义 第二个数字 第一个数字 否 0 0 也许 0 1 可能 1 0 一定 1 1 在这里我们开始使用2个二进制数字——2比特——建立了我们想传递给朋友关于晚饭的4个信息的“密码说明书”。如果我们很确定我们不参加，我们就给她发送信息 0 0。如果我们很确定参加，我们就给她发送 1 1。但我们还有两种可能性，“也许”由 0 1表示，“可能”由 1 0表示。我们可以比较一下用一个比特表示的“是或不是”和两个比特表示的4种信息。事实上，每次你加一个比特，你所能传递的信息个数就会加倍。所以3个比特就会有8个选择，4个比特就会有16个选择。如果有5个比特，你会有多少种选择呢？ 当我们有8个比特——这提供了256种组合——我们终于得到了一个可以实际使用的大小了。8个比特通常被叫做一个“字节”——这个术语可能是衍生于比特这个词。（你可以在网上找一找“nybble”这个词！）一字节的组合个数足够给字母表中的所有字母编码，包括大写字母和小写字母。有一个旧标准叫做“ASCII”——美国信息交换标准码—— 它把8比特的模式对应到字母表中的字母，标点，和一些其他标记符号。比如比特模式0100 0001表示大写字母A，而下一个模式0100 0010表示大写字母B。你可以在网上找到ASCII表（比如http://www.asciitable.com/），那里有所有的组合。要注意编码可能实际上不是用二进制表示因为对人类来说，读很长的一串0和1是很麻烦的。所以你可能看到的是等价的16进制编码，8进制编码或者我们熟悉的10进制编码。虽然你可能还记得高中数学课上学过的进制转换，但是最好还是实际练习一下——尤其是二进制，十六进制和十进制的转换。你也可能喜欢可汗学院上的Vi Hart的“二进制手舞”视频（在http://www.khanacademy.org上搜索，或者点击本章末尾给出的链接）。我们这本书的大多数工作是在十进制下进行的，但是关于数据的更复杂的工作通常需要理解16进制并要知道一个16进制数字，比如0xA3，是怎么转换成一个比特模式。搜索“二进制转换教程”，你会找到很多有用的网站。 把字节组合成更大的结构 既然我们知道了字节是由一些比特（通常是8个）构成的，它可以用来存储和传输事物比如字母和标点符号，用这种想法我们可以开始构建更大的东西。首先容易看出，我们可以把字节组合成列表来构造一“串”字母，这就是经常被谈论的“字符串”。如果我们有一些文本，比如“this is a piece of text”，我们可以用一些字节表示它： 10111010001101000011010010111001100100000011010010111001100100000011000010010000001110000011010010110010101100011011001010010000001101111011001100010000001110100011001010111100001110100 没人想看这样的东西，更别说手动编码和解码了，幸运的是，我们现在用的电脑和软件可以自动转换和存储。比如，当我们让开源数据语言“R”去存储”this is a piece of text”时，如下： 1myText&lt;- &quot;this is a piece of text&quot; 我们知道在电脑内部有很长一串0和1用来表示我们刚才存储的文本。顺便提一下，为了之后可以再次调用这段文本，我们可以为它建立一个存储标签（上面的“myText”）。每次我们想看看这段文本或者想用它干些其他事，我们可以使用标签“myText”去打开电脑中我们存放代表我们的文本那一长串二进制数字的那一块内存。由小于号（“&lt;”）和短横线（“-”）组成的向左指的箭头在R中的意思是把右边的东西（引文）赋值给左边（我们标记为“myText”的存储区域）。一些人把这个叫做赋值箭头，在一些计算机语言中，它用来告诉读或者写代码的人信息的流向。 从计算机的角度来看，存储，记忆和操作数字要比文本容易的多。我们记得一个8比特的字节可以容纳256种组合，所以只使用这么小的一个空间我们就可以存储数字0到255（当然，我们也可以存1到256，但是计算机中的多数计数是从0而不是1开始的）。但是255也不够我们使用。我们不能仅用255以下计数多数城市中的房子数量也不能计算一个大停车场里的车辆个数。如果我们把两个字节合在一起构成16比特，我们就可以计数从0到65535，但这对于如今世界上的一些大数字来说还是不够的（比如，仅仅美国就有超过2亿辆汽车）。多数时候，如果我们想自由的表示一个整数（没有小数的数字），我们把4个字节放在一起。4个字节放在一起是32比特，这允许我们最大存储到4294967295。 当我们开始存储负数或者有小数的数的时候事情就有些复杂了。如果你对此好奇，搜索“补码”可以知道一个带符号的数字是怎么存储的，搜索“浮点”可以知道带小数的数字是如何存储的。对于我们这本书而言，主要应该知道文本的存储方式和数字是不同的，而整数和浮点的存储方式也是不同的。之后我们会发现有时会要求我们把这两种表示法相互转化，所以知道一个数是如何表示的总是很重要的一件事。 到目前为止我们主要了解了如何在同一时间存储一个东西，比如一个数字或者一个字母，但是当我们处理数据时，我们经常需要同时存储一组相关的东西。最简单的方法就是用元素的列表存储，在列表中的元素的存储方式都相同。比如，我们可以有整数的列表，列表中的每一个元素是你的一个家人的年龄。列表可能看起来是这样的：43，42，12，8，5。前两个数字是父母的年龄，后三个数字是孩子的年龄。自然的，在电脑中每个数字都是用二进制存储，但幸运的是我们不需要用二进制打出它们，我们也不需要阅读二进制。因为没有小数，它们是纯整数，一个32比特的整数（4字节）足够存储它们了。这个列表包含的元素具有相同的“类型”。开源数据编程语言“R”中的一个列表中的所有元素的类型都是“向量”。我们在R中可以通过枚举数字来容易的建立一个向量，在一个括号里用逗号分隔： 1c(43,42,12,8,5) 括号前面的字母“c”代表联结。这有些晦涩，但是适当的练习之后就很容易掌握。我们也可以用类似之前我们学到的方法来给我们的向量用一个名字存储（记住一个向量是一个具有相同类型元素的列表）： 1myFamilyAges &lt;- c(43,42,12,8,5) 我们刚刚建立了我们的第一个“数据集”。当然，它很小，只有5个元素，但用它可以展示数据的一些核心概念。下面是一个回顾： 在计算机的内部，所有的数据都以二进制存储。一个二进制数字，或者说比特，是我们可以从一个地方传输到另一个地方的最小的数据块。 虽然所有的数据都用二进制存储，但是电脑和软件可以帮助我们用更方便的形式呈现数据。三种重要的表现方式是：“字符”用来表示文本，“整数”用来表示小数点后面没有小数的数字，“浮点”用来表示有小数的数。在我们的小数据中的数的列表是整数。 数字和文本可以放入列表，开源语言“R”中叫做向量。一个向量具有长度，意思是它里面元素的个数，还有一个“类型”代表数据在向量中的存储类型。我们刚才使用的向量的长度是5，类型是整数。 为了保存我们在哪里存储了数据，多数电脑语言，包括R，允许我们给一段电脑内存一个标签。我们给那个5个元素的向量一个名字“myFamiliAges”。一些人可能把具有名字的列表叫做“变量”，因为它的值可以变化，这取决于你在使用列表中的哪个成员。 如果我们把一个或多个变量集合起来变成一个有意义的群体，我们把它叫做“数据集”。通常来说，数据集只有一个变量是没有什么意义的，所以通常我们至少需要两个变量。但是，严格来说，即使是我们的很简单的“myFamilyAges”也算作是一个数据集，虽然是一个很小的数据集。 在这本书的后面我们会安装并运行开源“R”数据语言，学习如何创建数据集，在这些数据集中整理信息，对数据集进行简单的计算和变换。 章节挑战 理解“布尔逻辑”的意义和“且”，“或”，“非”和“异或”的规则。你学完后，不要看，在一张纸上写下所有代表这些规则的二进制操作。 资源 http://en.wikipedia.org/wiki/Claude_Shannon http://en.wikipedia.org/wiki/Information_theory http://cran.r-project.org/doc/manuals/R-intro.pdf http://www.khanacademy.org/math/vi-hart/v/binary-hand-dance http://www.khanacademy.org/science/computer-science/v/introduction-to-programs-data-types-and-variables http://www.asciitable.com/]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[数据科学(0)-数据科学：多面技能]]></title>
      <url>%2F2014%2F08%2F16%2F2014-08-data-science-chp0-data-science-many-skills%2F</url>
      <content type="text"><![CDATA[本文为Introduction to Data Science一书的翻译，由网友义务完成，了解参加翻译的网友，请点击这里，如果要加入我们请加入qq群171546473,了解翻译规则点击这里 数据科学：多面技能本节主要内容 数据科学需要很多其它工作都需要的一个重要技能-数据分析能力，但是这不是唯一需要的技能 通过超市自动收款系统（POS）案例来展示数据科学的各种挑战 数据科学家在数据架构，数据获取，数据分析，数据归档这四个与数据科学相关的设计与实施领域扮演积极角色 案例中突出强调了沟通技能，数据分析技能，伦理推理技能 有人可能一提到“数据科学”这个词就会在脑海里浮现出统计学家穿着白色的实验服眼睛盯着闪烁的计算机屏幕输入一行行的数据的图片。真相远非如此，首先统计学家不是穿着白色的实验服，这一“时髦”是给生物学家，医生以及其他需要保证在充满非普通液体环境中保持服装干净的人预留的。其次，这个世界上大多数数据是非数字化，非结构化的。这里非结构化是说数据不是以整齐的行列存在的。想象一下网页中的各种图片，朋友间的短消息，这当中很少有数字。尽管公司，学校，政府使用比如产品销售，成绩平均绩点（GPA），税收评估等大量数字信息是事实，，不过在这个世界上还有大量的其他信息数学家和统计学家需要观测和处理。因此尽管优秀的数学知识很有用，但数据科学的世界中还有更多我们已经习惯了的字词，列表，图像声音等其它信息也需要处理。 此外数据科学不只是简单的分析数据。许多人喜欢分析数据，他们可以一整天查看直方图与均值，而另一些人则更喜欢其它的工作。数据科学需要各种角色，多面技能。下面让我们以购买一盒麦片中涉及到的数据为例来分析。 不管你喜欢的麦片是水果型，巧克力型，纤维或者是坚果类，首先你会在购物清单里面加入“麦片”这一项。尽管它只是你用铅笔潦草写在信封背面，此时这一计划的购物已经是一份数据，当你到达杂货店，数据会提醒你从货架上取下一大盒水果味麦片放到购物篮里面。在收银台，收银员会扫描盒子上的条码，收款机会记录下价格。而库房里面，计算机会告诉存货管理人员因为你买的是商店里面最后一盒麦片了，所以商店需要向经销商下一个新订单了。同时由于你有一张针对你买的麦片的优惠卷，收银员需要扫描它，给你一个之前预设的折扣。在周末，一份所有来自该麦片制造商的优惠卷信息报表会传给麦片公司，以便他们向杂货店偿付他们向顾客所发的优惠卷折扣。最后，月末的时候，商场经理会看到一组五颜六色的饼图，上面展示了各种不同麦片的销售信息，由于水果类麦片销量强劲，经理决定在商场的有限货架空间中摆放更多的不同种类水果麦片。 小小的一点信息从你在购物清单上潦草涂写开始，在不同地方结束，最重要是它来到经理的桌上帮助决策。在从你的笔尖到经理的桌上的旅程中，数据经历了各种转化。除了计算机会终结或者存储数据以便长期使用外，大量如条码扫描仪之类的其它硬件也会涉及数据的收集，处理，转换和存储。此外，各种不同软件也被用于组织，汇总数据，将数据进行可视化与展示。最后各种“人类系统”也会与数据工作相关。有人需要决定购买或者安装什么系统，谁有权访问什么数据，数据完成使命后如何处理。在前面描述的场景成真之前，连锁杂货店的人员与和合作方需要上千次的具体决策与协商。 显然数据科学家不会参与到上面的每一步中，比如数据科学家不会设计与生产计算机，条码扫描仪等。那么数据科学家最重要的角色是什么呢？一般而言，数据科学家最重要的工作是以下关于数据的四个A：数据架构（data architecture），数据获取（data acquisition），数据分析（data analysis），数据归档（data archiving）。让我们以麦片购买为例来逐一分析以上各点。首先，对于数据架构而言很重要的一点，是在设计自动收款系统（零售商称之为收银机或相关设备）时，提前思考不同的用户应如何利用系统中的数据。对于系统架构师而言，需要具有敏锐的目光，比如发现尽管出于不同的需要，但是店长和经理都需要使用收银机中得数据。数据科学家通过提供数据的查询和组织方案，来协助系统架构师为不同的人提合适的供数据分析、可视化与展示服务。 其次，数据的获取主要关注在如何收集数据，更重要的是关注在数据在分析和展示，以及如何表示上。比如说，条形码仅是一串数字，无法很好的描述商品信息。当被扫描后，是否应该将商品描述、价格、净重或包装类型与之相连？不同的条形码可用于相同的商品（比如不同包装大小的麦片）。何时我们该注意到购买X与购买Y其实是购买的同一种不同尺寸的产品了？在数据能够被有效的分析之前，我们应该做好所有的数据的表示、转换、分组以及关联工作。而这些都是需要数据科学家介入的工作。 分析阶段是数据科学家参与最多的阶段。在此阶段，我们汇总数据，或用抽样数据来推断整体数据，同时使用表格、图或动画来展现数据。虽然会涉及到很多技术、数学、统计的知识。但请牢记，数据的最终用户始终是人。人才是最终的数据用户，满足人们的需求才是数据科学家的主要工作。因此也要求数据科学家具有良好的沟通能力。如果不能将信息有效的传递给用户，即使具有熟练的统计技巧，也是无用的。 最后，数据科学家也会参与数据归档。将收集的数据以某种形式进行展现使得数据可高度复用。大家可以把它认为是“数据监护“的概念。这往往是个不小的挑战，因为往往很难预测未来数据的使用方式。比如当tweeter的开发者在考虑如何存储数据时，他们可能永远不会预料到这批数据会被用于精确寻找地震和海啸。不过开发者也能预见到显示用户所在位置的地理编码是有用的数据。 总而言之，燕麦和杂货店的例子让我们认识到数据科学家的职责和所需的技巧。通过这些例子，我们总结了如下的技巧： 学习应用领域的知识。数据科学家必须快速的学习数据在不同的场景下如何使用。 与用户的良好沟通。数据科学家应够拥有足够的技巧来倾听和理解用户的需求且能够在技术、统计等专业术语与业务术语间不断的转换也是一项重要的能力。 能够全局的看待复杂的系统。在理解应用需求后，数据科学家必须能够预料到数据是如何在相关的系统和用户间流动的。 知道数据该被怎样表示。数据科学家必须清楚的理解元数据（描述数据是如何被组织的数据），理解数据是如何被存储和关联的。 数据转换和分析。当决策者需要使用数据时，数据科学家必须清楚的知道如何转换，汇总数据以及如何对数据进行推断。如上所述，将分析的结果很好的传达给用户也是一项十分重要的技巧。 可视化与展现。尽管数字具有精确性。但是良好的数据展示可以更有效的将分析结果传达给用户。 关注质量。无论数据集合的质量有多好，都不可能达到完美的程度。数据科学家必须知道数据的局限性,了解如何量化数据的精确性，并对提高数据在未来的使用中的质量提出建议 伦理推理。如果数据足够重要，使得人们想要收集这些数据。那么这些数据往往也重要到影响了人们的生活。数据科学家必须意识到可能存在的伦理问题，比如隐私问题。并将这些局限有效的交流给用户，并防止数据或分析的误用。 以上提及的技巧和能力仅是冰山一角。同时，对数字和数学的敏锐理解也是十分重要的。特别是对数据分析而言，数据科学家需要具有良好的沟通能力，具有系统思维以及数据可视化的良好直觉。能够清楚的认识到决策者是如何使用数据，数据是如何影响人们生活的。当然，仅有少数的人能够同时具有这些能力，因此有些人可能对某个领域比较精通，而其他人可能对另一领域比较精通。因此团队的协作就变得十分重要。 本书通过一系列逐渐复杂的例子来阐述数据科学家应该具有的知识和能力。我们使用开源数据软件R以及R-STUDIO来演示真实的数据问题，阐述数据科学家面临的挑战以及解决问题所采用的一些技术。我们使用尽可能真实的数据来进行演示面临的问题。 没有任何一本书能够完全覆盖数据科学所涉及的领域。本书的最后列出了参考文献和资源，有兴趣的读者可以获得更多的知识。R和Rstudio所代表的开源精神告诉我们尽可能的使用基于互联网、自由免费得资料。实际上，本书使用最多的参考文献是维基，一个免费、在线并且有用户自己维护的百科全书。尽管一些人对维基百科的合法性存在疑问，并且维基百科也不是十分完美，但是维基百科仍然是十分有用的学习资源。因为它是免费、自由的，并且覆盖的主题超过任何一本纸质的百科全书不止50倍，并且随时在不断的更新中。因此维基百科是快入门某一主题的有用参考。尽管仅仅通过阅读维基百科，还不能使你成为专家，但是你可以得到很好的入门。 另外一个有用的资源是可汗学院。很多人认为可汗学院仅提供了很多视频给初中高中生来介绍一些数学概念。但是全球成千上万的成年人都通过可汗学院来复习或者快速入门某些学科。可汗学院所有的课程都是免费得，通过你得google或者facebook账户登录，你可以做各种习题，并且追踪你的学习历程。 每章的最后都列出了相关的维基百科的资源以及可汗学院的课程，以及其学习他资源。这些资源使得读者可以深入学习每章的主题以及没有涉及的细节。 当你阅读本书时，你可能需要在IPAD或者其他苹果的设备商访问某些阅读器软件，你可以通过如下网址获取本书，http://jsresearch.net/wiki/projects/teachdatascience/Teach_Data_Science.html. 阅读本书的同时，访问本书提供的各种interne链接是十分有用的。你也需要在你的电脑上实践本书涉及的一些R代码。 最后一点，本书提供的阅读顺序可以并不假设读者具有任何的计算机或统计学知识或经验。如果你具有一定的知识和经验，你可以任意的跳过相关章节。以下为大家提供了免费得学习资源： http://en.wikipedia.org/wiki/E-Sciencehttp://en.wikipedia.org/wiki/E-Science_librarianshiphttp://en.wikipedia.org/wiki/Wikipedia:Size_comparisonshttp://en.wikipedia.org/wiki/Statisticianhttp://en.wikipedia.org/wiki/Visualization_(computer_graphics)http://www.khanacademy.org/http://www.r-project.org/http://www.readwriteweb.com/hack/2011/09/unlocking-big-data-with-r.phphttp://rstudio.org/]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[R数据科学实战-1]]></title>
      <url>%2F2014%2F08%2F16%2F2014-8-16-R%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E5%AE%9E%E6%88%98-1%2F</url>
      <content type="text"><![CDATA[Data Science 团队 Project sponsor：代表了商业需求，赞助项目。提供项目目标 Client：代表了最终用户，领域专家 Data scientist：设定，执行分析战略；与sponsor和client沟通 Data Architect：管理存储数据，有时也收集数据 Operation：管理基础架构，部署，运营 Data Science项目不断循环 目标定义：解决什么问题？ 收集与管理数据：需要什么信息？ 建立模型：寻找数据的模式 评估与优化模型：模型是否解决了我的问题 展示结果/报告：问题解决情况 模型部署：在真实环境部署 目标以一个贷款数据为例) Why do the sponsors want the project in the first place? What do they lack, and what do they need? What are they doing to solve the problem now, and why isn’t that good enough? What resources will you need: what kind of data and how much staff? Will you have domain experts to collaborate with, and what are the computational resources? How do the project sponsors plan to deploy your results? What are the constraints thathave to be met for successful deployment? The goal should be specific and measurable, not “We want to get better at finding bad loans,” but instead,“We want to reduce our rate of loan charge-offs by at least 10%, using a model that predictswhich loan applicants are likely to default.” 数据收集 What data is available to me? Will it help me solve the problem? Is it enough? Is the data quality good enough? 当然对于拿到的数据你还需要根据情况修复数据错误，进行数据变换。 建模 Classification— Deciding if something belongs to one category or another Scoring— Predicting or estimating a numeric value, such as a price or probability Ranking— Learning to order items by preferences Clustering— Grouping items into most-similar groups Finding relations— Finding correlations or potential causes of effects seen in the data Characterization— Very general plotting and report generation from data 1234567library(&apos;rpart&apos;)load(&apos;GCDData.RData&apos;)model &lt;- rpart(Good.Loan ~Duration.in.month +Installment.rate.in.percentage.of.disposable.income + Credit.amount +Other.installment.plans, data=d, control=rpart.control(maxdepth=4), method=&quot;class&quot;) 模型评估 Is it accurate enough for your needs? Does it generalize well? Does it perform better than “the obvious guess”? Better than whatever estimate you currently use? Do the results of the model (coefficients, clusters, rules) make sense in the context of the problem domain? 123resultframe=data.frame(Good.Load=creditdata$Good.Load, pred=predict(model,type=&quot;class&quot;))rtab=table(resulfframe) 报告 How should they interpret the model? What does the model output look like? If the model provides a trace of which rules in the decision tree executed, how do theyread that? If the model provides a confidence score in addition to a classification, how should theyuse the confidence score? When might they potentially overrule the model?]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[机器学习与R(5)-数据准备续]]></title>
      <url>%2F2014%2F08%2F16%2F2014-8-16-machine-learning-and-R(5)-data-prepare-cont%2F</url>
      <content type="text"><![CDATA[Data Framesdata frame应该是R中最常用的数据结构了，你可以把它类比于excel中的表，有行有列。 123pt_data &lt;- data.frame(subject_name, temperature, flu_status, gender, blood, stringsAsFactors = FALSE)pt_data 1234## subject_name temperature flu_status gender blood## 1 John Doe 98.1 FALSE MALE O## 2 Jane Doe 98.6 FALSE FEMALE AB## 3 Steve Graves 101.4 TRUE MALE A 以上代码创建了一个data frame，其中的stringsAsFactors是指出不要把字符变量转换为factor（缺省会转换），所以如果你用 1str(pt_data) 123456## &apos;data.frame&apos;: 3 obs. of 5 variables:## $ subject_name: chr &quot;John Doe&quot; &quot;Jane Doe&quot; &quot;Steve Graves&quot;## $ temperature : num 98.1 98.6 101.4## $ flu_status : logi FALSE FALSE TRUE## $ gender : Factor w/ 2 levels &quot;FEMALE&quot;,&quot;MALE&quot;: 2 1 2## $ blood : Factor w/ 4 levels &quot;A&quot;,&quot;B&quot;,&quot;AB&quot;,&quot;O&quot;: 4 3 1 查看数据结构，subject_name是字符类型。 有了前面基础，你应该很容易理解下面的代码: 1pt_data$subject_name 1## [1] &quot;John Doe&quot; &quot;Jane Doe&quot; &quot;Steve Graves&quot; 1pt_data[c("temperature", "flu_status")] 1234## temperature flu_status## 1 98.1 FALSE## 2 98.6 FALSE## 3 101.4 TRUE 不过data frame还有些新用法 12# 取第1行，第2列pt_data[1, 2] 1## [1] 98.1 12# 取第1行pt_data[1, ] 12## subject_name temperature flu_status gender blood## 1 John Doe 98.1 FALSE MALE O 12# 取第1列pt_data[,1 ] 1## [1] &quot;John Doe&quot; &quot;Jane Doe&quot; &quot;Steve Graves&quot; 12#更复杂的取1，3行的2，4列pt_data[c(1, 3), c(2, 4)] 123## temperature gender## 1 98.1 MALE## 3 101.4 MALE 12#取整个data framept_data[ , ] 1234## subject_name temperature flu_status gender blood## 1 John Doe 98.1 FALSE MALE O## 2 Jane Doe 98.6 FALSE FEMALE AB## 3 Steve Graves 101.4 TRUE MALE A 12#这2个等价，想想...pt_data[c(1, 3), c("temperature", "gender")] 123## temperature gender## 1 98.1 MALE## 3 101.4 MALE 1pt_data[-2, c(-1, -3, -5)] 123## temperature gender## 1 98.1 MALE## 3 101.4 MALE matrixes 与 arrays除了data frame，matrix和array也可以用来保存表格数据，只是要求数据类型相同。比如 12m &lt;- matrix(c('a', 'b', 'c', 'd'), nrow = 2)m 123## [,1] [,2]## [1,] &quot;a&quot; &quot;c&quot; ## [2,] &quot;b&quot; &quot;d&quot; 这里nrow代表行数，matrix缺省按行排列，但你也可以指定： 12m &lt;- matrix(c('a', 'b', 'c', 'd'), nrow = 2)m 123## [,1] [,2]## [1,] &quot;a&quot; &quot;c&quot; ## [2,] &quot;b&quot; &quot;d&quot; 对比一下： 12m &lt;- matrix(c('a', 'b', 'c', 'd'), nrow = 2,byrow=FALSE)m 123## [,1] [,2]## [1,] &quot;a&quot; &quot;c&quot; ## [2,] &quot;b&quot; &quot;d&quot; 类似还有 12m &lt;- matrix(c('a', 'b', 'c', 'd', 'e', 'f'), ncol = 2)m 1234## [,1] [,2]## [1,] &quot;a&quot; &quot;d&quot; ## [2,] &quot;b&quot; &quot;e&quot; ## [3,] &quot;c&quot; &quot;f&quot; 1m[1, ] 1## [1] &quot;a&quot; &quot;d&quot; 1m[, 1] 1## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; Array与matrix类似，只是可以有更多维。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[机器学习与R(4)-数据准备]]></title>
      <url>%2F2014%2F08%2F16%2F2014-8-16-machine-learning-and-R(4)-data-prepare%2F</url>
      <content type="text"><![CDATA[尽管数据准备与整理没有建立模型有趣，但却是极为重要的一步。因为数据的质量决定了模型质量。而多数情况下，输入数据都很复杂，散乱，或者是来自多个数据源，或者是不同格式。正是由于这一复杂性，机器学习大量的工作时实际在于数据准备与探索阶段。 对于应用R来完成这一任务，则需要我们： 掌握R的基本数据结构，利用它们来存储，抽取数据 利用R从不同数据源获取数据 利用R来理解并将复杂数据可视化 R数据结构因为R是大量用于统计数据分析的编程语言，因此R中数据结构的设计也是为了更好的操作这些数据。R中常用数据结构有 vectors factors lists arrays data frames vectorsvector是R中基础数据结构，实际就是一个有序排列的元素，这些元素既可以是数字，也可以是factor，逻辑值(TRUE/FALSE)，也可以是字符。我们通常用c()来建立vector，而赋值则可以使用=或者=。关于=还是=的争论好比之前的咸豆浆还是甜豆浆的争论，大家可以看看大神谢益辉。 以下代码分别建立了三个不同的vector(向量)： 123subject_name &lt;- c("John Doe", "Jane Doe", "Steve Graves")temperature &lt;- c(98.1, 98.6, 101.4)flu_status &lt;- c(FALSE, FALSE, TRUE) 要获取某个向量的值也很简单，用[]直接取下标： 1temperature[2] 1## [1] 98.6 不过要注意R里面下标都是从1开始，而不是像其他程序从0开始。 如果我想取某几个值则可以同时包含对应下标 1temperature[2:3] #取vector的2，3下标对应值 1## [1] 98.6 101.4 1temperature[c(1,3)] #取vector的1，3下标对应值 1## [1] 98.1 101.4 如果前面加一个-就表示，不要包含该值了 1temperature[-2] 1## [1] 98.1 101.4 1temperature[c(-1,-3)] 1## [1] 98.6 除了这种方式，采用逻辑变量来取子集也是一样，比如 1temperature[c(TRUE, TRUE, FALSE)] 1## [1] 98.1 98.6 FactorsFactors通常用于nomial变量，比如性别：male,female。也许你会问为什么不直接用character vector呢？很简单采用fator更高效，计算机会把factor：male,male,female保存为1,1,2，更加高效。另外一个原因就在于机器学习有特殊的处理分类变量的方式。 建立factor相当简单 12gender = factor(c("MALE", "FEMALE", "MALE"))gender 12## [1] MALE FEMALE MALE ## Levels: FEMALE MALE 上面是采用了缺省顺序，你也可以指明factor的顺序，例如： 12blood = factor(c("O", "AB", "A"),levels = c("A", "B", "AB", "O"))blood 12## [1] O AB A ## Levels: A B AB O Listslist是一种特殊类型向量，通常向量只能保存相同类型数据，而list则可以保存不同类型数据。正是由于这一灵活性，list通常用来保存输入输出数据或者机器学习中的配置参数。 以前面的数据微冷，如果我们要看某一病人的信息，我们需要执行如下操作： 1subject_name[1] 1## [1] &quot;John Doe&quot; 1temperature[1] 1## [1] 98.1 1flu_status[1] 1## [1] FALSE 1gender[1] 12## [1] MALE## Levels: FEMALE MALE 1blood[1] 12## [1] O## Levels: A B AB O 实在麻烦，那我可以建立一个list： 12345subject = list(fullname = subject_name, temperature = temperature, flu_status = flu_status, gender = gender, blood = blood) 这时输入: 1subject 12345678910111213141516## $fullname## [1] &quot;John Doe&quot; &quot;Jane Doe&quot; &quot;Steve Graves&quot;## ## $temperature## [1] 98.1 98.6 101.4## ## $flu_status## [1] FALSE FALSE TRUE## ## $gender## [1] MALE FEMALE MALE ## Levels: FEMALE MALE## ## $blood## [1] O AB A ## Levels: A B AB O 全部信息同时都有了，如果我只想看某个变量那么： 1subject[1][1] #代表查看list中第一个元素的第1列变量值 12## $fullname## [1] &quot;John Doe&quot; &quot;Jane Doe&quot; &quot;Steve Graves&quot; 1subject$temperature #代表查看list中temperature列 1## [1] 98.1 98.6 101.4 1subject[c("temperature", "flu_status")] #代表查看list中temperature列以及flu_status列 12345## $temperature## [1] 98.1 98.6 101.4## ## $flu_status## [1] FALSE FALSE TRUE]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[机器学习与R(3)-利用R进行机器学习]]></title>
      <url>%2F2014%2F08%2F16%2F2014-8-16-machine-learning-and-R(3)-use-r-for-machine-learning%2F</url>
      <content type="text"><![CDATA[R机器学习的包R有着超过了4000多个包，那么哪些与机器学习相关呢？CRAN Task View则是你应该关注的地方。 安装R包以下命令是安装weka这个机器学习的R包。 1install.packages("RWeka") 当然你也可以指定安装的目录 1install.packages("RWeka", lib="/path/to/library") 如果不懂lib参数是什么，看帮助则是最好选择 1?install.packages 安装完R包以后要使用它则可以使用 123library(RWeka)#或者require(RWeka) 这个library和require有什么区别？看帮助;-)]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[机器学习与R(2)-机器学习的类型]]></title>
      <url>%2F2014%2F08%2F16%2F2014-8-16-machine-learning-and-R(2)-machine-learning-type%2F</url>
      <content type="text"><![CDATA[有监督与无监督(supervised learning vs unsupervised learning)机器学习可以分成两组：构造预测模型的有监督学习与构造描述性模型的无监督学习。 预测模型需要理由数据中的其它变量值来预测某个变量的值，需要预测那个变量就成为了目标变量，其它的变量则是feature变量。预测模型明确指出了什么需要学习，那么这个过程就是有监督学习。 常用的有监督学习方法有分类(classification)，回归(regression)等。比如预测： 某人是否信用卡会赖帐 某球队是否会赢 地震是否发生 收入 测试成绩 描述模型更多的是对数据的模式的总结，因为没有学习目标，所以是无监督学习。例如market basket analysis，分析客户买的东西间的关联。描述性模型将数据集分成同类组叫聚类(clustering) 算法与数据匹配通常Classification问题我们用： Nearest Neighbor naive Bayes Decision Trees Classification Rule Learners 数字预测我们用: linear regression regression trees model trees 而Neural Networks,Support vetor machines则两种情况都可以。 对于无监督学习，常用association rules来解决模式detection，k-means clustering来进行聚类分析。 总结起来我们的问题就变成了：分类，数字预测，模式detection，聚类。不同的问题需要不同算法。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[机器学习与R(1)-什么是机器学习]]></title>
      <url>%2F2014%2F08%2F16%2F2014-8-16-machine-learning-and-R(1)-what-is-machine-learning%2F</url>
      <content type="text"><![CDATA[机器学习与数据挖掘智能终端的普及，传感器的大量使用，视频监控，可穿戴设备,UGC，…数据爆炸让我们进入了大数据时代，也许更确切说应该是我们的时代更容易获取数据。大量的数据可以通过机器获取，利用它们我们可以做出更好决策。机器学习某种意义上就是研究利用计算机算法来把数据转换成智能。它的产生来自于数据的可获得性，统计方法，计算能力的同步，快速发展。数据的增长，推动了计算能力的发展，进而推动分析大规模数据的统计方法的发展，三者互为动力，循环发展。 与机器学习经常一起出现的就是数据挖掘了，两种经常会有重叠的地方，不过数据挖掘某种意义上更多的是关注从大量的数据中获得新的见解（insight），机器学习聚焦于进行已知的任务，而数据挖掘则是搜寻隐藏的信息。例如电商利用机器学习来决定向谁推荐什么产品，数据挖掘用来了解什么样的人喜欢什么产品。某种意义上机器学习的算法是数据挖掘的基础，反之则未必。 机器学习的使用与滥用利用机器学习我们可以： 预测选举 垃圾邮件过滤 根据路况进行自动的信号灯变换 犯罪预测 估计客户流失率 自动导航 定向广告 … 你也许也听过了Target向某用户寄尿片广告的故事？不知道？点这里看看福布斯的报道,零售商已经开始利用你的购买模式，分析出你的家庭，你的收入，你的喜好，甚至你的健康….，对数据的利用已经无处不在。会员卡的信息也可以用来进行机器学习。 零售商可以用机器学习来管理广告，定向促销，存货，商店布局，设置于零售商说不定以后可以根据你买的东西在结账的时候再来动态向你促销。 不过数据的滥用也导致了大量的问题，个人的隐私已经无所遁形，这已经成为了一把双刃剑，我们可以在多大程度上利用这些数据，如何利用？未来是我们改变了数据还是数据改变了我们？ 机器如何学习无论人还是机器学习，无外乎三步： 数据的输入 抽象 泛化 将原始数据赋予意义的过程就是抽象，这是知识表达的基础。对计算机而言则是将输入变成一个模型，而模型的获得则是通过对数据的training来完成。为何不是学习呢？因为学习不是止于数据抽象，学习需要更进一步，将知识泛化，用于未来的新数据。Training更形象说明了，我们用模型来适配数据的过程。以上过程可以表达为： 观察–&gt;数据–&gt;模型 泛化The term generalization describes the process of turning abstracted knowledge into a form that can be utilized for action. 不过在这个泛化过程我们始终存在着bias与variance，variance产生源于如果我们用不同的数据来训练，那么我们获得的模型可能有不同，另一方面bias则是在于我们用模型来适配现实问题是，模型的简化，我们不可以找到一个100%与现实中问题匹配的模型。机器学习始终都会面临bias与variance的选择，评估模型一方面我们要避免bias，另一方面我们希望variance足够小。 机器学习的步骤 数据收集：哪些数据可用，还需要收集什么数据，如何收集… 探索与准备数据数据的质量绝对了模型。通常这个阶段会占掉80%时间。此时你需要判断数据的模式，质量，有无异常，不是所有数据总是可获得，这时怎么处理… 训练模型 评估模型 模型优化 部署]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[数据之路]]></title>
      <url>%2F2014%2F08%2F16%2F2014-8-16-data-road%2F</url>
      <content type="text"><![CDATA[数据科学家自我修养——一份数据科学的开放课程清单最近一年以来，大数据这个概念被吹嘘的天花乱坠，仿佛你要是不说大数据就落伍了。继云计算之后，大数据已然成为IT行业的热点。《哈佛商业评论》更是宣称“数据科学家”是二十一世纪最性感的职业。所谓性感，既代表着难以名状的诱惑，又说明了大家都不知道它干的是什么。这里我不想重复什么是大数据，什么是数据科学，而是想以个人过去接近2年的通过MOOC（开放课程）来学习数据科学的实践来给出一份个人建议的数据科学学习之路的课程与书籍清单。 数据科学家的自我修养Drew Conway给出的数据科学的一个文氏图，很好的诠释的数据科学的技能要求。而这里我基于传统的道，术，用来将数据科学的课程分成三类在后面一一列出。不过还是让我们先从数据科学入门谈起。 数据科学入门如果你公司的管理层，只是感觉想了解一下什么是大数据，个人建议从big data for performance这门课开始，课程有4个模块，很简单的内容，当然也有些不是很正确的内容;-)，但作为入门还是不错的。接下来你需要了解一下Hadoop，来自Udacity的Intro to Hadoop and MapReduce是个不错的选择。 最后如果你要想忽悠一下别人，看看维克托•迈尔•舍恩伯格(Viktor Mayer-Schönberger)的大数据时代，这是国外大数据研究的先河之作。 如何利用大数据数据科学之道要掌握数据科学，基础的数学与统计学知识不可避免，这里强烈推荐： 普林斯顿大学Statistics One 统计学基础，假设检验，ANOVA，线性回归等等 斯坦福大学Statistics learning 基本的有监督学习介绍，回归，分类，聚类，树，SVM，K-means clustering等等 这两门课都是名校教授讲解，课程深入浅出，一个帮助你统计学入门，一个帮助你数据分析与机器学习入门。不过如果你听统计学初步都觉得吃力，那么可以考虑先听一下台湾大学的概率论的前几讲，对概率有了初步知识后再学统计。 当然如果你想更深入一些你一定不能错过斯坦福大学的Machine Learning，这是Coursera创始人的经典课程。 除了以上课程，你也可以看看约翰霍普金斯大学5月开设的3门课程： Statistical Inference Practical Machine Learning Regression Models 当然Edx,Coursera也是寻找相关课程的好地方 数据科学之术有了数据科学之道，我们下一步需要进行的就是如何实现它了，有人推荐Python，也有人推荐R，或者来自Apache的mahout，个人推荐Python+R，于是你可以看看： 莱斯大学的An Introduction to Interactive Programming in Python 华盛顿大学的High Performance Scientific Computing 约翰霍普金斯大学的R Programming 之前约翰霍普金斯大学还开过： Computing for data analysis data analysis 不过似乎在去年我学完后没有再开了。 了解了R，Python，下一步就是大名鼎鼎的Hadoop生态系统了，Udacity上的Intro to Hadoop and MapReduce是不错的入门选择，之后IBM 的大数据大学上的Hadoop，云计算，课程也是不错的选择。 数据科学之用一切技术最终都要回归商业的本质，掌握了数据科学之道和数据科学之术，我们还需要将其应用于商业中。这里首当其冲的就是如何把从数据中解读的智慧表达出来，说服别人，这里要推荐来自密歇根大学的Introduction to Public Speaking。这门课可以帮助你更好的组织你的讲演，演示等（目前这门课正在开课中）。 除了表达能力，很多时候我们的数据不是单纯的数据，我们需要理解数据分析与公司战略的关系，如果我们要开发数据产品，那么它是如何影响我们的运营，财务决策的，当然最终所有的一切都会受到宏观经济的影响，以下的几门课程可以帮助你更好的理解数据之用： 马里兰大学 Developing Innovative Ideas for New Companies 弗吉尼亚大学Foundations of Business Strategy 加州大学The Power of Macroeconomics 清华大学财务分析与决策 宾夕法尼亚大学An Introduction to Operations Management 最后我要推荐Data Science for Business这本书，这本书将数据科学之道与用完美结合，亚马逊上五星评价！ 其它资源除了以上列出的课程，你也可以参考： coursera上约翰霍普金斯大学的数据科学专业课程 data sicence。一共9门课，目前每个月都在开课。 来自网络上的数据科学硕士开发课程清单 以及学堂网收集的数据科学课程]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[apply,sappy,]]></title>
      <url>%2F2014%2F08%2F03%2Fapply%2F</url>
      <content type="text"><![CDATA[投稿作者：你的网名 函数功能Returns a vector or array or list of values obtained by applying a function to margins of an array or matrix. 函数参数说明apply(X, MARGIN, FUN, …) X –MARGIN – 函数使用示例]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[软件定价(11)-价格与你]]></title>
      <url>%2F2014%2F03%2F04%2F2013-07-12-software-pricing-11%2F</url>
      <content type="text"><![CDATA[关于版权本书的翻译采用CC协议，转载请标明出处http://www.zjdian.com, 翻译错误请联系微博行走的家园提出您的宝贵意见 第五章：价格如何代表你（以及你如何改变它） 价格从来都不是中性的。它们代表了某种信号。例如高价格可能意味你有高质量的产品。消费者认为昂贵的香水和酒比便宜的要好，即使这没有任何证据。 低价可能告诉消费者你的性价比或者你的特别。如果你的竞争对手以10，000美元每个拷贝出售软件，而你以100元价格出售，那么这一价格已经说明了你。当然，你可能会说这是‘改变游戏规则’，但是你的客户听到的可能是‘玩具’。 模仿你的竞争对手，你可能被认为只是一个‘复制’产品。如果你是复制产品，有着模仿的功能，复制的价格，那么人们为什么要从你那里购买呢？尤其是当市场中已经有了一个强势，垄断的产品时。 不管你选择什么价格，它发出的信号需要与你的品牌匹配，你的品牌又需要和你的现实相符。如果你不愿意花费资金在市场营销上来支持你的品牌，在研发上投入来使得高质量成为现实，在客户服务上投入向用户提供高质量品牌的服务，那么使用高价格来暗示你有高质量的产品是毫无意义的。 1996年，麦当劳发布了Arch Deluxe产品，一种为追求精致生活的成年消费者提供的汉堡包。为了收回高质量原料的额外成本以及2亿美元的营销费用，麦当劳对给这一新的三明治定价比巨无霸高32美分。但是他们想试图建立的产品（高质量，溢价）与麦当劳的品牌相冲突（便宜，便利），Arch Deluxe彻底失败。其中一种说法是可能他们给这款汉堡包定价太低，32美分的加价没有给出足够的质量暗示。 你的商业模式和战略需要支撑你的定价模式。如果你有着花费巨大的销售人员开着豪华汽车，带着你的客户出去打高尔夫，你的最终用户需要大量的手把手协助和对你卖给他们的软件进行定制，那么采取低价你是无法支撑的。类似，如果你是在网络上销售简单包装的大众软件，那么高价将毫无成效。 当Red Gate公司试图进入网络负荷自动测试市场时，其中一个失败的原因就是我们在一个高价格解决方案主导的市场采用了低价，高销量方法（还有许多其它原因，包括软件产品也不够好）。我们认为用户会喜欢能够直接下载，试用然后再购买的产品，但实际是我们的客户需要比我们能提供的还多的大量支持。对大多数客户而言，他们需要的不只是一个产品，他们需要的是人员密集的服务，以及一个大名鼎鼎，昂贵的供应商能提供的保证。我们的负荷测试工具获得了普通的成功，但从来没有达到我们梦想的成功。 .com泡沫的破灭包含了大量没有将他们的商业模式与定价模式匹配的例证。例如，Kozmo.com公司提供1小时内向城市居民配送零食，租的DVD和星巴克咖啡的服务。不幸的是，它选择的低价，大用户规模的商业模式与使用昂贵的自行车配送员配送这些小东西的现实相矛盾。也许高价，低用户规模的商业模式可能会成功，英式贵族的管家证明了这一点。 改变战略可能会相当困难。比如当英特尔引入8080处理器时标价340美元。最终它以2美元的价格出售，然而英特尔发现要改变人们心中最初的那个高价的印记却极为困难。 ##实践出真知原理 现在，你已经学习了很多理论，它们基于我的经验和充分的研究（有些你可以在本书后面的文献参考中找到）。但是你的实际情况可能与本书描述的可能不尽相同，因此永远也不要忘了实践出真知原理。 产品定价不仅是科学也是一门艺术，工艺。当然理解定价经济学和心理学会有帮助，但理论只能告诉你那么多。在某个时间，你必须做出决策并实施。利用本书中的信息来进行一个好的定价是多少的有根据的尝试，你的客户会如何反应，实际尝试它。完全正确的价格并不重要-让它大致正确，而不是完全搞砸-之后你可以调整。 你永远也不知道你选择的价格是否正确，但是一旦你设定了一个初始价格你就需要进行试验；不是形成假设的科学意义上的试验，改变一个变量，之后接受或者否定这个假设，而是改变一些项然后看会发生什么。 对定价进行科学实验实在太难，其结果也不明确。这当中有太多变量在变化。当你改变你的产品价格时，你可能只是因为你发布了一个新版本，或者与一个大的市场活动同步，或者其它你无法控制的因素，例如经济状况或者竞争对手的反应将交织在一起。 尽管符合科学的纯粹，不过一次只修改一个变量的单纯科学方法并不是经常有意义的。理论上你不应该同时修改你的产品价格，折扣策略和捆绑销售类型。实际而言，可能这是正确的。解决问题要比理解为什么不行更重要。一个科学家去约会不成功，理论上他一次应该改变一个变量，然后重新约会。首先他改变约会对象，但是还是去看同样的电影，买同样的花。之后，他可以保持约会对象不变，花不变，改变电影，以此类推。但是实用主义者会随意改变，或者应该改变约会的女孩，电影，花，再买件新衣服，刮刮胡子等。如果成功了，他可能不知道为什么，但是他至少会有一个女朋友。 在早期，进行试验可能很容易。你可以进行A/B测试，把用户分到随机组，给每个组不同的小册子，上面有不同价格，然后度量结果。而现在，这有着极大风险。网络让人们很轻松的就知道别人付多少钱。 你可能想先做个调查，测试一下人们对新定价模式或者改变现价的反应。然而，调查很少有效。客户所说的和他们所做的总是不一致。当麦当劳发布它的Arch Deluxe汉堡包时（参见上文），焦点人群中的客户喜欢它，80%的人说他们会买。实际很少人真的买了。 如何改变你的定价你可能会担心当你改变价格时客户会如何反应。完全不必如此。对大多数企业而言，我们的客户有其它更值得担心的事。如果我们把价格从100美元涨到150美元，大多数人不会注意，那些注意到的里面又只有很少人会在意。如果你从Red Gate网站购买SQL Compare，在2000年只要50美元，现在你则需要395美元。购买全套工具则需要1595美元。 当然在Red Gate我们花了几乎10年才达到那一价格。我们花了几百万美元开发软件，这是几十个人年的工作。客户从我们的软件获得的价值大大超过了其成本。然而在我们的几十万客户中只有几百个曾经指出我们的价格上涨了。 不是你的客户说价格多么重要，而是他们的行为。无论何时你改变了价格，密切注意你的客户怎么做。如果他们不再购买，那么就需要重新考虑了。 产品定价清单圆满结束！以下是帮助您决定定价的清单： 你的战略是什么？你是想定低价，大量销售，还是高价，少量？这如何与你的品牌，你的产品，以及你想表达的形象匹配？ 你的产品是什么？不要忘记它不只是你销售软件，它包含软件周围的所有内容。 你的客户会如何判断定价的公平？他们会使用什么参考点？他们会如何决定什么价格是看起来正确的？他们会对你选择的价格感到犹豫或者是他们会接受它？ 谁是你的客户？他们的生意是如何运作的？他们期望的收费方式是什么样的？他们有多少钱？他们喜欢一次性付费，还是每月付费？深入他们的心里！ 谁是你的竞争对手？他们对你的定价如何响应？你的产品比他们的价值多或者少多少？他们的商业模式是什么？他们的价格是多少？如果你定价较低，是否会导致价格战？如果你这样做，钱包的钱是否足够多以支持你打赢价格战？你是要和你的竞争对手并存还是要消灭他们？ 你如何销售你的软件？你需要派你的销售人员带客户去打高尔夫吗？或者你计划采用在网络上的低顾客接触方式的销售？你是否需要一个电话销售团队？每个销售人员的成本是多少？你是通过渠道或者经销商来销售？他们将获得多少分成？ 你可以细分客户，创建不同版本吗？你的软件对不同人群价值是否不同？可以用不同定价来反映这一点吗？例如学生和商务人士，或者普通和高级用户，或者你可以根据地区或品味划分？ 你可以如何捆绑销售你的软件？你可以创建一个更大的包，包含不止一个软件吗？ 有根据的推测你的价格不管任何心理学和经济学，你最终还是需要选择一个价格。某个价格-任意价格-要比没有价格好。 实践实践出真知原理。实践你的定价，看有什么结果。如果你的价格大致正确-并且你已经做了这么多应该做的-那么你还可以以后再调整它。 参考文献Anderson, E. and Simester, D. (2003) ‘Minding your pricing cues’Harvard Business Review, September 2003 Breckon, N. (2009) ‘Valve: Left 4 dead half-price sale saw 3000%increase, beat launch numbers’http://www.shacknews.com/onearticle.x/57308 Chapman, R. (2006)‘In Search of Stupidity’, 2nd ed, Apress, Berkeley Crampes, C. and Laffont, J-J. (2002) ‘Copying and software pricing’ Cusumano, M. (2007) ‘The changing labyrinth of software pricing’ Communications of the ACM, Vol. 50, Issue 7, pp. 19-22Davidow, W. (1986) ‘Marketing high technology – an insider’s view’, The Free Press, NewYork Gallaugher, J. and Wallace, E. (2002) ‘Understanding network effects insoftware markets: evidence from web server pricing’ MIS Quarterly, Vol. 26, No. 4, pp. 303-327 Gilbert, A. (2003) ‘CRM software or CRM shelfware?’http://news.cnet.com/CRM-software-or-CRMShelfware/2100-1012_3-990880.html Gourville, J. and Soman, D. (2002) ‘Pricing and the psychology ofconsumption’ Harvard Business Review, September 2002 Gourville, J. (2006) ‘Eager sellers and stony buyers’,Harvard Business Review, June 2006 Gourville, J. and Soman, D. (2005) ‘Overchoice and assortment type:when and why variety backfires’, Marketing Science, Vol. 24, No. 3, pp. 382-395Gourville, J. and Soman, D. (2001) ‘Transaction decoupling: how pricebundling affects the decision to consume’Journal of Marketing Research, Vol. 38, pp. 30-44 Gourville, J. and Soman, D. (2007) ‘Extremeness seeking: when andwhy consumers prefer the extremes’,Harvard Business Review Harford, T. (2008) ‘Business life: Fair trade or foul’http://timharford.com/2008/04/business-life-fair-trade-or-foul/ Knopf, J. (2000) ‘The Origin of Shareware’http://www.asp-shareware.org/users/history-of-shareware.asp Levitt, T. (1980) ‘Marketing success through differentiation – ofanything’,Harvard Business Review, January – February 1980 Macrovision (2007) ‘Key trends in software pricing and licensing’ Mason, M. (2008) ‘The pirate’s dilemma’,The Free Press, New York Miller, P. (2006) ‘Sony losing mad loot on each PS3’http://www.engadget.com/2006/11/16/sony-losing-mad-loot-on-each-ps3/ Murph, D. (2006) ‘Wii Manufacturing Costs ring up to just $158?’http://www.engadget.com/2006/12/15/wii-manufacturing-costs-ring-up-to-just-158/ Packard, D. (1996)‘The HP Way’, HarperCollins, New York Quint, B. (2001) ‘Dialog rolls out new connect-time pricing’http://www.allbusiness.com/sales/1012692-1.html Sink, E. (2004) ‘Product Pricing Primer’,http://www.ericsink.com/bos/Product_Pricing.html Spolsky, J. (2004) ‘Camels and Rubber Duckies’http://joelonsoftware.com/articles/CamelsandRubberDuckies.html Spolsky, J. (2006) ‘Simplicity’ –http://www.joelonsoftware.com/items/2006/12/09.html Stiff, D. (2007) ‘How DeWALT turned customers into influencers’http://credibilitybranding.typepad.com/blog/2007/03/how_dewalt_turn.html Sutton, J. (2001)‘Technology and market structure’, 2nd ed, The MIT Press Varian, H. (2003)‘Intermediate Microeconomics’, 6th ed, W.M. Norton, New YorkWendt O., von Westarp, F. and König, W. (2000) ‘Pricing in NetworkEffect Markets’, ECIS Proceedings, Paper 82Wayne, B. (2009) ‘YouTube is doomed (GOOG)’http://www.businessinsider.com/is-youtube-doomed-2009-4 Wikipedia – ‘3DO Interactive Multiplayer’http://en.wikipedia.org/wiki/3DO_Interactive_Multiplayer]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[软件定价(7)-定价陷阱]]></title>
      <url>%2F2014%2F03%2F04%2F2013-07-9-software-pricing-7%2F</url>
      <content type="text"><![CDATA[关于版权本书的翻译采用CC协议，转载请标明出处http://www.zjdian.com, 翻译错误请联系微博行走的家园提出您的宝贵意见 #第三章：定价陷阱 到此为止，我们已经讨论了一些经济学理论，定价心理学。相信，你已经对如何定价有了一些想法。但是我们还需要记住关于定价的其他因素，以及需要小心对付的定价陷阱。 ##竞争对手 当你为你的产品设定价格时你需要考虑你的竞争对手如何应对。如果你降价，他们是否会发起价格战？即使你的竞争对手有一个高成本的商业模式，无法和你在价格上长期竞争，但由于你严重威胁到了他们的生存而使他们必须回应的风险仍然存在，他们只能寄望你先出局。 航空行业就是发起价格战是无益的最好例证。1977年9月26日，Freddie Laker的第一条空中列车航班从伦敦的盖特威克飞往纽约，回程航班价格只要238.5美元（再加上几美元饭钱），比竞争对手票价的一半还要低。 5年后，Laker航空业已成为了它发起的这场恶意，可恨价格战的牺牲品，因价格战而梦碎。正如Laker一样， EOS航空，Silverjet以及Maxjet都相继发现，以价格为基础来开拓一个行业在最好的情况下是高风险，而最坏的情况则是自杀，尤其是当你的竞争对手除了厮杀到死而别无选择的时侯。 如果你将从价格上进行竞争，那么你必须降低竞争对手反击的可能性。不要敲着战鼓，告诉媒体你将毁灭他们（网景公司的Marc Andreessen犯的一个错误就是当他说“我们将让他们屁股冒烟”-指微软（或者如Andreessen所说的“那些在雷得蒙德的傻瓜”）。专注于他们的边际客户，期望当他们发现你时已经太晚才是正确的方式。 另一方面，如果你的产品定价过高，是否会有其他竞争对手进入？将Time Tracker定价在10，000美元，你可以创建一个新的市场，不过你的竞争对手将生产Tyme Trakka 3000，廉价出售，抢走你的生意。 微软就以经常这样干而出名。他们等待竞争对手（经常还是合作伙伴）去证明一个高价，低市场容量的市场-不管是CRM还是测试工具或者商业智能-之后他们就会以低成本，高市场容量的模式加入战团。 ##公平 不管你如何为你的产品定价，一定要记住消费者对公平有着敏锐的识别力，尽管这时常是非理性的。违反这一点前，请三思而后行。 出版业为此提供了一个绝佳的例子。经济学家会说我从阅读纸版Sebastian Faulk的詹姆斯.邦德惊险小说“不顾一切”获得的价值（传统经济学都是关于价值）与阅读电子版一样。然而纸版和电子版价格都是一样（14美元）。短视的出版商对纸版和电子版定价一样是不公平的。我们会感觉被愚弄了，我们不喜欢这样。 ##盗版 如果你定价太离谱，那么你会为一类特殊竞争对手创造机会：盗版。为软件定价过高，或者定在人们认为‘不公平’的价位，那么准备着最终被盗窃吧。 不过凡事有两面，盗版者有时也可以是你的朋友。 首先，如果你的战略是为每一个潜在客户以他/她能承受的价格提供产品，最终形成全球垄断，那么盗版为你提供了一个便宜的地下渠道。他们把你的软件拷贝提供给那些不会付钱，无法承受这个价格的人手中，这些人要么是不够正直要么就是太有原则了。结果是，使用盗版的人里面最终有些人会付钱。 这就是早期的共享软件运作的方式。80年代早期，电子公告牌和用户讨论组是网络传播盗版软件的主要方式。1982年Andrew Fluegleman和Jim Knopfpiggy利用这一网络，在他们的软件里面加上了一个提示，请求人们如果喜欢他们的软件请为之付费，共享软件由此产生。 Adobe公司也利用了盗版，只是可能没有意识到而已。尽管Photoshop软件有很多更便宜，甚至免费的竞争对手，它的售价还是700美元。这怎么可能？人们先使用盗版Photoshop，之后当他们良心发现或者更有钱的时侯就会购买它。如果Adobe把价格降到300美元，盗版用户可能还是不会买，那么公司在那些愿意付700美元的用户上获得的收入就会减少。所以最好的策略还是拥有一个保持高价，盗版者希望，有一天，他们终会合法拥有的产品。 第二个盗版者是朋友的原因是他们是一个风向标。他们预示了某个市场被疏忽。大多数人不是天生的骗子，但是高价会迫使他们偏离良好的本性。苹果公司意识到了非法下载站的成功，这预示了人们对便宜可下载音乐的需求。满足人们的需求的战略远比音乐唱片行业逃避现实的策略要好。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[2012智能手机总结与2013展望]]></title>
      <url>%2F2014%2F03%2F04%2F2013-01-06-mobile-phone-summary%2F</url>
      <content type="text"><![CDATA[一个IT屌丝的2012智能机回眸与2013智能手机展望 回眸-20122012注定是中国手机史上值得浓墨重彩的一年。三星大屏当道登上王者之位，诺基亚放手塞班黯然陨落；一边是两年前异军突起的HTC的滑铁卢，一边是国产厂商联想的成功逆袭；一边是中兴，华为，酷派背靠大树好乘凉，报住运营商大腿的同时又在看着用互联网来玩手机的小米崛起，另一边则是360特供机，青橙，大可乐，小辣椒的搅局，我认识的几个朋友他们公司刚发布的自拍手机Pandora也在新年来临前凑热闹；特立独行的魅族仍一步步向前迈进，设计突出的Oppo Find5也让人惊艳。这是手机的战国，这是手机的诸子百家时代，这是最好的年代，也是最坏的年代！仿佛间智能手机在从09年到12年的3年多时间就快走完PC十年的路，CPU从单核到双核，四核；内存从128M到，256，512，1G，2G；显示屏从3.5寸到3.7,4,4.5,5寸，从TFT,OLED，AOLED，Super AMOLED，IPS让我们眼花缭乱，分辨率从320x480到480x800,540x960,720x1280,1080x1920，摄像头从300万，500万像素，到800万,1300万像素…。2013.1.4，这是民政局那些等着登记结婚的新人的愿望，也是此时正认为抓住了中国智能机黄金时代的厂商的想法。手机厂商明年还能有爱你一生一世想法吗？ 展望-2013经历了波澜壮阔的一年，也许2013会有些沉闷？国内智能手机市场还能有什么新的突破？苹果随着天才乔布斯的去世，是真的被咬了一口了，不知道2013还有什么惊喜？Android厂商的6核，8核的CPU，4G,6G的内存，超越视网膜屏后的手机是否有实际价值？我们需要可穿戴的设备？谷歌眼镜，新的苹果？抑或是可折叠手机？2013，我似乎嗅到了血的味道，手机的战国，捉对厮杀吧！既然如此，不如我们来买个彩票预测一下？水晶球不一定很准，不过让我们来看看有什么可能也好。这已经不是三国杀的时代？ 东汉末年的三国鼎立之后就是魏晋，南北朝？还是历史不会重演？也许不会，iOS，Android，WP的三国杀还未结束，一帮前诺基亚员工的旗鱼（Sailfish），Mozilla的Firfox os也将在2013加入战团，手机版Ubuntu也在1月6日已经透了个风声，传言三星将“移情别恋”Tizen系统，2013对智能手机操作系统而言注定将是不平凡的一年。Android和iOS已经建立起坚固app store城墙，其他系统的杀入必然需要在利用已有规则的同时，找到自己的利基市场。于是旗鱼的Jolla手机推出了一个解释层来运行大多数Android应用，Firefox OS和Ubuntu则直接利用HTML5来完成自己的使命。也许在13年Android，iOS借助已有的趋势仍然会保持领先，苦逼的WP还是会不死不活，但是我们不要忘记iOS专注高端，Android，WP在低端设备上的流畅性都不敢恭维，如果能有一个系统在比如300～400元左右的手机上能流畅运行，不能说不是一个新的利基市场，中国也好，不发达国家也好必然会有大量的如此需求，传言Firfox OS似乎有这能力？又或者是Jolla手机的手势控制界面，Firefox OS的HTML 5能给我们一种新的体验，都将是值得期待的？苹果当年以一句“Think Difference”的广告打响了与IBM的“大卫与歌利亚”之战，很期待2013的新面孔。 市场-知识青年到农村去手机市场销售我们可以简单的分成运营商渠道，零售店渠道，互联网渠道。小米的互联网期货模式让各手机厂商都眼红，然而这不是谁都能玩的，也不是什么时候都可以玩的，未来这种模式必然还将继续，但是是否还能达到小米最初的效果就要大一个大大的问号了。从发达国家运营商模式，我们可以看到运营商是一个主要的手机销售渠道，用户签订合约，每2～3年都会换手机，换机成本极低。目前国内运营商的0元购机在个人看来都是忽悠，但这不代表被OTT阉割的运营商未来不会学习国外运营商，也走上这条路，如果这样，抱运营商大腿未尝不是一个办法。不过个人私下以为中国要建设有中国特色的社会主义，智能手机是否也是如此？毛主席当年号召“知识青年要到农村去”，我们的智能手机厂商是否应该考虑城乡结合部的零售店，重走山寨机时代的农村包围城市的长征路？从最近几年换机的人群分析，09年Android异军突起，10，11渐入佳境，12年的舍我其谁，城市换智能机的人群必然会逐渐越来越少，相反这股新风可能才吹到农村，是否应该研究一下底层屌丝的需求，把零售店开到他们家门口？这方面文艺青年很不屑的尼采已经先行，虽然我们不一定看好尼采手机，但是那些所谓的尼采工厂店未尝不是尼采老板的一步棋？生活在成都的许多人也许不知道在成都北部的有川陕和传化两个物流市场，在那有几个占地面积只有几个平方的小手机应用安装店，每天都有不少货车司机花5～10元 去那里给自己的手机拷音乐，装游戏，下电影，这些小店活得很快活，因为这两个物流市场每天的司机都好几千。如果哪天尼采的工厂店不只卖尼采手机，还干点别的，变成了所谓城乡结合部的用户买手机的门户，我不会奇怪，这就是互联网的流量。个人的观点是未来换机的人在哪里，我们就跟到哪里，城市，农村一把抓固然好，但要看自己的能力和谁最匹配。 大哥，智能手机不是PC机，不要对着硬件参数买看了手机操作系统，市场，我们再来看看手机硬件的发展。2012手机硬件的飞跃太多，也许2013不是硬件发展的重点。正如当年我们经历了对着硬件参数买PC时代后的回归，智能手机单纯比拼硬件后也会回归。PC组装机繁荣之后，活下来的只有品牌机，智能机是否会如此？消费者在眼花缭乱之后是否会更看重手机的外观设计，系统稳定性，易用性，品牌？手机硬件方面，如果一定要找一个我期待的，也许我唯一期待的就是电池，大屏，多核创造了一个移动电源产业。不过好像石墨烯电池短时间还不能商用，所以个人感觉看点不多。当然芯片商还是会不断推出新产品，但不会那么刺激了，是否会有新的传感器出现？ 手机研发-工欲善其事，必先利其器风大了，猪都会飞。所以现在会飞，不代表以后能飞起来，我们还是需要看看手机研发。然而，操作系统和硬件已经在那里了，我们能做什么？个人认为手机研发不是我们普通意义的研发，也许手机研发可以在这些方向努力： 工业设计：什么样的手机让人看了就忘不了？当年TCL的钻石手机的红火不会在智能机时代重现，但通过工业设计借尸还魂还是极有可能的。因为iPhone都已经是街机了，哥要的个性还没来。 Android手机的差异化：HTC 弄自己的Sense UI，Moto，三星也有自己的UI，这是一个方式。但是我们不能考虑另一种用户人群差异化的方式吗？那些只要打电话，发短信功能的人大有人在，面向他们的智能机是什么，360搞了个学生机，只是一个价格表象，有更深层次的分析学生的需求吗？或者学生人群是个伪命题？小米ROM开局，那你是弄个无间道的ROM来满足他们的需求先，还是单刀直入？这样的按照用户人群来分类的方式还有很多，是否有研究的价值？ 快速并行多型号手机研发的能力：如果你放弃了硬件的比拼，有无能力并行开发面向不同群体的手机，成本能否控制，速度是否够快？于是这又回到了手机的产品管理，研发团队的管理模式，供应链上面，天龙八部里面那个少林老僧才是最终的高手啊。 品牌与定位-高富帅，屌丝？老人说，“人啊，就是要认命。” 这句话，放到手机行业，有一定意义。智能手机的定位与品牌决定了最终的市场，如果我们把市场按价格来分，大致可以以1000为一档（实际还有一个1000以下的），2000为一档，3000为一档。3000～5000甚至更高，已经完全是苹果，三星，HTC，诺基亚等品牌的天下，国产品牌只有魅族，OPPO勉强各有一个2999的机型。于是国内手机厂商就给自己定义了两个战场，一个是2000左右的中端战场，另一个1000左右的低端战场，不是不想定高价，而是一步错，步步错，国产手机最早开始就是从中低端开始，于是这个帽子就一直扣在那里了，好比大宝哪天弄一护肤品卖兰蔻的价格，没有人接招啊。其实从技术，还有质量的角度看，我完全相信国产的和这些卖3000的品牌没有区别，然而一旦你在人的心理留下了那个大宝的印记，就很难被遗忘。这也是为什么中兴去年非得弄一个“牛逼啊”（Nubia）品牌出来，要和中兴手机划清界限，可惜想法很好，定位高端，现实好像有点骨感。2013年我真心希望国产品牌能杀出重围在中高端上有突破，有人说，中低端活得好好的，费那劲干嘛，大家不要忘了2000年后山寨机是如何失败的。你不知道哪天三星们突然大降价，中低端型号亏本卖，再来剿灭一次国产厂商。 总结-颠覆性创新与持续创新苹果最早的iPhone在我看来智能手机的是一次颠覆性创新，无论是交互技术还是App Store，之后这几年智能手机都是在走持续性创新的路子。下一次颠覆性创新又是什么呢？可穿戴设备？或者弄个硬件App store模式，手机成为了物连网的中介，将人与其他物连接起来？盼望着，下一次颠覆性创新早日到来。 由于本人非手机行业从业人士，以上只是个人的臆想，欢迎拍砖！]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[企业成长课程笔记（2）]]></title>
      <url>%2F2014%2F03%2F04%2F2013-02-10-grow_to_greatness_smart_growth_for_private_businesses_2%2F</url>
      <content type="text"><![CDATA[在决定成长之前我们需要评估成长的各种可能（alternatives），评估没有在选择的优势与劣势。之后我们再从strategy focus,scaling,financial controls,complexity of growth等方面来进行评估。例如我们应该专注什么产品，哪一类客户？我们能承受的成长，支出？成长，扩张，财务风险，质量风险等之间的关系？小步测试各种可能？花时间评估各种可能，好处，坏处…？ 关键在于： 成长要求更多的人力，过程与控制 thinking about growth strtegically requires disciplined,focused thinking 利用Growth Risk Tool来评估 成长要求制造或生产规模，销售，服务，分销，人力，会计等的扩张 成长是在获得收入前支出 成长需要你考虑成长的优先顺序，流程实施的顺序 成长可以分阶段进行]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[企业成长课程笔记（3）]]></title>
      <url>%2F2014%2F03%2F04%2F2013-03-03-grow_to_greatness_smart_growth_for_private_businesses_3%2F</url>
      <content type="text"><![CDATA[CEO Quotes Begin with the end in mind. Give yourself an afternoon weekly to think about fine critical things going in your business and make sure you are focused on big opportunities or big problems. Don’t get involved in all brush fires-focus on the thing with the biggest impact. I do not pay you to get everything done everyday.I pay you to get the most important things done. Focus-be two inches wide, two miles deep. For me ,the most difficult part is figuring out what not to do. The more we focused and said no, the more we grew. I do not want a single point of failure. I’v got a philosophy that which is not audited will always get worse,never better and will create surprises. The management challenge is when to let up on the (growth) gas pdeal and let people and processes catch up to the growth. 成长的考虑： 计划 思考组织结构，公司基础设施在更高一级会如何？ 组织结构：组织结构图，人员？要记住一个经理通常只能管理7～9人 基础设施：办公室，设备，支持技术… 是否外包？ 工资发放 福利 生产 销售 交付 优先级 战略层面 我卖什么？What 卖给谁 Who 为什么他们会买 Why 每天 每天关键决策 评估情况，处理你有最大影响的 如何知道当前状态？ 瓶颈？ set up 3 or 4 priorities that take precedence over everything else:Manage cash flow;focus on customer and quality service;accelerate revenue growth;and all the rest-unless something is on fire-can wait. 每日集会（Riz Carlton Hotels） KPI全员可见 流程 什么是流程 Checklist: 飞行员起飞前的检查 Instructions：指导你的菜谱 为什么要流程 To ensure: 99% defect-free, on-time delivery to the customer 成长步伐 Right people and right process in place 跑之前先走 清晰的汇报结构，责任人 人力流程 招聘 面试 入职 评估与解聘]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[企业成长课程笔记（4）]]></title>
      <url>%2F2014%2F03%2F04%2F2013-04-01-grow_to_greatness_smart_growth_for_private_businesses_4%2F</url>
      <content type="text"><![CDATA[成长的四种方式前面已经学习到成长的烦恼： 成长可能是好的也可能是坏的-看情况;-) 成长不是总是更好，因为更大意味着管理企业更复杂 持续改进最关键 成长不是想当然，它应该是一个战略决策，需要同时考虑成长与不成长的优势，劣势 成长将给人员，流程，控制带来压力。影响企业文化，客户价值主张，带来各种风险。合适的步伐是必要的 成长要求战略聚集：产品与客户 成长将把你推入不同的竞争空间，游戏将改变 那么讨论了这么久的Growth（成长），它到底意味着什么？ 更大客户？ 销售更多产品/服务给现有客户？ 更多收入？ 更多利润？ 更多资产？ 更多员工？ Growth有不同的意义，但是Growth的方式却只有4种： 改进：更快，更好，更便宜 创新（Innovations）：Doing something really new or different for you that drives revenue and/or profit growth 规模控制（Scaling）：卖更多东西给现有客户或者卖给更多客户 战略收购(strategic acquisition): 买别人的产品/服务/客户或者别人开发新产品/服务的能力 改进（Improvements）持续改进实乃伟大企业之魂。你可以： 改进产品-更好满足客户 更快生产产品 提高客户服务 缩短销售周期（improve sales cycle） 回款更快（collect cash faster） 让与你做生意一切更容易更简单（make doing business with you easier and “smoother”） 更高效运营，降低成本，提供利润 改进流程以提高生产率 改进所有流程，质量，收入，客户… 改进一切 那么究竟如何来实现这些提高呢？很重要的一点就是小规模试验新东西，然后学习，学习=金钱。实际这也是Lean Startup的概念 Idea-&gt;build）-&gt;product-&gt;(measure)-&gt;data-&gt;(learn)。这里两个循环一个是IPD，一个是BML。即想法，产品与数据，构建，度量，学习两个循环 创新（Innovations）这里的创新不一定是要发明什么前所未有的东西，而是： 可以是以新方式把现有的组件祝贺起来（combining existing things in a new way） 从其他行业学习，然后应用到你的行业 不断的问“为什么”来挑战通常应该这样做的假设。多问为什么我们这个行业要这样做？ 向竞争者学习（沃尔玛总是让高管去看别的超市的运作方式） 同客户一起创新（customer co-creation） lean-small,cheap,fast experiments that test new ideas and ways of doing 规模扩张（Scaling）规模扩张是高速成长的加速计，但它需要你之前做好了planning, priorization,process,pace。 选择外包的原因？更快扩张？更便宜？还是可以让你专注最重要的事情？不外包的原因？质量控制？财务控制？客户？ 扩张无外乎两种，要么卖更多产品/服务给现有客户，要么把现有产品/服务卖给更多客户。因此我们可以检查： 我可以卖给现有客户哪些附加产品/服务？ 我有能力为我们的客户提供哪些新产品/服务？ 是否可以为不同客户群体提供一个更便宜的版本，包含更少功能？ 如何增加购买频率？会员制？ 是否可以有新的分销渠道？网络？ 可以捆绑产品或者服务吗？ 定价方式？ 战略收购战略收购需要的完全是不同的技能：尽职调查，估价，收购整合技能等。通常这也是高风险的。那么我们可以买到什么呢？地区上的扩张，客户，新产品/服务，能力？只有当你已经用完了前面规模扩张的所有可能后，仍想扩张时你才会考虑到战略收购。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[软件定价(1)-前言]]></title>
      <url>%2F2014%2F03%2F04%2F2013-06-03-software-pricing-1%2F</url>
      <content type="text"><![CDATA[关于版权本书的翻译采用CC协议，转载请标明出处http://www.zjdian.com, 翻译错误请联系微博行走的家园提出您的宝贵意见 不只是摇骰子实用软件定价精简指南 ##作者序 在2007年商业软件年会上Michael Pryor主持了一个关于软件定价的重要会议。该会议有太多的人出席，大量的人员不断涌入，以致于在所有人完成了自我介绍后已经没有时间来谈论软件定价了。关于大家对软件定价的关注，我也有着类似的经历；实际上“如何为我的软件定价？”可能是软件企业家，产品经理问我最多的问题。 这本指南的目的就是回答这一问题。 首先我需要感谢Phil Factor，Tony Davis以及Michael Pryor对本书的编辑，审阅以及建议。还有很多我无法提及的人对本书提供了帮助，他们提供了与定价相关的轶事，为本书提供了校订。正是有了他们的帮助，本书才能比之前更加完善。谢谢你们！ Neil Davidson剑桥（英国），2009年8月 TwitterBlog ##前言：产品定价 1938年，2个年轻的工程师打算开发一款他们自己的产品。他们纠结于应该建造一个什么产品。在考虑了放大器，无线电设备，航空控制器，口琴甚至帮助家庭妇女锻炼肌肉的电极后，最终他们决定开发一款示波器。为了不让顾客对产品的1.0版本感到不安，他们明智的把这款示波器命名为200A型号。 接下来呢？就是定价。 他们最终给产品定价54.40美元。这是否是因为它代表了产品制造成本再加上合理的利润？事实并非如此，这些工程师完全没有考虑这些因素。实际上，他们很快就意识到了示波器的生产成本比他们的要价要高。那么这一定价是否是基于竞争对手的定价？也不是。他们甚至没有花心思去了解通用无线对类似型号产品的定价是400美元。 他们选择54.40美元只是因为这让他们想起1844年在西北太平洋地区建立美国北部边境运动的口号（”54” 40’ or Fight!”）。 这是多么愚蠢的产品定价方式！ 然而这两个年轻人从失败中爬了起来，200A模型成为了整个时代销售时间最长的基本电子产品，33年后仍在销售。他们建立的公司成为了知名企业。他们的名字？他们是比尔.休利特和戴维.帕卡德（HP创始人）。 如果休利特和帕卡德这两个有着美好前程的斯坦福毕业生在产品定价时都可以犯如此重大的错误，那么我们又能报何希望呢？ 正像HP的结果，机会很大！]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[软件定价(2)-一点经济学]]></title>
      <url>%2F2014%2F03%2F04%2F2013-06-05-software-pricing-2%2F</url>
      <content type="text"><![CDATA[关于版权本书的翻译采用CC协议，转载请标明出处http://www.zjdian.com, 翻译错误请联系微博行走的家园提出您的宝贵意见 第一章 一点——但不是很多——经济学 为了理解产品定价，懂点经济学很有帮助。最容易的方式是通过一个简单例子来说明。 假设你现在发布了Time Tracker 3000产品。这是一可下载的软件产品，它可以记录你一天里使用不同应用的时间，然后把使用信息发到一个统一网站。从这个网站你可以了解你一天的应用使用情况。假设你已经决定了对这个软件进行一次性收费，尽管在本书我们还会讨论网络应用，社交网络以及其它定价模型，但是目前为止我们还是让问题简单些。 你会如何对你的产品定价？ 如果你决定免费，那么你会有很多的用户。现在让我们假设你有1000个用户，这里面包括到处找便宜货的Belinda，学生Stewart，网络初创公司创始人Willhelm，产品经理Pat和企业开发人员Ernest。 让我们用一个无限细（代表0美元价格），1000个单位长（代表数量）的水平条来表示这1000个免费用户： 如果你决定不免费，软件定价为100美元，那么愿意购买的用户将急剧下降。Belinda是喜欢便宜货的人，他使用这一软件只是因为它免费，Stew是一个学生，所以他们俩都不会购买。为了简单起见，我们假设你会获得500个用户而不是开始的1000个用户。让我们再用一个100美元高，500单位长的条形面积来代表这一结果。那么你的收入是多少呢？它就是条形的面积，100美元x500=50,000美元。 现在让我们和第一个条形叠加在一起： 如果你把价格提高到200美元又会如何？一些在100美元会购买的用户不再购买，但是仍然会有些人购买。Willhelm经营自己的公司，他认为这一价格不值得购买，所以他不再对软件感兴趣。现在让我们假设仍然有300人会购买，我们再次用一个矩形来代表它，并与第一个图叠加。那么你从300个购买这一软件的用户获得的收入就是矩形的面积，200美元x300=60,000美元： 现在让我们把价格增加到500美元。此时，更少人会购买你的产品。产品经理Pat不再购买，因为此时她宁愿从你的竞争手购买。假设有50人最终会购买，再次用矩形表示，叠加到同一图上。价值就是矩形面积：50人以500美元价格购买，50x500美元=25,000美元 最终尽管Time Tracker 3000有些价值，但它不是无价的，我们假设定价在1000美元时会无人购买，同时我们还是用一个宽带为0的条形来在图上代表它: 至此为止我们已经在图上画了5个点，它们构成了对应特定价格购买Time Tracker 3000用户数的价格曲线。更进一步，你还可以通过计算对应该点的矩形面积获得在特定价格时的总收入（价格x购买人数）： 经济学家把这条曲线命名为需求曲线。 为了最大化Time Tracker 3000的收入，我们需要在图上找到一个点，在该点之下的矩形面积最大。为了说明这一点，有必要画一条面积（比如总收入）相对价格的曲线。对Time Tracker 3000而言，图形如下。（我已经把之前得到的5个数据点在图上标出）： 从图中，你可以看出你应该将Time Tracker 3000定价在300美元左右。这个价格不是你销量最大的位置，但是你却能收入最大化。 需求曲线理论上是非常直接的，但是实际却很困难。在真实世界中，你无法知道需求曲线的形状，或者是你的当前价格在哪个位置。 在某些曲线以及曲线上的某些点，提高价格是正确的行为：涨价带来的每人收入增加超过了购买产品人数减少的损失。而在另一些曲线，或者同一需求曲线的其它点上，涨价会导致销售额的大规模减少，你的收入会减少。 更进一步，需求曲线的形状是动态的，它取决于多方因素，包括了你的竞争对手他们如何应对价格变化，你的客户有多少钱打算花在这上面以及你的产品质量。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[软件定价(3)-定价心理学之你的产品价值]]></title>
      <url>%2F2014%2F03%2F04%2F2013-06-07-software-pricing-3%2F</url>
      <content type="text"><![CDATA[关于版权本书的翻译采用CC协议，转载请标明出处http://www.zjdian.com, 翻译错误请联系微博行走的家园提出您的宝贵意见 第二章 定价心理学：你的产品价值前一章讨论的需求曲线可能是动态的，取决于多方因素，但是你仍然可以对影响它的形状。本章中，我们将讨论人们如何决定为某个产品付多少钱以及你如何改变它。 在开始前首先你需要回答一个简单的问题：什么是你的产品？ 你的产品是什么？你可能错误的认为你的软件产品只是客户下载的一个个字节与位。实际上你的产品的定义比它更广泛。它不仅是软件——它还是产品文档，产品运行需要的帮助和出现问题时提供的支持。它还是继续开发新版本的承诺，未来的产品路线图。在某些情况下，它还是一个梦想；一种生活方式。 在会计行业的一个典型例子就是，在Red Gate公司，我们使用Sage公司的会计软件。我们公司并不是唯一的一个：Sage公司是一家有580万客户的软件企业，它雇佣了14，500名雇员，市值接近50亿美元。它在英国的会计行业占主导地位，不时受到来自微软和Intuit以及其它公司的联合攻击。 然而其它公司的软件都失败了。 Sage的软件又慢又难用。当我1999年开始使用时，甚至工具栏上的按钮在点击时也不会有点击的效果；它们只是一个灰色背景上的静态图片。这一应用是如此的差，以致于在Sage的网站上几乎都不太提及这一产品。 如果你理解了Sage的产品不仅仅是软件，你就可以理解糟糕的产品，极大的成功这两个对立的事实。 只要你买了Sage的软件，你买的就不只是软件了。你买到了保证：在税法变动时，软件也会更新。你买到了对产品的熟悉：如果你买的Sage软件，很可能你的会计已经会使用它了。你买到的是支持：如果你不理解其中的一个会计科目或者流程，你可以打电话给Sage的支持人员寻求帮助。要知道，每天有超过4万人打进Sage的支持热线。 Sage软件在英国占据垄断地位是因为他们理解什么是他们的产品。你也一样需要理解什么是你的产品。 感知价值 一旦你决定了你的产品是什么，你就需要考虑它对客户的价值。以Time Tracker 3000而言，我们假设它可以为Willhelm这个特定客户节约3小时的工作，而Willhelm将他的时间定价为50美元每小时。这意味着如果我们假设Willhelm没有其他更好的消费，只要Time Tracker 3000的价格在150美元以下，他就应该购买。 当然这要求Willhelm是经济学家，是一台喜爱理性决策的机器才行。实际上Willhelm是一个有血有肉的非理性人类，他不会为他的时间定价并计算成本收益。他有一个对Time Tracker 3000的感知价值，这一价值可能会也不会与产品的客观价值一致。 感知价值也可能高于产品的客观价值。2003年，高德纳咨询公司的报告指出几乎有一半的客户关系管理系统（CRM）无人使用。这是那些聪明人认为值几十亿美元的软件，实际上却非如此。 彩票也是另一个感知价值高于客观价值的例子。购买一张5美元的彩票，基于概率统计长期来看你只能获得3美元的回报。但是仍然有上百万的人购买彩票。 一个产品的感知价值也可能会比其客观价值低。几年前，我碰到一个坚持要把Excel表格处理软件当做Word文档处理软件来用的人。这个用户认为，购买微软Word软件的额外花费比他获得的好处小。这当然只是一个感觉而不是现实。但这也是在通常情况的极端案例。在Red Gate公司，我偶尔也会碰到有些人想购买我们的软件，但是却无法向他们的老板证明其价值。感觉上工具带来的对某人时间的节约值不了这个价（几百美元），而现实则是在工具上的支出几周就回收了成本。 回到Willhelm和Time Tracker 3000的例子，如果你想改变Willhelm为你的产品所付的价格，改变你的产品是一个选择，但是这只有你同时也改变他的感知才有效。实际上，如果你还可以改变Willhelm对你的产品价值的感知而无需改变你的产品。这也正是市场营销要做的事情之一。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[软件定价(4)-定价心理学之感知价值]]></title>
      <url>%2F2014%2F03%2F04%2F2013-06-13-software-pricing-4%2F</url>
      <content type="text"><![CDATA[关于版权本书的翻译采用CC协议，转载请标明出处http://www.zjdian.com, 翻译错误请联系微博行走的家园提出您的宝贵意见 人们如何设定他们的感知那么人们如何产生对一个产品的感知价值？他们是如何思考的？ 在完全真空的情况下开始是极其困难的。试着问一个英国国会会员一品脱牛奶值多少钱，让人竞猜抽屉的正确价格，或者普通超市购物者应该为一瓶漂白剂付多少钱。他们的回答会陷入困难。 人们的感知价值总是有一个参考点。如果你卖待办事项列表软件，那么他们会看看其它的待办事项列表软件卖多少钱，如果他们在网上发现你的竞争对手的软件卖100美元，那么他们就会把对所有待办事项列表软件正确价格的感知定在100美元。 在1982年微软发布DOS 1.0的时候，他们将价格定在了50美元。在那时，面向大众计算机的操作系统是一个全新的市场。由于没有参考点，50美元看起来不错，于是它成为了大众接受的’公平‘价格。当IBM在1989年发布OS/2 1.0的时候将其定价为340美元，消费者就完全犹豫是否购买。这不是经济的原因：DOS和OS/2完全是不同的操作系统，340美元可能是对更先进的OS/2操作系统能为用户带来的额外收益的公平反映。但是微软已经定义了操作系统的参考点，当IBM试图改变它时，他们失败了。 这并非意味着你需要就此使用参考点。如果你的产品比竞争对手更好，并且你能展示这一额外价值，创造对该价值的感知，那么你是可以定价更高的。 当然如果你的产品比竞争对手价值低很多，那么你唯一的选择只有选择要价更低。在价格上竞争成为了你唯一的选择。以药品为例，我们本地市场上有30种左右不同类型的止痛药。你可以以1.97镑（3.40美元）的价格买一包16片装的Nurofen或者以0.32镑（大约50美分）的价格买一包16片的Tesco。真实的物理产品-200毫克的异丁苯丙酸-在这个普通品牌和知名品牌产品中含量完全一样。但是不要忘了整个产品不止是化学成分。它含包含市场营销，品牌和包装。用更广义的定义，Nurofen是明星产品，Tesco只能通过价格与之竞争。 人们对你的产品价值的感知还取决于他们的品味。有些人喜欢好酒，他们可以花50美元买一瓶酒，而其他人觉得5美元的酒味道也不错。此外人们属于哪一个族群也会影响他们愿意付多少钱的意愿。正如Dave O’Flynn对我写道： “在我加入Atlassian前，我从来不认为苹果的产品值那么高的溢价。然而12个月后，我很高兴以高额的溢价买了一台Macbook Air，因为我周围都是重视设计和优雅的人。而在那之前我周围的人都是看中物有所值的人，那时我的笔记本是一台普通的AMD，与现在的Macbook Air比似乎有1吨重。我还是我；改变的是预期以及对我周围的价值感知。” 人们有多少钱也决定了他们对价值的感知。Dennis Kozlowski，泰科的前CEO会觉得15，000美元的狗形雨伞架很值，然而大多数人不会这样认为。 知识也会影响人们对产品价值的感知。一台拥有1.4G赫兹主频的双核处理器，4G内存，蓝光驱动器，Ubuntu系统的笔记本对我而言比一台使用N系列安腾处理器，DVD驱动器的笔记本价值要高，然而对我母亲则不是如此。 另外尽管这是一个低价的把戏，但是5和9在人们对价值的感知上有着额外的心理影响。这正如在超市商品里面1.99美元似乎要比2美元便宜很多，在网上1995美元似乎比2000美元要少很多一样。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[软件定价(5)-定价心理学之感知价值(续)]]></title>
      <url>%2F2014%2F03%2F04%2F2013-07-03-software-pricing-5%2F</url>
      <content type="text"><![CDATA[关于版权本书的翻译采用CC协议，转载请标明出处http://www.zjdian.com, 翻译错误请联系微博行走的家园提出您的宝贵意见 提高感知价值医药行业有另一个关于市场营销无需改变产品实质而提高产品感知价值的很好实例。在1981年葛兰素公司想发布他们的治疗溃疡的药品Zantac，他们面临的是史克公司的Tagamet产品所垄断的市场。尽管葛兰素公司认为他们的药比史克公司的更有效，美国食品与药物管理局将Zantc评价为与其他治疗方式相比疗效几乎没有区别。为了不把Zantac定位为一个与Tagemet类似的模仿产品，葛兰素决定对他们的销售和市场渠道大量投入。这一独特的促销增加了Zantac的感知价值，葛兰素成功的将产品定价更高以反映这一增加的价值。到了80年代末它已经打败了Tagamet，成为了全球最畅销的药。 以下是更多的一些增加你的产品感知价值的方式： 提高产品客观价值。感知价值和客观价值不一定完全一致，但他们是相关的。引用Joel Spolsky在2006年所说1： “借这六年运营自己的软件公司的经验，我可以告诉你，除了发布具有新功能的新版本外，我们在Fog Creek没用使用任何其他手段，去试图增加我们的收入。决没有。我们发布的新版本绝对是相当出色的。就像地心引力一样。我们试图利用Google广告时，我们植入多种附属产品时，或者当关于FogBugz的文章出现在媒体上时，我们几乎看不见任何影响。而当发布带有新功能的新版本时，我们看到了收入出现了突然的、相当不错的、持续的、固定的增长。” 给你的产品个性。37signals的项目管理软件可能不是最好的，但它有个性，37signals代表了：极致的简单。想要更多功能？很难。如果需要更多功能，买别的产品。 将产品与你关联，然后将你定义，宣传成为一个专家。在诺顿被赛门铁克收购之前的早期，彼得.诺顿即是诺顿。所有的诺顿产品都有一个他自己交叉抱着手臂的图片。 让人们爱上你的产品。当Black&amp;Decker引入他们的DeWALT钻头生产线时，他们在午餐时间到建筑工地和木料场分发手撕猪肉三明治，进行产品演示，举行有奖钻速比赛。他们到他们的用户出现的全美汽车比赛协会的比赛和放牧人竞技比赛上展示。他们让人们不仅喜欢他们的产品，同时也让人们喜欢上这一品牌。现在DeWALT钻头也拥有大量的的业余爱好者；人们想把自己与专业联系起来。尽管如著名谚语所说，人们买的是钻头，而不是孔。400美元的钻头！ 提供更好的服务。人们购买软件，希望软件能正确运行，在软件出问题时你能在旁边。如果你是一个有大个竞争对手的小公司，这是你可以比他们做得更好的地方。利用好这一点！ 通过你的信誉来提供保障。起初，品牌是建立信任的机制。回溯到1880年代，你买的下一块象牙皂保证和之前的一样。最近,早期的PC模仿者都受困于人们“没人因为从IBM购买而被解雇”心理。 建立一个群体。产品可以成为一个归属的符号。如果你可以把你的产品变成一个人们可以用来表明他们是谁的徽章，他们属于哪个群体，不属于哪个群体，那将是无价的。业余的自己制作爱好者完全不需要花400美元去买DeWALT钻头，但是他们喜欢属于‘专业’群体的感觉。 提醒人们你在你的产品上花了多少精力。人们更愿意为花了几年时间开发的软件付钱而不是一个简单抄袭的软件产品。1996年20岁的比尔.盖茨在组装电脑俱乐部的那封现在已经很出名的‘致爱好者的公开信’中就用了这一技巧： “一年以前，我和保罗艾伦很高兴地看到业余电脑爱好者市场迅速壮大，我们雇佣了Monte Davidoff一同开发了ALTIR BASIC程序。尽管最初的工作只用了两个月时间，但我们三人几乎用了去年一年的时间编写文档，继续改善BASIC，添加新的特性。现在，我们已经有了4K，8K，更多内存的磁盘BASIC和ROM BASIC。如果计算我们的工作价值，我们已经花费了4万多美元。” 给人们正直的感觉。咖啡店对公平贸易咖啡豆额外收取10美分，他们将自己的口袋与你的道德联系了起来。在你的公平贸易拿铁咖啡中，这10美分中有多少给了种植这0.25盎司咖啡豆的农民？少于一美分。 不只是销售物理产品。最新的宝马广告完全表达了这一点。视频显示着快乐的人们做着开心的事情，画外音“我们是一家汽车公司。但是我们不只是生产汽车。[…]我们很久前就意识到让人们感觉到与你生产什么一样重要。在宝马，我们不仅是生产汽车。我们创造快乐。”快乐值多少钱？明显比汽车价值高。 最后，我们需要区别你的产品。以功能，产品的好处，销售方式，提供的服务，来自哪个国家-以上任何方式都可以。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[企业成长课程笔记（1）]]></title>
      <url>%2F2014%2F03%2F04%2F2013-02-01-grow_to_greatness_smart_growth_for_private_businesses_1%2F</url>
      <content type="text"><![CDATA[成长的真相长期以来很多企业家都认为： 成长总是好的 越大越好 我们的企业“要么成长要么死亡” 然而这当中却并没有什么科学研究来证明这些关于成长的“神话”。最近新东方的俞敏洪也在说后悔公司扩张太快。所以关于成长的真相应该是成长可能是好的也可能是坏的。没有准备的过度成长将压垮你的人员，流程以及控制。 成长的真相： 质量控制的压力 财务控制的压力 影响原来的价值主张(customer value proposition) 文化的影响 进入不同的竞争空间（当你小的时候没人在意，现在巨头们开始关注了） 大不一定好。越大，官僚和复杂将随之而来，这可能超过了现有管理能力和经验 “成长或者死亡”并非正确。收入的增长与成本增长要匹配 也许“改进或者死亡”更好 在公司成长过程中不可避免的我们会碰到各种挑战。首先人员，控制，你的人员的成长能否跟上公司成长？是否有足够的控制，财务，质量，客户服务…？技术更新，物流，竞争等等都可能带来新的挑战。 真实情况是很少有持续的超越平均值的成长！成长要求你有： 战略聚集（strategic focus） 卓越的运营（operational excellence） 持续改进（constant improvement） 客户为中心（customer centricy） 员工高度敬业（high employee engagement） 生物学告诉我们某些物种刻意限制自己的种群增长以提高他们的存活率，因为过度的成长会增加被捕食的风险。许多生物体具有有限的能量在再生，成长，维护和生存之间分配，所以，我们不要试图去打破这一平衡，正如汽车油门不能一直踩着，我们要学会适时松油门。成长应该是评估了优点和缺点以及成长与不成长的风险之后的决定，因为成长对人员，过程，控制的要求更高。 在成长之前不妨看看： 为什么我们要成长？ 如何成长？ 成长多少？ 我们能承受多少成长？ 是否有足够的人？ 是否有正确的人？ 招聘与培训的流程？ 足够的财务控制？ 足够的质量控制？ 成长带来的风险？ 文化？ 客户服务？ 客户体验？ 现金流？ 供应链，原材料，存货？ 分销与交付？ 财务安全？ 如何降低这些风险？ 有无足够的每日信息来监控这些风险？ 谁来监控，管理，纠正这些风险？ 是否需要控制成长速度？]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[软件定价(6)-定价心理学之产品路标]]></title>
      <url>%2F2014%2F03%2F04%2F2013-07-08-software-pricing-6%2F</url>
      <content type="text"><![CDATA[关于版权本书的翻译采用CC协议，转载请标明出处http://www.zjdian.com, 翻译错误请联系微博行走的家园提出您的宝贵意见 ##路标 现在你知道了客户会把你的产品价格与一个参考点比较，你需要尽全力让他们与对你有利的参考点比较而不是不利的参考点。如果你想在市场价格是100美元时，销售200美元的待办事项列表软件，那么你需要增加一些新功能让你的客户无法进行直接的比较，之后你促使他们与那些卖300美元的效率提升软件比较，而不是与待办事项列表软件比较。同时，你需要避免任何与开源软件的比较。 如果你的客户无法为你的产品找到任何参考点，那么他们会寻找近似值或者路标。超市经常利用这一点：消费者决定是否昂贵的冰淇淋（他们不经常买的东西）价格是否合理是基于日常的可乐（他们经常买的东西）的价格。如果超市的一听可乐卖2美元，那么消费者会觉得其它产品都太贵了。 假设你销售两种产品：Time Tracker 3000和Task List 400，一个待办事项列表应用。当人们有事情需要完成时，他们把它记录在Task List 400软件中，之后他们可以对这些任务排优先顺序，分解任务到子任务中，跟踪任务进度，最终自豪的标记任务已经完成。 我们假定Time Tracker 3000没有任何竞争对手，而Task List 400有着很多。你的客户将通过你对Task List 400的定价来判断Time Tracker 3000的价格。为你的待办事项列表应用定价为合理的25美元，同时你的客户会相信你的话，那么300美元的定价对Time Tracker 3000也是一个合理价格。如果为第一个应用定价为1000美元，那么他们会认为你在新产品上敲竹杠。 如果你的产品独一无二，客户无法找到参考点或者路标，那么你就有机会设定客户预期，定义他们的感知。如果你告诉你的客户Time Tracker 3000值300美元，那么他们很可能会相信你。我们在微软为第一版的MS-DOS定价上已经看到了这一点。 如果你在市场上有竞争对手，那么你的客户对成本有清醒认识。但是如果你的产品建立了一个新品类，那么早期采纳者通常对价格不敏感。如果你可以开发一个全新品类的产品-传送点，它可以把你毫发无损，快乐的从纽约传送到巴黎，那么你不仅可以定义自己的价格，甚至你还可以从20，000美元涨价到25，000美元，人们仍然会购买它。如果你制造汽车，一个已有品类中的新产品，把价格从20，000美元涨到25，000美元那么你的销售收入将损失。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[软件定价(8)-定价陷阱之转换成本]]></title>
      <url>%2F2014%2F03%2F04%2F2013-07-10-software-pricing-8%2F</url>
      <content type="text"><![CDATA[关于版权本书的翻译采用CC协议，转载请标明出处http://www.zjdian.com, 翻译错误请联系微博行走的家园提出您的宝贵意见 ##转换成本 如果你试图说服人们从竞争对手的产品转换到你的产品，那么你需要确定客户面临的转换成本。 假如你试图说服客户从他破旧的500美元字处理应用转向一个更优秀的100美元产品上。首先你需要为经济转换成本定价。将文件转换成新格式，学习新的菜单布局需要花费客户的时间，这些最终都是金钱。 其次，你还需要克服心里转换成本。人们总是对已经拥有的定价过高，而对还未拥有的定价过低。你会为我上图的三个巴沙木做的企鹅估价多少？他们花了我10美元（从奥斯陆的卡通刻展会上购得）。在意提高你的价格吗？不要这样认为，我100美元也不会卖。我敢打赌你家里面也一定摆满了这样的东西。 另一个有力的心理因素就是人们很难克服对已经花掉的钱的情感归属。理性上看，钱已经花了。这已成为沉没成本。你的客户不应该再想着他花在那个破旧的字处理器上的500美元。可是，他就是这样。 为了明白这一点，让我们用一组学生的案例来说明。假设他们意外的买了同一个周末的一张50美元滑雪票和一张100美元的滑雪票，50美元的那个滑雪之旅会更有意思。他们会选择哪一个旅行，而又放弃哪一个旅行？理性告诉他们需要选择便宜的那个，但实际上超过一半的会选择没有那么有趣，但更贵的那个旅行。 为了降低转换成本，你可以采取一些措施让它们变得有利于你。以下是一些例子。Open Office是一个包含开源字处理和表格应用的软件，它支持打开微软Word格式的文件。早期的微软Word不仅可以打开WordPerfect文件，还在帮助里面有一个用户专区，甚至允许用户使用WordPerfect快捷键。 这里还有另一个例子。如果你决定在90天的免费试用期后停止使用FogBugz软件，那么Fog Creek公司会退回你花的钱。 该策略有两个影响：首先，它减少了转换到Word和Fogbugz的心理和经济影响。其次，一旦你已经转换，那么你就已经花了时间精力来使用新的软件，此时你会有新的转换成本，这将阻止你退回去。 ##你应该考虑自己的成本吗？ 很显然你不能把你的软件定价比单个生产成本低。这是你的边际成本。你可能觉得这些成本为零，实际上它们不是。 你需要找到潜在客户，说服他们购买。如果你有销售团队，你需要付销售佣金给他们。支持客户需要花钱，争取不花钱的客户也要花钱。 如果你是依赖于面对面的集中销售模式，那么你的销售成本明显比低接触的网络销售模式高。但是两种情况下你都需要考虑成本。 如果你计划对大部分用户不收费，那么仔细考虑每一个额外用户的成本。如果你觉得成本为零，那么你绝对错了。如果你是运营一个网站，那么每一个额外用户需要花费你的存储空间，CPU时间，带宽等。这个成本可能很低-也许就几分之一美分-但是如果你有大量用户那么一点成本乘以大量用户也会是一笔大开销。 以YouTube为例，它提供免费服务，理论上可以通过广告来支撑。支持一个额外的视频的成本极低（大约0.1美分），但是在2009年它支持了大约750亿个视频。将这个极低的成本与极大的量级相乘，你就可以理解为何谷歌每年要花费7.1亿美元来运营它了。它完全不能通过广告收入来支撑其花费。 Paperback软件公司是另一个错误理解软件销售的例子，它无法承担其成本，最终导致了灾难。当Adam Osborne在1984年成立Paperback软件公司时，它是在软件成本太高的前提下建立的。 他们在1986年以99.95美元的价格发布了VP-Planner，在市场上直接与500美元的Lotus 123竞争。在80年代时，大多数软件都是通过经销商销售。经销商从卖出的每一个软件中获得佣金，所以他们不喜欢低成本，低边际利润的VP-Planner，于是他们说它的坏话，鼓励人们购买高成本，高利润的Lotus 123. 更糟的是由于VP-Planner和Lotus 123完全一样，客户也要求这个便宜软件和昂贵的Lotus 123一样的支持，这严重影响了Paperback 软件的利润。Paperback公司成功的从Lotus手中获得了市场份额，但是却无法有足够的钱来应对Lotus发起的诉讼。 如果你的客户愿意付的钱低于你的软件销售成本，那么你就没有业务，产品也会失败。你需要降低销售成本，或者改变定价模式使得客户在产品生命周期中付更多的钱。 当松下1994年发布了游戏机3DO时，时代杂志把它提名为了年度产品。32位RISC处理器，定制的数学协处理器，2M内存，这远远超越了它的时代。但是松下将产品定价为699美元，这远远高于竞争对手的价格和目标市场的早期采纳者的承受能力。高价格与混乱的市场营销导致了3DO的彻底失败。 其它的游戏设备厂商吸取了这个教训。当PS3和Xbox 360发布时，生产成本比市场能接受的销售价格高，于是索尼和微软定了低价，接受了他们在每台游戏机上会亏钱的事实（多达300美元）。然后他们从消费者购买游戏的版税中进行补偿。真实的游戏机的价格被隐藏了起来；一个聪明的定价模型掩盖了它。 任天堂的Wii又采取了一个不同的方式。他们想面向一个更广阔的市场，而不是他们的竞争对手所面向的18-35岁男性这一最佳位置，他们意识到老人，家庭妇女愿意在游戏机上花的钱要比游戏迷少。于是他们降低了生产成本，使用更便宜，更慢的部件。当Wii在2006年9月发布是，任天堂卖出的每一个游戏机都是盈利的。这使得游戏生产成本也更低，尽管版税可以更低，但是购买游戏不一定便宜。为什么？到现在，对你来说答案已经很明显了。 你可能注意到了我有一点没有提到，那就是产品开发成本。到目前为止我讨论了边际成本-生产，销售每一份软件的成本。你的一次性成本是不同的。你可能花了几百或者几百万来开发你的产品，但是这个钱已经花了。用掉了。这是沉没成本。重要的不是你花了多少钱，而是人们愿意付多少钱。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[软件定价(10)-定价进阶之定价模式]]></title>
      <url>%2F2014%2F03%2F04%2F2013-07-11-software-pricing-10%2F</url>
      <content type="text"><![CDATA[关于版权本书的翻译采用CC协议，转载请标明出处http://www.zjdian.com, 翻译错误请联系微博行走的家园提出您的宝贵意见 ##不同定价方式 你的产品应该收费多少并不是你需要做的唯一决策。你还需要决定如何收费。收费模式有很多： 订阅：大多数SaaS（软件即服务）公司使用这一模式。如任何模式一样，这有好处也有坏处。好处很明显包含，一次性预付费较少。但在除了明显的持续收入外，这一模式还有其它意外好处： 付多次小钱心理上要比一次付很多更容易接受。这也是为什么人们用信用卡买车，然后以20%利率还款，或者是假期一周花的钱用25年的抵押贷款来还。尽管要付的总数实际更多，但让人感觉却更少。 如果你的销售面向企业，那么你的最终用户向他们的老板解释一小笔，定期支出要比解释一次性大笔支出更容易。 持续的付款将推动经常使用。以健身俱乐部为例，那些一次性付年费的人，会在他们的大笔支出的头几个星期里频繁使用健身俱乐部，然后就不再来了。而那些按季度付款用户的使用模式则是锯齿形的，在付款后有一个高峰期，之后降低直到下一次付费。每月付费的用户则显示了平稳同时更高的使用率。最重要的是，由于他们是经常性用户，所以他们更可能为会员费续约，保持会员的时间也更长。 我们已经讨论过免费增值模式，这当中一小部分付费用户补偿大量免费用户。吉列模式则是对免费增值模式的小调整。吉列以将他们的产品分成两部分出售而出名：刀架和刀片。刀架很便宜，但是他们通过刀片获利。这一战略相当普遍。 Adobe公司对Acrobat软件采用了类似的战略。阅读文档免费，但创建文档则需要付费。惠普在打印机上亏钱，但是在墨盒上赚钱。第一款福特Fiestas亏本销售，但福特在配件和汽车金融上赚钱。微软和索尼在售出的每一个X-Box和PlayStation上亏钱，但在游戏版权上赚钱。 每用户定价的方式有很多种。通常方式包括按每个指定用户许可证或者同时使用用户模式。在Red Gate公司，我们使用每用户许可证模式。如果你有一组10人，所有的人都想用我们的软件，那么你需要买10个用户许可证。如果你无法统计总用户数，或者只有一些人会同时使用，那么按照同时使用用户定价就有意义。这一模式通常在服务器软件，比如数据库中使用。 另一常用许可证模式是每处理器或者每处理器核心模式。这一模式的典型问题是处理器会很快变得越来越快，拥有更多核。如果说你的缺陷跟踪系统绑定于客户的硬件能力，那么按照摩尔定律每两年他们从你的软件获得的好处将翻倍，而无需付一分钱。 每物理服务器/虚拟服务器许可证模式与每处理器模式有着相同缺点。更多的处理器被塞进了服务器中，客户可能以固定成本获得指数性增长的收益。 按照使用收费模式根据用户使用你的软件多频繁来收费。这可以是根据存储了多少兆字节，处理了多少交易，传输了多少G字节或者很多其它方式来决定。这在过去比其它模式使用相对较少，但在云计算发展起来后将变得越来越普遍，人们也希望按照按需使用付费。这一模式的一个不利之处是由于人们不清楚一次需要花费多少，所以他们不愿意购买。 向最终用户收费不是唯一的软件定价方式。你可以免费提供软件，而通过例如咨询，安装和培训或者销售广告赚钱。不过广告虽然是网站的一个通用模式，但实际很难赚钱。CPM-每千次展示成本-可能低至1美元。换句话说，要产生1000美元收入，你需要1百万次页面展示。要支撑3到4人的一个团队，这意味着需要有需要每月千万次的PV。大多数网络应用无法吸引这么高的流量。 给你的客户选择许可证模式的机会是有意义的。例如，如果你购买微软的SQL Server 2008，你可以选择每处理器许可证，也可以购买服务器许可证，然后按照客户端连接数收费。第一种模式价格5，999美元/处理器。第二种情况下服务器端需要付885美元，之后每个额外用户访问数据库需要付费162美元。 许多企业最终采用混合模式。例如Red Gate公司组合了一次性付费和每年10%-15%的支持和升级费模式。这种模式，我们既获得了一次性收入也有每年持续收入。 然而如果你选择这样做，你需要小心其中的陷阱。支持和升级费不只是一种获得收入的廉价方式，它也迫使你在即使不是适合你，你的客户或者产品的时间为了发布软件而发布软件。如果你想定期向客户收费，那么你需要让他们定期获得或者感知到价值。 在2001年发布了Windows XP后，微软引入了‘软件保证’计划。以年费的方式，微软保证企业客户可以升级到下一个操作系统版本。理论上，每一方都会获利：微软获得了有保证的收入来支持未来开发，客户摊薄了成本，以更便宜的价格升级到微软2003年将发布的代号Longhorn的新操作系统。但是Longhorn没有在2003年发布，2004年也没有，2005年还是没有。直到2006年底它才上市，被阉割成了Windows Vista。而大多数企业拒绝升级。 ##选择正确模式 在选择你的定价模式时，这里有两个建议。第一，乏味。第二，以你的客户期望的模式给予软件许可授权-适合他们的商业模式。 Red Gate的第一个产品是Aardvark，一个在线缺陷跟踪系统。当我们在2000初发布时，我们决定采用按使用收费模式。我们根据录入到系统的每个缺陷收费。这对我们而言是有意义的，因为我们提供的服务是与客户对它的使用联系在一起的，但这对客户的工作方式或者期望的收费方式则没有意义。这是我们犯的第一个错误。我们犯的第二个错误就是忘记了要乏味，把每个使用单元称为‘一听蠕虫’。我们觉得这很酷。我们的客户却有不同意见，我们很快就转移到了按用户付费。 还有更糟的错误定价模式，在1990年代，Knight-Ridder和MAID合并成立了Dialog公司。它向企业和政府销售数据。用户登录，并在Dialog存储的60亿页的信息中搜索信息。 Dialog决定采用按使用付费模式。用户购买‘DialUnits’，不同的操作花费不同数量的DialUnits，这取决于该操作需要多少资源，访问的数据的价值。想对你的结果排序？这要比只是保持它贵。贵多少？这取决于你搜索的是什么数据库，搜索有多频繁。排序或者去掉重复结果是资源密集的操作，因此需要花费更多的DialUnits。有些操作则是免费。即使在一轮精简后，公司仍然需要4页纸来向客户解释定价模式。 2001年，Dialog公司引入了多种定价计划，希望用户可以选择按使用定价或者按时间定价中更便宜的模式。于是就有了四种不同平台-Dialog 交易,Dialog高级，Dialog企业。再加上折扣方式，多年选择，以及不同用户界面如Dialog经典，Dialog网页，Dialog经典网页，如一个用户所说，就是任意想一个数字，然后将它翻倍，然后加上你的母亲的年龄也比这一定价策略清晰，更好！]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[软件定价(9)-定价进阶]]></title>
      <url>%2F2014%2F03%2F04%2F2013-07-11-software-pricing-9%2F</url>
      <content type="text"><![CDATA[关于版权本书的翻译采用CC协议，转载请标明出处http://www.zjdian.com, 翻译错误请联系微博行走的家园提出您的宝贵意见 #第四章：定价进阶 到目前为止，我们只考虑了销售单个产品。如果有几个产品要销售或者是要销售同一产品的多个版本呢？ ##版本 每一个潜在客户都有一个购买你的产品的心理价位。回到前面的例子，Belinda（买便宜货的人）和Stewart（学生）只会在软件是免费时才使用Time tracker 3000。Willhelm愿意付150美元，Pat最多愿意付400美元。让我们假设Ernest愿意付600美元。 下表是你在每个价格点的收入： 价格 谁会购买 收入 0 美元 每个人 0 美元 150 美元 Willhelm,Pat, Ernest 450 美元（3人以150美元价格） 400 美元 Pat, Ernest 800 美元 （2人以400美元价格） 600 美元 Ernest 600 美元 （只有Ernest） 1000 美元 没有人 0 美元 如果这5人是你的全部目标市场，那么为了最大化你的收入，你对Time Tracker 3000的定价应该是400美元。这是最好的单一价格，但是你将失去来自Willhelm的销售，同时你也会失去Ernest愿意付的额外价格收入。 如果有某种方式可以把我们的产品以每个客户能接受的最高价格出售的话，那么你能获得的软件销售收入就是1150美元。这就是版本要做的事。它是一种将你的用户根据他们付款意愿来分类的机制。你计算你是否可以不同方式对你的客户进行分组，然后看这些组是否愿意为你的产品付不同的价格。 以下是一些具体的实施的方式： 根据功能。比如你可以有‘标准’版和‘高级’版工具。这在软件行业极为普遍。微软的Visual Studio 2008就有5个不同版本：精简版（免费），标准版（299美元），专业版（799美元），团队系统（5，469美元）和团队套装（10，939美元）。这是一个覆盖了所有群体的价格，根据功能不同对应了从没钱的爱好者到绩优企业的开发人员。在Time Tracker 3000例子中，你可以建立一个专业版本让他们可以与其它从事类似工作的人使用不同产品的统计进行比较。 根据可获得性。一些客户可能愿意为更快的得到你的产品付更多钱。精装书就是这方面的一个例子。它们有和普通纸版书一样的内容，只是包装不同，面向那些不想等待的人。对Time Tracker 3000你也可以销售一个额外的订阅服务使客户可以更快获得软件。 根据人群特征。学生比企业钱少，爱好者比专业人士钱少，学校的孩子比婴儿潮时的人钱少。你可以提供一个专供学生的Time Tracker 3000版本，只要他们能证明他们是全职学生。 根据地区。美国的客户会愿意比印度和中国的客户付更多钱。微软为了和来自开源软件的威胁竞争，提供了一个低价的‘起步’版Vista操作系统，只在如印度和墨西哥这样的贫穷国家提供。Time Tracker 3000可能在印度仅以其在美国价格的10%提供，但是由于本地化为印度语，使它对西方国家用户毫无意义。 根据平台。Mac用户可能比Windows用户愿意为软件付更多费用或者与之相反。那么你可以对Time Tracker的Mac版比Windows版定价更高。 当然你也需要注意进行版本区分的风险。你需要保证你为每个版本选择的功能要对你的目标细分客户群有吸引力。例如，如果你为产品引入一个‘精简’版，你想要确信专业用户不会降级去使用该版本。 当你利用这些准则进行版本细分时，如果你的目标是满意的客户，那么你最好要牢记客户对公平的敏锐感觉。Adobe公司试图根据地区来区分不同版本；他们的Acrobat 9 Pro在美国卖449美元，而在英国则是445英镑（750美元）。从经济学的角度这是有意义的，但这仍然让我无奈的狂砸键盘以表示我的愤怒。 版本细分有几个微妙之处。让我们用提供不同大小健怡可乐的快餐店为例： 产品 价格 小杯 1美元 中杯 1.5美元 大杯 2美元 选择这些价格是为了最大化连锁快餐店的利润。没什么钱的人，或者不是那么口渴的人会买小杯；那些有点口渴的会买中杯，非常口渴的买大杯。额外的一点可乐对餐厅而言几乎成本为零：这就是找到一个适合所有人的价格点。 在这里你可以看到参考点的使用。消费者看到‘小杯’饮料，会认为“中杯”饮料是一个优惠（一点钱更多的饮料）。 到目前为止是如此明显，不过这就是一个微妙之处：增加‘特大杯’饮料会提振‘大杯’饮料的销售，即使是无人购买‘特大杯’也是一样。在临界点增加更多选择会让人们选择中间值。他们不想显得小气或者贪婪，所以会选择安全的中间值。本例中，增加‘特大杯’让中间值向上移动，于是获得更多收入。 然而还有第二个微妙之处。这只有人们可以容易的在不同版本产品之间进行比较时才有效。对于汽水，它有效。特大杯汽水明显要比大杯多，大杯又比中杯多，中杯比小杯多。于是人们选择一个安全的中间值。 但如果人们困惑于选择不同版本的产品时效果将适得其反。在这种情况下，人们会逃离中间值而走极端。以笔记本为例，假设你让人们在以下产品间进行选择： 笔记本类型 功能 价格 ‘标准’笔记本 正常功能 1000美元 X100 标准+DVD 1100美元 X102 标准+无线网卡 1100美元 X103 标准+更快处理器 1100美元 X104 标准+DVD+无线 1200美元 X105 标准+无线+更快处理器 1200美元 ‘超级’笔记本 标准+DVD+无线+更快处理器 1300美元 这时人们开始走极端而不是选择中间选项。他们要么选‘标准’版，要么选‘超级’版。这是因为他们无法比较选择不同项目带来的好处。无线网卡是比更快处理器更好的选择？DVD呢？最终，人们选择了“全部或者都不”的简单决定。 当人们面临一系列让他们无法进行比较的迷惑人的选择时，走极端不是他们唯一的选择。他们会有推迟决定的倾向：要么不买，要么选择竞争对手的产品。 这一违反直觉的行为会产生一些有趣的结果。如果消费者面临比如买夏普或者是东芝微波炉的选择时，大概一半会买夏普，一半会买东芝。如果让他们在一个东芝和夏普的多个型号间选择时，那么两种结果都可能发生。 如果他们可以容易的比较不同的夏普型号（例如它们之间只是价格和一个特性的区别，比如尺寸和功率），那么更多的人会买夏普而不是松下。这是对提供一个产品的多个型号可以提高产品销售的证明。 另一方面，如果他们无法容易的在夏普的不同型号间进行比较，效果就会相反。比如，如果夏普的一个型号有可调速旋钮，一个有湿度传感器，一个有可编程菜单，另一个有‘保温’功能，那么消费者会避开夏普，拒绝混乱，选择松下。这显示了提供多个型号的产品会如何降低产品销售。 以下是微软的Vista操作系统的可能版本： 功能 家用基本版 家用高级版 商业版 终级版 建议零售价格（MSRP） 199.95美元 259.95美元 299.95美元 319.95美元 最安全的Windows √ √ √ √ 快速搜索 √ √ √ √ 优雅的Windows飞行体验 √ √ √ √ 笔记本最佳选择… √ √ √ 文档共享与协作… √ √ √ 在移动PC上使用第二屏幕… √ √ √ 一体化媒体中心… √ √ √ 硬件失败保护… √ √ 扫描，传真以及接收文档与图片… √ √ 远程访问… √ √ 简单网络连接… √ √ √ √ 数据丢失保护… √ 轻松刻录DVD… √ √ 更多乐趣… √ √ 硬件失败保护是否比一体化媒体中心功能更重要？远程访问企业资源是否比刻录DVD更重要？这很难鉴别，因此消费者很可能做出以下三种选择之一的决定： 走极端-购买家用基本版（199.95美元）或者终级版（319.95美元） 推迟决策-仍然用一Windowx xp 购买竞争对手产品 于是我有史以来第一次购买了Mac产品。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[《软件定价》一书全文]]></title>
      <url>%2F2014%2F03%2F03%2F2014-03-04-software-pricing%2F</url>
      <content type="text"><![CDATA[《软件定价》一书的翻译缘起于2012年时自己对某软件解决方案的定价的纠结，很薄的一本书，不过内容却是相当的丰富，对创业的朋友也大有实战意义，于是在联系作者Neil Davidson获得了同意后对本书进行了断断续续的翻译，2013.1月完成初稿，最终在2013.7月左右基本完成最终修订。个人能力有限难免有遗漏与错误之处，欢迎大家指正，请微博关注行走的家园并提出您的宝贵意见。 关于版权本书的翻译采用CC协议，转载请标明出处http://www.zjdian.com, 翻译错误请联系微博行走的家园提出您的宝贵意见 不只是摇骰子实用软件定价精简指南 作者序在2007年商业软件年会上Michael Pryor主持了一个关于软件定价的重要会议。该会议有太多的人出席，大量的人员不断涌入，以致于在所有人完成了自我介绍后已经没有时间来谈论软件定价了。关于大家对软件定价的关注，我也有着类似的经历；实际上“如何为我的软件定价？”可能是软件企业家，产品经理问我最多的问题。 这本指南的目的就是回答这一问题。 首先我需要感谢Phil Factor，Tony Davis以及Michael Pryor对本书的编辑，审阅以及建议。还有很多我无法提及的人对本书提供了帮助，他们提供了与定价相关的轶事，为本书提供了校订。正是有了他们的帮助，本书才能比之前更加完善。谢谢你们！ Neil Davidson剑桥（英国），2009年8月 TwitterBlog 前言：产品定价1938年，2个年轻的工程师打算开发一款他们自己的产品。他们纠结于应该建造一个什么产品。在考虑了放大器，无线电设备，航空控制器，口琴甚至帮助家庭妇女锻炼肌肉的电极后，最终他们决定开发一款示波器。为了不让顾客对产品的1.0版本感到不安，他们明智的把这款示波器命名为200A型号。 接下来呢？就是定价。 他们最终给产品定价54.40美元。这是否是因为它代表了产品制造成本再加上合理的利润？事实并非如此，这些工程师完全没有考虑这些因素。实际上，他们很快就意识到了示波器的生产成本比他们的要价要高。那么这一定价是否是基于竞争对手的定价？也不是。他们甚至没有花心思去了解通用无线对类似型号产品的定价是400美元。 他们选择54.40美元只是因为这让他们想起1844年在西北太平洋地区建立美国北部边境运动的口号（”54” 40’ or Fight!”）。 这是多么愚蠢的产品定价方式！ 然而这两个年轻人从失败中爬了起来，200A模型成为了整个时代销售时间最长的基本电子产品，33年后仍在销售。他们建立的公司成为了知名企业。他们的名字？他们是比尔.休利特和戴维.帕卡德（HP创始人）。 如果休利特和帕卡德这两个有着美好前程的斯坦福毕业生在产品定价时都可以犯如此重大的错误，那么我们又能报何希望呢？ 正像HP的结果，机会很大！ 第一章 一点——但不是很多——经济学 为了理解产品定价，懂点经济学很有帮助。最容易的方式是通过一个简单例子来说明。 假设你现在发布了Time Tracker 3000产品。这是一可下载的软件产品，它可以记录你一天里使用不同应用的时间，然后把使用信息发到一个统一网站。从这个网站你可以了解你一天的应用使用情况。假设你已经决定了对这个软件进行一次性收费，尽管在本书我们还会讨论网络应用，社交网络以及其它定价模型，但是目前为止我们还是让问题简单些。 你会如何对你的产品定价？ 如果你决定免费，那么你会有很多的用户。现在让我们假设你有1000个用户，这里面包括到处找便宜货的Belinda，学生Stewart，网络初创公司创始人Willhelm，产品经理Pat和企业开发人员Ernest。 让我们用一个无限细（代表0美元价格），1000个单位长（代表数量）的水平条来表示这1000个免费用户： 如果你决定不免费，软件定价为100美元，那么愿意购买的用户将急剧下降。Belinda是喜欢便宜货的人，他使用这一软件只是因为它免费，Stew是一个学生，所以他们俩都不会购买。为了简单起见，我们假设你会获得500个用户而不是开始的1000个用户。让我们再用一个100美元高，500单位长的条形面积来代表这一结果。那么你的收入是多少呢？它就是条形的面积，100美元x500=50,000美元。 现在让我们和第一个条形叠加在一起： 如果你把价格提高到200美元又会如何？一些在100美元会购买的用户不再购买，但是仍然会有些人购买。Willhelm经营自己的公司，他认为这一价格不值得购买，所以他不再对软件感兴趣。现在让我们假设仍然有300人会购买，我们再次用一个矩形来代表它，并与第一个图叠加。那么你从300个购买这一软件的用户获得的收入就是矩形的面积，200美元x300=60,000美元： 现在让我们把价格增加到500美元。此时，更少人会购买你的产品。产品经理Pat不再购买，因为此时她宁愿从你的竞争手购买。假设有50人最终会购买，再次用矩形表示，叠加到同一图上。价值就是矩形面积：50人以500美元价格购买，50x500美元=25,000美元 最终尽管Time Tracker 3000有些价值，但它不是无价的，我们假设定价在1000美元时会无人购买，同时我们还是用一个宽带为0的条形来在图上代表它: 至此为止我们已经在图上画了5个点，它们构成了对应特定价格购买Time Tracker 3000用户数的价格曲线。更进一步，你还可以通过计算对应该点的矩形面积获得在特定价格时的总收入（价格x购买人数）： 经济学家把这条曲线命名为需求曲线。 为了最大化Time Tracker 3000的收入，我们需要在图上找到一个点，在该点之下的矩形面积最大。为了说明这一点，有必要画一条面积（比如总收入）相对价格的曲线。对Time Tracker 3000而言，图形如下。（我已经把之前得到的5个数据点在图上标出）： 从图中，你可以看出你应该将Time Tracker 3000定价在300美元左右。这个价格不是你销量最大的位置，但是你却能收入最大化。 需求曲线理论上是非常直接的，但是实际却很困难。在真实世界中，你无法知道需求曲线的形状，或者是你的当前价格在哪个位置。 在某些曲线以及曲线上的某些点，提高价格是正确的行为：涨价带来的每人收入增加超过了购买产品人数减少的损失。而在另一些曲线，或者同一需求曲线的其它点上，涨价会导致销售额的大规模减少，你的收入会减少。 更进一步，需求曲线的形状是动态的，它取决于多方因素，包括了你的竞争对手他们如何应对价格变化，你的客户有多少钱打算花在这上面以及你的产品质量。 第二章 定价心理学：你的产品价值前一章讨论的需求曲线可能是动态的，取决于多方因素，但是你仍然可以对影响它的形状。本章中，我们将讨论人们如何决定为某个产品付多少钱以及你如何改变它。 在开始前首先你需要回答一个简单的问题：什么是你的产品？ 你的产品是什么？你可能错误的认为你的软件产品只是客户下载的一个个字节与位。实际上你的产品的定义比它更广泛。它不仅是软件——它还是产品文档，产品运行需要的帮助和出现问题时提供的支持。它还是继续开发新版本的承诺，未来的产品路线图。在某些情况下，它还是一个梦想；一种生活方式。 在会计行业的一个典型例子就是，在Red Gate公司，我们使用Sage公司的会计软件。我们公司并不是唯一的一个：Sage公司是一家有580万客户的软件企业，它雇佣了14，500名雇员，市值接近50亿美元。它在英国的会计行业占主导地位，不时受到来自微软和Intuit以及其它公司的联合攻击。 然而其它公司的软件都失败了。 Sage的软件又慢又难用。当我1999年开始使用时，甚至工具栏上的按钮在点击时也不会有点击的效果；它们只是一个灰色背景上的静态图片。这一应用是如此的差，以致于在Sage的网站上几乎都不太提及这一产品。 如果你理解了Sage的产品不仅仅是软件，你就可以理解糟糕的产品，极大的成功这两个对立的事实。 只要你买了Sage的软件，你买的就不只是软件了。你买到了保证：在税法变动时，软件也会更新。你买到了对产品的熟悉：如果你买的Sage软件，很可能你的会计已经会使用它了。你买到的是支持：如果你不理解其中的一个会计科目或者流程，你可以打电话给Sage的支持人员寻求帮助。要知道，每天有超过4万人打进Sage的支持热线。 Sage软件在英国占据垄断地位是因为他们理解什么是他们的产品。你也一样需要理解什么是你的产品。 感知价值 一旦你决定了你的产品是什么，你就需要考虑它对客户的价值。以Time Tracker 3000而言，我们假设它可以为Willhelm这个特定客户节约3小时的工作，而Willhelm将他的时间定价为50美元每小时。这意味着如果我们假设Willhelm没有其他更好的消费，只要Time Tracker 3000的价格在150美元以下，他就应该购买。 当然这要求Willhelm是经济学家，是一台喜爱理性决策的机器才行。实际上Willhelm是一个有血有肉的非理性人类，他不会为他的时间定价并计算成本收益。他有一个对Time Tracker 3000的感知价值，这一价值可能会也不会与产品的客观价值一致。 感知价值也可能高于产品的客观价值。2003年，高德纳咨询公司的报告指出几乎有一半的客户关系管理系统（CRM）无人使用。这是那些聪明人认为值几十亿美元的软件，实际上却非如此。 彩票也是另一个感知价值高于客观价值的例子。购买一张5美元的彩票，基于概率统计长期来看你只能获得3美元的回报。但是仍然有上百万的人购买彩票。 一个产品的感知价值也可能会比其客观价值低。几年前，我碰到一个坚持要把Excel表格处理软件当做Word文档处理软件来用的人。这个用户认为，购买微软Word软件的额外花费比他获得的好处小。这当然只是一个感觉而不是现实。但这也是在通常情况的极端案例。在Red Gate公司，我偶尔也会碰到有些人想购买我们的软件，但是却无法向他们的老板证明其价值。感觉上工具带来的对某人时间的节约值不了这个价（几百美元），而现实则是在工具上的支出几周就回收了成本。 回到Willhelm和Time Tracker 3000的例子，如果你想改变Willhelm为你的产品所付的价格，改变你的产品是一个选择，但是这只有你同时也改变他的感知才有效。实际上，如果你还可以改变Willhelm对你的产品价值的感知而无需改变你的产品。这也正是市场营销要做的事情之一。 人们如何设定他们的感知那么人们如何产生对一个产品的感知价值？他们是如何思考的？ 在完全真空的情况下开始是极其困难的。试着问一个英国国会会员一品脱牛奶值多少钱，让人竞猜抽屉的正确价格，或者普通超市购物者应该为一瓶漂白剂付多少钱。他们的回答会陷入困难。 人们的感知价值总是有一个参考点。如果你卖待办事项列表软件，那么他们会看看其它的待办事项列表软件卖多少钱，如果他们在网上发现你的竞争对手的软件卖100美元，那么他们就会把对所有待办事项列表软件正确价格的感知定在100美元。 在1982年微软发布DOS 1.0的时候，他们将价格定在了50美元。在那时，面向大众计算机的操作系统是一个全新的市场。由于没有参考点，50美元看起来不错，于是它成为了大众接受的’公平‘价格。当IBM在1989年发布OS/2 1.0的时候将其定价为340美元，消费者就完全犹豫是否购买。这不是经济的原因：DOS和OS/2完全是不同的操作系统，340美元可能是对更先进的OS/2操作系统能为用户带来的额外收益的公平反映。但是微软已经定义了操作系统的参考点，当IBM试图改变它时，他们失败了。 这并非意味着你需要就此使用参考点。如果你的产品比竞争对手更好，并且你能展示这一额外价值，创造对该价值的感知，那么你是可以定价更高的。 当然如果你的产品比竞争对手价值低很多，那么你唯一的选择只有选择要价更低。在价格上竞争成为了你唯一的选择。以药品为例，我们本地市场上有30种左右不同类型的止痛药。你可以以1.97镑（3.40美元）的价格买一包16片装的Nurofen或者以0.32镑（大约50美分）的价格买一包16片的Tesco。真实的物理产品-200毫克的异丁苯丙酸-在这个普通品牌和知名品牌产品中含量完全一样。但是不要忘了整个产品不止是化学成分。它含包含市场营销，品牌和包装。用更广义的定义，Nurofen是明星产品，Tesco只能通过价格与之竞争。 人们对你的产品价值的感知还取决于他们的品味。有些人喜欢好酒，他们可以花50美元买一瓶酒，而其他人觉得5美元的酒味道也不错。此外人们属于哪一个族群也会影响他们愿意付多少钱的意愿。正如Dave O’Flynn对我写道： “在我加入Atlassian前，我从来不认为苹果的产品值那么高的溢价。然而12个月后，我很高兴以高额的溢价买了一台Macbook Air，因为我周围都是重视设计和优雅的人。而在那之前我周围的人都是看中物有所值的人，那时我的笔记本是一台普通的AMD，与现在的Macbook Air比似乎有1吨重。我还是我；改变的是预期以及对我周围的价值感知。” 人们有多少钱也决定了他们对价值的感知。Dennis Kozlowski，泰科的前CEO会觉得15，000美元的狗形雨伞架很值，然而大多数人不会这样认为。 知识也会影响人们对产品价值的感知。一台拥有1.4G赫兹主频的双核处理器，4G内存，蓝光驱动器，Ubuntu系统的笔记本对我而言比一台使用N系列安腾处理器，DVD驱动器的笔记本价值要高，然而对我母亲则不是如此。 另外尽管这是一个低价的把戏，但是5和9在人们对价值的感知上有着额外的心理影响。这正如在超市商品里面1.99美元似乎要比2美元便宜很多，在网上1995美元似乎比2000美元要少很多一样。 提高感知价值医药行业有另一个关于市场营销无需改变产品实质而提高产品感知价值的很好实例。在1981年葛兰素公司想发布他们的治疗溃疡的药品Zantac，他们面临的是史克公司的Tagamet产品所垄断的市场。尽管葛兰素公司认为他们的药比史克公司的更有效，美国食品与药物管理局将Zantc评价为与其他治疗方式相比疗效几乎没有区别。为了不把Zantac定位为一个与Tagemet类似的模仿产品，葛兰素决定对他们的销售和市场渠道大量投入。这一独特的促销增加了Zantac的感知价值，葛兰素成功的将产品定价更高以反映这一增加的价值。到了80年代末它已经打败了Tagamet，成为了全球最畅销的药。 以下是更多的一些增加你的产品感知价值的方式： 提高产品客观价值。感知价值和客观价值不一定完全一致，但他们是相关的。引用Joel Spolsky在2006年所说1： “借这六年运营自己的软件公司的经验，我可以告诉你，除了发布具有新功能的新版本外，我们在Fog Creek没用使用任何其他手段，去试图增加我们的收入。决没有。我们发布的新版本绝对是相当出色的。就像地心引力一样。我们试图利用Google广告时，我们植入多种附属产品时，或者当关于FogBugz的文章出现在媒体上时，我们几乎看不见任何影响。而当发布带有新功能的新版本时，我们看到了收入出现了突然的、相当不错的、持续的、固定的增长。” 给你的产品个性。37signals的项目管理软件可能不是最好的，但它有个性，37signals代表了：极致的简单。想要更多功能？很难。如果需要更多功能，买别的产品。 将产品与你关联，然后将你定义，宣传成为一个专家。在诺顿被赛门铁克收购之前的早期，彼得.诺顿即是诺顿。所有的诺顿产品都有一个他自己交叉抱着手臂的图片。 让人们爱上你的产品。当Black&amp;Decker引入他们的DeWALT钻头生产线时，他们在午餐时间到建筑工地和木料场分发手撕猪肉三明治，进行产品演示，举行有奖钻速比赛。他们到他们的用户出现的全美汽车比赛协会的比赛和放牧人竞技比赛上展示。他们让人们不仅喜欢他们的产品，同时也让人们喜欢上这一品牌。现在DeWALT钻头也拥有大量的的业余爱好者；人们想把自己与专业联系起来。尽管如著名谚语所说，人们买的是钻头，而不是孔。400美元的钻头！ 提供更好的服务。人们购买软件，希望软件能正确运行，在软件出问题时你能在旁边。如果你是一个有大个竞争对手的小公司，这是你可以比他们做得更好的地方。利用好这一点！ 通过你的信誉来提供保障。起初，品牌是建立信任的机制。回溯到1880年代，你买的下一块象牙皂保证和之前的一样。最近,早期的PC模仿者都受困于人们“没人因为从IBM购买而被解雇”心理。 建立一个群体。产品可以成为一个归属的符号。如果你可以把你的产品变成一个人们可以用来表明他们是谁的徽章，他们属于哪个群体，不属于哪个群体，那将是无价的。业余的自己制作爱好者完全不需要花400美元去买DeWALT钻头，但是他们喜欢属于‘专业’群体的感觉。 提醒人们你在你的产品上花了多少精力。人们更愿意为花了几年时间开发的软件付钱而不是一个简单抄袭的软件产品。1996年20岁的比尔.盖茨在组装电脑俱乐部的那封现在已经很出名的‘致爱好者的公开信’中就用了这一技巧： “一年以前，我和保罗艾伦很高兴地看到业余电脑爱好者市场迅速壮大，我们雇佣了Monte Davidoff一同开发了ALTIR BASIC程序。尽管最初的工作只用了两个月时间，但我们三人几乎用了去年一年的时间编写文档，继续改善BASIC，添加新的特性。现在，我们已经有了4K，8K，更多内存的磁盘BASIC和ROM BASIC。如果计算我们的工作价值，我们已经花费了4万多美元。” 给人们正直的感觉。咖啡店对公平贸易咖啡豆额外收取10美分，他们将自己的口袋与你的道德联系了起来。在你的公平贸易拿铁咖啡中，这10美分中有多少给了种植这0.25盎司咖啡豆的农民？少于一美分。 不只是销售物理产品。最新的宝马广告完全表达了这一点。视频显示着快乐的人们做着开心的事情，画外音“我们是一家汽车公司。但是我们不只是生产汽车。[…]我们很久前就意识到让人们感觉到与你生产什么一样重要。在宝马，我们不仅是生产汽车。我们创造快乐。”快乐值多少钱？明显比汽车价值高。 最后，我们需要区别你的产品。以功能，产品的好处，销售方式，提供的服务，来自哪个国家-以上任何方式都可以。 路标 现在你知道了客户会把你的产品价格与一个参考点比较，你需要尽全力让他们与对你有利的参考点比较而不是不利的参考点。如果你想在市场价格是100美元时，销售200美元的待办事项列表软件，那么你需要增加一些新功能让你的客户无法进行直接的比较，之后你促使他们与那些卖300美元的效率提升软件比较，而不是与待办事项列表软件比较。同时，你需要避免任何与开源软件的比较。 如果你的客户无法为你的产品找到任何参考点，那么他们会寻找近似值或者路标。超市经常利用这一点：消费者决定是否昂贵的冰淇淋（他们不经常买的东西）价格是否合理是基于日常的可乐（他们经常买的东西）的价格。如果超市的一听可乐卖2美元，那么消费者会觉得其它产品都太贵了。 假设你销售两种产品：Time Tracker 3000和Task List 400，一个待办事项列表应用。当人们有事情需要完成时，他们把它记录在Task List 400软件中，之后他们可以对这些任务排优先顺序，分解任务到子任务中，跟踪任务进度，最终自豪的标记任务已经完成。 我们假定Time Tracker 3000没有任何竞争对手，而Task List 400有着很多。你的客户将通过你对Task List 400的定价来判断Time Tracker 3000的价格。为你的待办事项列表应用定价为合理的25美元，同时你的客户会相信你的话，那么300美元的定价对Time Tracker 3000也是一个合理价格。如果为第一个应用定价为1000美元，那么他们会认为你在新产品上敲竹杠。 如果你的产品独一无二，客户无法找到参考点或者路标，那么你就有机会设定客户预期，定义他们的感知。如果你告诉你的客户Time Tracker 3000值300美元，那么他们很可能会相信你。我们在微软为第一版的MS-DOS定价上已经看到了这一点。 如果你在市场上有竞争对手，那么你的客户对成本有清醒认识。但是如果你的产品建立了一个新品类，那么早期采纳者通常对价格不敏感。如果你可以开发一个全新品类的产品-传送点，它可以把你毫发无损，快乐的从纽约传送到巴黎，那么你不仅可以定义自己的价格，甚至你还可以从20，000美元涨价到25，000美元，人们仍然会购买它。如果你制造汽车，一个已有品类中的新产品，把价格从20，000美元涨到25，000美元那么你的销售收入将损失。 第三章：定价陷阱到此为止，我们已经讨论了一些经济学理论，定价心理学。相信，你已经对如何定价有了一些想法。但是我们还需要记住关于定价的其他因素，以及需要小心对付的定价陷阱。 竞争对手当你为你的产品设定价格时你需要考虑你的竞争对手如何应对。如果你降价，他们是否会发起价格战？即使你的竞争对手有一个高成本的商业模式，无法和你在价格上长期竞争，但由于你严重威胁到了他们的生存而使他们必须回应的风险仍然存在，他们只能寄望你先出局。 航空行业就是发起价格战是无益的最好例证。1977年9月26日，Freddie Laker的第一条空中列车航班从伦敦的盖特威克飞往纽约，回程航班价格只要238.5美元（再加上几美元饭钱），比竞争对手票价的一半还要低。 5年后，Laker航空业已成为了它发起的这场恶意，可恨价格战的牺牲品，因价格战而梦碎。正如Laker一样， EOS航空，Silverjet以及Maxjet都相继发现，以价格为基础来开拓一个行业在最好的情况下是高风险，而最坏的情况则是自杀，尤其是当你的竞争对手除了厮杀到死而别无选择的时侯。 如果你将从价格上进行竞争，那么你必须降低竞争对手反击的可能性。不要敲着战鼓，告诉媒体你将毁灭他们（网景公司的Marc Andreessen犯的一个错误就是当他说“我们将让他们屁股冒烟”-指微软（或者如Andreessen所说的“那些在雷得蒙德的傻瓜”）。专注于他们的边际客户，期望当他们发现你时已经太晚才是正确的方式。 另一方面，如果你的产品定价过高，是否会有其他竞争对手进入？将Time Tracker定价在10，000美元，你可以创建一个新的市场，不过你的竞争对手将生产Tyme Trakka 3000，廉价出售，抢走你的生意。 微软就以经常这样干而出名。他们等待竞争对手（经常还是合作伙伴）去证明一个高价，低市场容量的市场-不管是CRM还是测试工具或者商业智能-之后他们就会以低成本，高市场容量的模式加入战团。 公平不管你如何为你的产品定价，一定要记住消费者对公平有着敏锐的识别力，尽管这时常是非理性的。违反这一点前，请三思而后行。 出版业为此提供了一个绝佳的例子。经济学家会说我从阅读纸版Sebastian Faulk的詹姆斯.邦德惊险小说“不顾一切”获得的价值（传统经济学都是关于价值）与阅读电子版一样。然而纸版和电子版价格都是一样（14美元）。短视的出版商对纸版和电子版定价一样是不公平的。我们会感觉被愚弄了，我们不喜欢这样。 盗版如果你定价太离谱，那么你会为一类特殊竞争对手创造机会：盗版。为软件定价过高，或者定在人们认为‘不公平’的价位，那么准备着最终被盗窃吧。 不过凡事有两面，盗版者有时也可以是你的朋友。 首先，如果你的战略是为每一个潜在客户以他/她能承受的价格提供产品，最终形成全球垄断，那么盗版为你提供了一个便宜的地下渠道。他们把你的软件拷贝提供给那些不会付钱，无法承受这个价格的人手中，这些人要么是不够正直要么就是太有原则了。结果是，使用盗版的人里面最终有些人会付钱。 这就是早期的共享软件运作的方式。80年代早期，电子公告牌和用户讨论组是网络传播盗版软件的主要方式。1982年Andrew Fluegleman和Jim Knopfpiggy利用这一网络，在他们的软件里面加上了一个提示，请求人们如果喜欢他们的软件请为之付费，共享软件由此产生。 Adobe公司也利用了盗版，只是可能没有意识到而已。尽管Photoshop软件有很多更便宜，甚至免费的竞争对手，它的售价还是700美元。这怎么可能？人们先使用盗版Photoshop，之后当他们良心发现或者更有钱的时侯就会购买它。如果Adobe把价格降到300美元，盗版用户可能还是不会买，那么公司在那些愿意付700美元的用户上获得的收入就会减少。所以最好的策略还是拥有一个保持高价，盗版者希望，有一天，他们终会合法拥有的产品。 第二个盗版者是朋友的原因是他们是一个风向标。他们预示了某个市场被疏忽。大多数人不是天生的骗子，但是高价会迫使他们偏离良好的本性。苹果公司意识到了非法下载站的成功，这预示了人们对便宜可下载音乐的需求。满足人们的需求的战略远比音乐唱片行业逃避现实的策略要好。 转换成本 如果你试图说服人们从竞争对手的产品转换到你的产品，那么你需要确定客户面临的转换成本。 假如你试图说服客户从他破旧的500美元字处理应用转向一个更优秀的100美元产品上。首先你需要为经济转换成本定价。将文件转换成新格式，学习新的菜单布局需要花费客户的时间，这些最终都是金钱。 其次，你还需要克服心里转换成本。人们总是对已经拥有的定价过高，而对还未拥有的定价过低。你会为我上图的三个巴沙木做的企鹅估价多少？他们花了我10美元（从奥斯陆的卡通刻展会上购得）。在意提高你的价格吗？不要这样认为，我100美元也不会卖。我敢打赌你家里面也一定摆满了这样的东西。 另一个有力的心理因素就是人们很难克服对已经花掉的钱的情感归属。理性上看，钱已经花了。这已成为沉没成本。你的客户不应该再想着他花在那个破旧的字处理器上的500美元。可是，他就是这样。 为了明白这一点，让我们用一组学生的案例来说明。假设他们意外的买了同一个周末的一张50美元滑雪票和一张100美元的滑雪票，50美元的那个滑雪之旅会更有意思。他们会选择哪一个旅行，而又放弃哪一个旅行？理性告诉他们需要选择便宜的那个，但实际上超过一半的会选择没有那么有趣，但更贵的那个旅行。 为了降低转换成本，你可以采取一些措施让它们变得有利于你。以下是一些例子。Open Office是一个包含开源字处理和表格应用的软件，它支持打开微软Word格式的文件。早期的微软Word不仅可以打开WordPerfect文件，还在帮助里面有一个用户专区，甚至允许用户使用WordPerfect快捷键。 这里还有另一个例子。如果你决定在90天的免费试用期后停止使用FogBugz软件，那么Fog Creek公司会退回你花的钱。 该策略有两个影响：首先，它减少了转换到Word和Fogbugz的心理和经济影响。其次，一旦你已经转换，那么你就已经花了时间精力来使用新的软件，此时你会有新的转换成本，这将阻止你退回去。 你应该考虑自己的成本吗？很显然你不能把你的软件定价比单个生产成本低。这是你的边际成本。你可能觉得这些成本为零，实际上它们不是。 你需要找到潜在客户，说服他们购买。如果你有销售团队，你需要付销售佣金给他们。支持客户需要花钱，争取不花钱的客户也要花钱。 如果你是依赖于面对面的集中销售模式，那么你的销售成本明显比低接触的网络销售模式高。但是两种情况下你都需要考虑成本。 如果你计划对大部分用户不收费，那么仔细考虑每一个额外用户的成本。如果你觉得成本为零，那么你绝对错了。如果你是运营一个网站，那么每一个额外用户需要花费你的存储空间，CPU时间，带宽等。这个成本可能很低-也许就几分之一美分-但是如果你有大量用户那么一点成本乘以大量用户也会是一笔大开销。 以YouTube为例，它提供免费服务，理论上可以通过广告来支撑。支持一个额外的视频的成本极低（大约0.1美分），但是在2009年它支持了大约750亿个视频。将这个极低的成本与极大的量级相乘，你就可以理解为何谷歌每年要花费7.1亿美元来运营它了。它完全不能通过广告收入来支撑其花费。 Paperback软件公司是另一个错误理解软件销售的例子，它无法承担其成本，最终导致了灾难。当Adam Osborne在1984年成立Paperback软件公司时，它是在软件成本太高的前提下建立的。 他们在1986年以99.95美元的价格发布了VP-Planner，在市场上直接与500美元的Lotus 123竞争。在80年代时，大多数软件都是通过经销商销售。经销商从卖出的每一个软件中获得佣金，所以他们不喜欢低成本，低边际利润的VP-Planner，于是他们说它的坏话，鼓励人们购买高成本，高利润的Lotus 123. 更糟的是由于VP-Planner和Lotus 123完全一样，客户也要求这个便宜软件和昂贵的Lotus 123一样的支持，这严重影响了Paperback 软件的利润。Paperback公司成功的从Lotus手中获得了市场份额，但是却无法有足够的钱来应对Lotus发起的诉讼。 如果你的客户愿意付的钱低于你的软件销售成本，那么你就没有业务，产品也会失败。你需要降低销售成本，或者改变定价模式使得客户在产品生命周期中付更多的钱。 当松下1994年发布了游戏机3DO时，时代杂志把它提名为了年度产品。32位RISC处理器，定制的数学协处理器，2M内存，这远远超越了它的时代。但是松下将产品定价为699美元，这远远高于竞争对手的价格和目标市场的早期采纳者的承受能力。高价格与混乱的市场营销导致了3DO的彻底失败。 其它的游戏设备厂商吸取了这个教训。当PS3和Xbox 360发布时，生产成本比市场能接受的销售价格高，于是索尼和微软定了低价，接受了他们在每台游戏机上会亏钱的事实（多达300美元）。然后他们从消费者购买游戏的版税中进行补偿。真实的游戏机的价格被隐藏了起来；一个聪明的定价模型掩盖了它。 任天堂的Wii又采取了一个不同的方式。他们想面向一个更广阔的市场，而不是他们的竞争对手所面向的18-35岁男性这一最佳位置，他们意识到老人，家庭妇女愿意在游戏机上花的钱要比游戏迷少。于是他们降低了生产成本，使用更便宜，更慢的部件。当Wii在2006年9月发布是，任天堂卖出的每一个游戏机都是盈利的。这使得游戏生产成本也更低，尽管版税可以更低，但是购买游戏不一定便宜。为什么？到现在，对你来说答案已经很明显了。 你可能注意到了我有一点没有提到，那就是产品开发成本。到目前为止我讨论了边际成本-生产，销售每一份软件的成本。你的一次性成本是不同的。你可能花了几百或者几百万来开发你的产品，但是这个钱已经花了。用掉了。这是沉没成本。重要的不是你花了多少钱，而是人们愿意付多少钱。 第四章：定价进阶到目前为止，我们只考虑了销售单个产品。如果有几个产品要销售或者是要销售同一产品的多个版本呢？ 版本每一个潜在客户都有一个购买你的产品的心理价位。回到前面的例子，Belinda（买便宜货的人）和Stewart（学生）只会在软件是免费时才使用Time tracker 3000。Willhelm愿意付150美元，Pat最多愿意付400美元。让我们假设Ernest愿意付600美元。 下表是你在每个价格点的收入： 价格 谁会购买 收入 0 美元 每个人 0 美元 150 美元 Willhelm,Pat, Ernest 450 美元（3人以150美元价格） 400 美元 Pat, Ernest 800 美元 （2人以400美元价格） 600 美元 Ernest 600 美元 （只有Ernest） 1000 美元 没有人 0 美元 如果这5人是你的全部目标市场，那么为了最大化你的收入，你对Time Tracker 3000的定价应该是400美元。这是最好的单一价格，但是你将失去来自Willhelm的销售，同时你也会失去Ernest愿意付的额外价格收入。 如果有某种方式可以把我们的产品以每个客户能接受的最高价格出售的话，那么你能获得的软件销售收入就是1150美元。这就是版本要做的事。它是一种将你的用户根据他们付款意愿来分类的机制。你计算你是否可以不同方式对你的客户进行分组，然后看这些组是否愿意为你的产品付不同的价格。 以下是一些具体的实施的方式： 根据功能。比如你可以有‘标准’版和‘高级’版工具。这在软件行业极为普遍。微软的Visual Studio 2008就有5个不同版本：精简版（免费），标准版（299美元），专业版（799美元），团队系统（5，469美元）和团队套装（10，939美元）。这是一个覆盖了所有群体的价格，根据功能不同对应了从没钱的爱好者到绩优企业的开发人员。在Time Tracker 3000例子中，你可以建立一个专业版本让他们可以与其它从事类似工作的人使用不同产品的统计进行比较。 根据可获得性。一些客户可能愿意为更快的得到你的产品付更多钱。精装书就是这方面的一个例子。它们有和普通纸版书一样的内容，只是包装不同，面向那些不想等待的人。对Time Tracker 3000你也可以销售一个额外的订阅服务使客户可以更快获得软件。 根据人群特征。学生比企业钱少，爱好者比专业人士钱少，学校的孩子比婴儿潮时的人钱少。你可以提供一个专供学生的Time Tracker 3000版本，只要他们能证明他们是全职学生。 根据地区。美国的客户会愿意比印度和中国的客户付更多钱。微软为了和来自开源软件的威胁竞争，提供了一个低价的‘起步’版Vista操作系统，只在如印度和墨西哥这样的贫穷国家提供。Time Tracker 3000可能在印度仅以其在美国价格的10%提供，但是由于本地化为印度语，使它对西方国家用户毫无意义。 根据平台。Mac用户可能比Windows用户愿意为软件付更多费用或者与之相反。那么你可以对Time Tracker的Mac版比Windows版定价更高。 当然你也需要注意进行版本区分的风险。你需要保证你为每个版本选择的功能要对你的目标细分客户群有吸引力。例如，如果你为产品引入一个‘精简’版，你想要确信专业用户不会降级去使用该版本。 当你利用这些准则进行版本细分时，如果你的目标是满意的客户，那么你最好要牢记客户对公平的敏锐感觉。Adobe公司试图根据地区来区分不同版本；他们的Acrobat 9 Pro在美国卖449美元，而在英国则是445英镑（750美元）。从经济学的角度这是有意义的，但这仍然让我无奈的狂砸键盘以表示我的愤怒。 版本细分有几个微妙之处。让我们用提供不同大小健怡可乐的快餐店为例： 产品 价格 小杯 1美元 中杯 1.5美元 大杯 2美元 选择这些价格是为了最大化连锁快餐店的利润。没什么钱的人，或者不是那么口渴的人会买小杯；那些有点口渴的会买中杯，非常口渴的买大杯。额外的一点可乐对餐厅而言几乎成本为零：这就是找到一个适合所有人的价格点。 在这里你可以看到参考点的使用。消费者看到‘小杯’饮料，会认为“中杯”饮料是一个优惠（一点钱更多的饮料）。 到目前为止是如此明显，不过这就是一个微妙之处：增加‘特大杯’饮料会提振‘大杯’饮料的销售，即使是无人购买‘特大杯’也是一样。在临界点增加更多选择会让人们选择中间值。他们不想显得小气或者贪婪，所以会选择安全的中间值。本例中，增加‘特大杯’让中间值向上移动，于是获得更多收入。 然而还有第二个微妙之处。这只有人们可以容易的在不同版本产品之间进行比较时才有效。对于汽水，它有效。特大杯汽水明显要比大杯多，大杯又比中杯多，中杯比小杯多。于是人们选择一个安全的中间值。 但如果人们困惑于选择不同版本的产品时效果将适得其反。在这种情况下，人们会逃离中间值而走极端。以笔记本为例，假设你让人们在以下产品间进行选择： 笔记本类型 功能 价格 ‘标准’笔记本 正常功能 1000美元 X100 标准+DVD 1100美元 X102 标准+无线网卡 1100美元 X103 标准+更快处理器 1100美元 X104 标准+DVD+无线 1200美元 X105 标准+无线+更快处理器 1200美元 ‘超级’笔记本 标准+DVD+无线+更快处理器 1300美元 这时人们开始走极端而不是选择中间选项。他们要么选‘标准’版，要么选‘超级’版。这是因为他们无法比较选择不同项目带来的好处。无线网卡是比更快处理器更好的选择？DVD呢？最终，人们选择了“全部或者都不”的简单决定。 当人们面临一系列让他们无法进行比较的迷惑人的选择时，走极端不是他们唯一的选择。他们会有推迟决定的倾向：要么不买，要么选择竞争对手的产品。 这一违反直觉的行为会产生一些有趣的结果。如果消费者面临比如买夏普或者是东芝微波炉的选择时，大概一半会买夏普，一半会买东芝。如果让他们在一个东芝和夏普的多个型号间选择时，那么两种结果都可能发生。 如果他们可以容易的比较不同的夏普型号（例如它们之间只是价格和一个特性的区别，比如尺寸和功率），那么更多的人会买夏普而不是松下。这是对提供一个产品的多个型号可以提高产品销售的证明。 另一方面，如果他们无法容易的在夏普的不同型号间进行比较，效果就会相反。比如，如果夏普的一个型号有可调速旋钮，一个有湿度传感器，一个有可编程菜单，另一个有‘保温’功能，那么消费者会避开夏普，拒绝混乱，选择松下。这显示了提供多个型号的产品会如何降低产品销售。 以下是微软的Vista操作系统的可能版本： 功能 家用基本版 家用高级版 商业版 终级版 建议零售价格（MSRP） 199.95美元 259.95美元 299.95美元 319.95美元 最安全的Windows √ √ √ √ 快速搜索 √ √ √ √ 优雅的Windows飞行体验 √ √ √ √ 笔记本最佳选择… √ √ √ 文档共享与协作… √ √ √ 在移动PC上使用第二屏幕… √ √ √ 一体化媒体中心… √ √ √ 硬件失败保护… √ √ 扫描，传真以及接收文档与图片… √ √ 远程访问… √ √ 简单网络连接… √ √ √ √ 数据丢失保护… √ 轻松刻录DVD… √ √ 更多乐趣… √ √ 硬件失败保护是否比一体化媒体中心功能更重要？远程访问企业资源是否比刻录DVD更重要？这很难鉴别，因此消费者很可能做出以下三种选择之一的决定： 走极端-购买家用基本版（199.95美元）或者终级版（319.95美元） 推迟决策-仍然用一Windowx xp 购买竞争对手产品 于是我有史以来第一次购买了Mac产品。 捆绑捆绑是另一种给予你的客户更多价值，说服他们购买，产生更多收入的方式。简单说，就是人们喜欢特价。 以1595美元的价格获得价值5140美元的软件的想法当然是吸引人的（以Red Gate公司出售的SQL Toolbelt软件为例）。 即使是没有价格折扣，捆绑也是有意义的。 假如你有2种产品，Time Tracker 3000和Task List 400。网络创业公司创始人Willhelm是一个极为专注的人。一旦他开始做某事，他就要将其完成。但是他挣扎于把他需要做的事情组织起来。Pat则觉得任务列表没有什么意义，但是她感觉她浪费了工作日的大多数时间，所以她想知道这些时间她在干什么。因此Pat和Willhelm愿意为每个产品上付不同数量的钱： 顾客 Time Tracker 3000 Task List 400 Willhelm 150美元 400美元 Pat 400美元 150美元 如果你单独销售你的产品，你需要选择人们能承受的最大价格。你需要将Time Tracker 3000定价为150美元（Willhelm也愿意买），Task List 400定价为150美元（Pat也愿意买）。这意味这Willhelm和Pat每人为每个产品付150美元，你将产生600美元收入。 不过如果我们假定你把Time Tracker 3000和Task list 400捆绑销售。在这个点，绑定价值对Willhelm和Pat都是550美元。把价格定在550美元，捆绑销售产品给他们两人，你的收入是1100美元。Willhelm和Pat获得了他们想要的全部软件，你也产生了500美元额外收入。 然而捆绑也有缺点。当你把软件捆绑销售给顾客，他们很难理解他们是在为什么付费。因此，这意味着他们可能不会使用它。 例如一个按照固定价格菜单套餐就餐的人要比明确为咖啡付了费的就餐人员更可能不喝套餐中的咖啡。咖啡是捆绑销售的，所以就餐者没有将他消费的和他付钱的联系起来，所以更可能不消费。 对软件而言，如果客户对其中捆绑的一个软件使用很少，那么很可能不会购买未来的新版本，或者为维护付费。一种应对这种效果的方式是明确标出捆绑中的每一项的价值。 多用户许可证多用户许可证是另有一种捆绑软件的方式。不过在你决定为你的客户提供多用户折扣时，牢记以下三件事： 大公司有更多用户所以会倾向购买更多软件拷贝。向他们提供以2份软件价格获得3份软件拷贝的折扣，使他们比个人用户和小企业用户更有利。大公司更有钱，对价格没有那么敏感。这意味着没钱的在有效补偿富裕的。 长期而言销售收入可能下降。为两个软件许可证付费的公司可能在你要求的情况下愿意为3个软件许可证付费。而另一方面，他们实际没有这样做。任何人都喜欢打折，大公司也是如此。 大公司更有钱，但他们的采购流程也可能更严格。 在任意给定情形，要知道那个因素占主导地位是很难的。我知道一个公司就是因为上述的前两个原因而不使用多用户许可证模式的。他们从‘以3个许可证价格出售5个许可证’的捆绑模式改变成对多个拷贝，每个拷贝优惠10%的模式。 结果是他们的客户更喜欢购买多用户捆绑的便利性。如果他们的客户有2个用户，他们喜欢能够以3个许可证的价格买5个用户许可证，这样使他们在有新用户开始使用软件时获得折扣（2个‘免费’用户）。这种模式好处大于过度付费的风险，此时永远没有第三个用户。2个月后，在损失了几十万美元后，这个公司又回到了多用户许可证模式。 重要的是理论不能给你智慧，多用户模式也一样。唯一的方式是通过实践找到答案。 地区许可证你需要对地区许可证非常小心。销售一个地区许可证给微软或者沃尔玛，除非针对这一情况你能准确定价，而且价格够高，否则你将损失大量未来的收入。如果你坚持销售地区许可证，确保给‘地区’一个清晰的定义。它是一个特定办公室，或者国家，或者全球？ 采购流程在定价时你必须考虑你的客户的采购流程。如果你是面向企业，那么这里会有一系列的门槛，在你跨过时需要仔细考虑。例如： 如果你的产品售价为10美元，那么最终用户可能会用个人信用卡付款，也不会向公司报销。 如果到了50美元，他可能会用自己的卡购买，然后向公司报销。 如果到了995美元，他可能会借自己老板的公司卡购买，公司直接付费。 如果到了1000美元，他可能需要填写申请文件，向老板说明购买理由。 如果到了5000美元，他可能需要部门负责人的批准。 如果到了25000美元，他可能需要和CEO谈了。 在每一个关卡，不仅成本在增加，处理的麻烦也在增加。如果你能弄清这些临界点在哪里（临界点根据你的客户企业的特点会随着经济状况会发生变化），那么最好把软件价格定在恰好在临界点下而不是恰好高于临界点。 一旦你跨过了一个临界点，你会相对容易移到到下一个临界点。说服某人花10美元而不是1美元，要比让他们一下子就直接打开钱包容易。 另一个提供多用户折扣和捆绑销售的原因是如果你销售产品给一个组织，你将销售的那个人会帮助你跨越临界点。一旦你通过了一个临界点，他们可能更迫切希望你走得更远。 假设你说服了Blue Door软件公司的IT经理Frank购买100份time Tracker 3000 的用户许可证。Frank和你艰难协商把价格定在了25，000美元。他知道他还需要说服CEO Victor来批准这一支出。Victor是一个令人害怕的老板，他又十分忙碌，难以说服。Frank意识到他可能在未来6个月中的某一天可能需要更多的Task List 400拷贝，他不想再次去说服Victor。他知道尽管Blue Door软件公司目前只有100名员工，但未来一年公司员工将持续增长。如果他要求批准花25，000美元购买Time Tracker 3000，为什么不要求花30，000美元，然后让你提供一些免费的Task List 400拷贝呢？或者是花35，000美元，然后要求额外的50个许可证？这对Frank，对你，对Blue Dorr软件公司都有好处。 免费 有些人争论说软件的价格最终将不可避免的趋近于零。经济学家已经在任何有效市场证明了这一点，商品的价格将趋近于其边际生产成本。如果你是众多的销售扳手的生产者之一，消费者会去寻找最便宜的扳手。如果生产下一个扳手的成本是5美元，那么扳手生产者将在价格上竞争，竞相廉价出售，最终价格越来越低，直到能够使他们仍能盈利的5.01美元为止。 对于信息，理论而言其边际成本为零。提供下一系列的字节给下一个客户不花任何成本。因此最终客户为你的信息付的钱，和你销售的成本将趋近与零。Linux开源操作系统，Apache网络服务器，以及开源办公软件说明了这一点。 但是这一论调有几个漏洞。首先，如前面所讨论，你不只是在销售位与字节。你销售的是它周围的一系列内容，包括支持，文档等。你的客户购买的是多年，甚至是几十年你过去，现在以及将来的血，汗水和眼泪。这值100美元吗？或者是1000美元？见鬼，是的，你应该告诉客户这些。 第二，这儿没有商品一说。或者更准确的说，我们不需要像这样的商品。你的工作就是将你在做的事去商品化。如果你的潜在客户将你的待办事项列表应用或者字处理软件，会计软件，网站或者iPhone应用看作只是类似100个其它应用中的一个，那么你可以要求付的费用将大大降低。你必须想法让它引人注目，或者是无法比较。 如果星巴克可以将咖啡去商品化，对咖啡店和热水收费4美元，Stormhoek葡萄酒（我唯一所知的销售G-Strings的酒类品牌）可以将葡萄去商品化，毕雷矿泉水可以将水去商品化，那么你也可以将你开发的复杂软件应用去商品化。 尽管如此，我们也无须质疑‘免费’对消费者有着巨大魔力。这是一个你可以利用的魔力。 免费试用免费试用可以让你的客户免费试用你的软件，让他们购买前确信你的软件满足他们的要求。你甚至无需利用试用来获利。仅仅是客户可以试用你的软件，如果他们愿意的话，就传达了对你的软件质量的一个强烈信号。 当客户确实试用你的软件时，这可以增加它的感知价值。在一个著名的心理学实验中，那些拿着咖啡杯的人比那些只是看了一眼咖啡杯的人愿意为咖啡付的钱要多得多。如果允许提前使用它，人们开始有一种购买前就拥有它的感觉，正如我们所见，人们给予他们已有东西的评估价值比还没有的东西更高。 但是免费试用不是总是可行的。Red Gate公司销售将从SQL Server数据库中已删除内容进行恢复的工具。免费试用在这里就行不通：人们会下载软件，在免费试用过期前恢复数据，免费试用只适合那些人们需要不断使用的软件，免费试用也不会自己解决问题。 类似的，如果使用你的软件需要大量协助，或者软件质量很差，那么免费试用也不会成功。 免费增值模式包括提供为一些人提供免费版本，而为另一些人提供付费版本。典型的，‘标准版’产品会免费，‘增强’版会收费。Flickr， LinkedIn， Skype都使用了这一模式。 然而尽管免费提供软件十分流行，但是否是杰出的赚钱方式还不清楚。至少，你需要十分小心，免费版本需要足够有用，但又不能太有用以致损害到了付费销售。而且免费增值还需要用户量特别大才有用。Flickr只成功的发展了5%的标准用户升级到专业帐号。而存储，搜索Flickr那些免费用户的35亿张图片却不是没有成本的。 1993年，英国的移动电话市场开始热闹起来。一个由Cable&amp;Wireless公司和US West公司支持的新移动电信公司One2One决定以免费方式加入，他为新用户提供非高峰时段的免费电话。很快在圣诞节那天几千个想免费打电话的人涌入，网络被压垮，电话无法打通。One2One很快就获得了一个网络不可靠的名声，亏损了100万美元。 Flickr背后有雅虎，One2One背后有Cable&amp;Wireless，如果你想采用免费增值模式却没有一个有钱的‘干爹’，那就要小心了。 网络效应在有一种情况下免费是对你的产品的最好定价：当有很强的网络效应时。 网络效在你的客户使用你的产品获得的价值随着总用户增长而增长时发生。例如使用电话的价值随着你可以打电话的人的增加而增加；社交网络的价值随着越来越多人的加入而增加，email的价值随着拥有帐号的人的增加而增加，等等。在这些案例中，你获得了一个反馈环：越多人使用你的应用，它变得越有价值，于是更多人加入，如此循环。 免费在你的网络产品有竞争对手时变得更加重要。在这种情况下，结果只有两种：没有用户，或者是大量用户，同时在这当中有一个引爆点，超过它之后用户数会快速增加。 跨过引爆点你的用户数将快速增加。如果你不能达到引爆点那么你的用户数又会缩减到零。 如你在图中所看到的，在2009年初有一个明确的引爆点。审视一下传真机，电话以及其它严重依赖网络效应的发明的普及情况，你可以看到一个类似的模式。 因此，快速达到引爆点极为重要，而‘免费’则是达成它的一种很好方式。当然，一旦你跨过引爆点，你还需要保持产品盈利而又不失去用户。 特价特价与免费紧密联系：人们喜欢不花钱而获得物品。捆绑销售是一种类型的免费。你买Windows时，免费获得了IE。SQL Toobelt以1595美元的价格提供了价值5140美元的12个应用。这是3545美元的免费。 在你的网站上给一个或者两个产品打上‘大减价’标签，人们会认为这是一笔好交易，实际也是如此。但是如果给所有产品都标上‘大减价’那么人们会认为你在愚弄他们。 为了起到最好效果，特价只能限于特定产品或者特定时间。当在线游戏社区Steam，在2008年的假日季节进行第三方游戏特卖时，10%的折扣导致了了35%的销售收入增加（以收入美元记而不是销量）。25%的折扣导致了245%的销售收入增加；50%折扣导致320%销售收入增加，75%时，1470%的销售收入增加。 如果有什么东西比特价还让人喜欢那就是获得折扣，同时还让人感觉自己很聪明。 2006年圣诞前，Threshers（英国一家酒类经销商）为自己的供应商和朋友提供了可用于任何零售店的40%折扣特殊代金卷。 当这一代金卷“意外”的泄漏在网上（在Stormhoek的网站-我们之前已经提到过他们），消息就像野火一样传播开了。最终几百万人下载了代金卷。Threshers有义务接受这些代金券。他们的客户自以为很聪明，买到了便宜的酒，而Threshers则获得了暴利，同时宣传了他们的品牌。 不同定价方式你的产品应该收费多少并不是你需要做的唯一决策。你还需要决定如何收费。收费模式有很多： 订阅：大多数SaaS（软件即服务）公司使用这一模式。如任何模式一样，这有好处也有坏处。好处很明显包含，一次性预付费较少。但在除了明显的持续收入外，这一模式还有其它意外好处： 付多次小钱心理上要比一次付很多更容易接受。这也是为什么人们用信用卡买车，然后以20%利率还款，或者是假期一周花的钱用25年的抵押贷款来还。尽管要付的总数实际更多，但让人感觉却更少。 如果你的销售面向企业，那么你的最终用户向他们的老板解释一小笔，定期支出要比解释一次性大笔支出更容易。 持续的付款将推动经常使用。以健身俱乐部为例，那些一次性付年费的人，会在他们的大笔支出的头几个星期里频繁使用健身俱乐部，然后就不再来了。而那些按季度付款用户的使用模式则是锯齿形的，在付款后有一个高峰期，之后降低直到下一次付费。每月付费的用户则显示了平稳同时更高的使用率。最重要的是，由于他们是经常性用户，所以他们更可能为会员费续约，保持会员的时间也更长。 我们已经讨论过免费增值模式，这当中一小部分付费用户补偿大量免费用户。吉列模式则是对免费增值模式的小调整。吉列以将他们的产品分成两部分出售而出名：刀架和刀片。刀架很便宜，但是他们通过刀片获利。这一战略相当普遍。 Adobe公司对Acrobat软件采用了类似的战略。阅读文档免费，但创建文档则需要付费。惠普在打印机上亏钱，但是在墨盒上赚钱。第一款福特Fiestas亏本销售，但福特在配件和汽车金融上赚钱。微软和索尼在售出的每一个X-Box和PlayStation上亏钱，但在游戏版权上赚钱。 每用户定价的方式有很多种。通常方式包括按每个指定用户许可证或者同时使用用户模式。在Red Gate公司，我们使用每用户许可证模式。如果你有一组10人，所有的人都想用我们的软件，那么你需要买10个用户许可证。如果你无法统计总用户数，或者只有一些人会同时使用，那么按照同时使用用户定价就有意义。这一模式通常在服务器软件，比如数据库中使用。 另一常用许可证模式是每处理器或者每处理器核心模式。这一模式的典型问题是处理器会很快变得越来越快，拥有更多核。如果说你的缺陷跟踪系统绑定于客户的硬件能力，那么按照摩尔定律每两年他们从你的软件获得的好处将翻倍，而无需付一分钱。 每物理服务器/虚拟服务器许可证模式与每处理器模式有着相同缺点。更多的处理器被塞进了服务器中，客户可能以固定成本获得指数性增长的收益。 按照使用收费模式根据用户使用你的软件多频繁来收费。这可以是根据存储了多少兆字节，处理了多少交易，传输了多少G字节或者很多其它方式来决定。这在过去比其它模式使用相对较少，但在云计算发展起来后将变得越来越普遍，人们也希望按照按需使用付费。这一模式的一个不利之处是由于人们不清楚一次需要花费多少，所以他们不愿意购买。 向最终用户收费不是唯一的软件定价方式。你可以免费提供软件，而通过例如咨询，安装和培训或者销售广告赚钱。不过广告虽然是网站的一个通用模式，但实际很难赚钱。CPM-每千次展示成本-可能低至1美元。换句话说，要产生1000美元收入，你需要1百万次页面展示。要支撑3到4人的一个团队，这意味着需要有需要每月千万次的PV。大多数网络应用无法吸引这么高的流量。 给你的客户选择许可证模式的机会是有意义的。例如，如果你购买微软的SQL Server 2008，你可以选择每处理器许可证，也可以购买服务器许可证，然后按照客户端连接数收费。第一种模式价格5，999美元/处理器。第二种情况下服务器端需要付885美元，之后每个额外用户访问数据库需要付费162美元。 许多企业最终采用混合模式。例如Red Gate公司组合了一次性付费和每年10%-15%的支持和升级费模式。这种模式，我们既获得了一次性收入也有每年持续收入。 然而如果你选择这样做，你需要小心其中的陷阱。支持和升级费不只是一种获得收入的廉价方式，它也迫使你在即使不是适合你，你的客户或者产品的时间为了发布软件而发布软件。如果你想定期向客户收费，那么你需要让他们定期获得或者感知到价值。 在2001年发布了Windows XP后，微软引入了‘软件保证’计划。以年费的方式，微软保证企业客户可以升级到下一个操作系统版本。理论上，每一方都会获利：微软获得了有保证的收入来支持未来开发，客户摊薄了成本，以更便宜的价格升级到微软2003年将发布的代号Longhorn的新操作系统。但是Longhorn没有在2003年发布，2004年也没有，2005年还是没有。直到2006年底它才上市，被阉割成了Windows Vista。而大多数企业拒绝升级。 选择正确模式在选择你的定价模式时，这里有两个建议。第一，乏味。第二，以你的客户期望的模式给予软件许可授权-适合他们的商业模式。 Red Gate的第一个产品是Aardvark，一个在线缺陷跟踪系统。当我们在2000初发布时，我们决定采用按使用收费模式。我们根据录入到系统的每个缺陷收费。这对我们而言是有意义的，因为我们提供的服务是与客户对它的使用联系在一起的，但这对客户的工作方式或者期望的收费方式则没有意义。这是我们犯的第一个错误。我们犯的第二个错误就是忘记了要乏味，把每个使用单元称为‘一听蠕虫’。我们觉得这很酷。我们的客户却有不同意见，我们很快就转移到了按用户付费。 还有更糟的错误定价模式，在1990年代，Knight-Ridder和MAID合并成立了Dialog公司。它向企业和政府销售数据。用户登录，并在Dialog存储的60亿页的信息中搜索信息。 Dialog决定采用按使用付费模式。用户购买‘DialUnits’，不同的操作花费不同数量的DialUnits，这取决于该操作需要多少资源，访问的数据的价值。想对你的结果排序？这要比只是保持它贵。贵多少？这取决于你搜索的是什么数据库，搜索有多频繁。排序或者去掉重复结果是资源密集的操作，因此需要花费更多的DialUnits。有些操作则是免费。即使在一轮精简后，公司仍然需要4页纸来向客户解释定价模式。 2001年，Dialog公司引入了多种定价计划，希望用户可以选择按使用定价或者按时间定价中更便宜的模式。于是就有了四种不同平台-Dialog 交易,Dialog高级，Dialog企业。再加上折扣方式，多年选择，以及不同用户界面如Dialog经典，Dialog网页，Dialog经典网页，如一个用户所说，就是任意想一个数字，然后将它翻倍，然后加上你的母亲的年龄也比这一定价策略清晰，更好！ 第五章：价格如何代表你（以及你如何改变它） 价格从来都不是中性的。它们代表了某种信号。例如高价格可能意味你有高质量的产品。消费者认为昂贵的香水和酒比便宜的要好，即使这没有任何证据。 低价可能告诉消费者你的性价比或者你的特别。如果你的竞争对手以10，000美元每个拷贝出售软件，而你以100元价格出售，那么这一价格已经说明了你。当然，你可能会说这是‘改变游戏规则’，但是你的客户听到的可能是‘玩具’。 模仿你的竞争对手，你可能被认为只是一个‘复制’产品。如果你是复制产品，有着模仿的功能，复制的价格，那么人们为什么要从你那里购买呢？尤其是当市场中已经有了一个强势，垄断的产品时。 不管你选择什么价格，它发出的信号需要与你的品牌匹配，你的品牌又需要和你的现实相符。如果你不愿意花费资金在市场营销上来支持你的品牌，在研发上投入来使得高质量成为现实，在客户服务上投入向用户提供高质量品牌的服务，那么使用高价格来暗示你有高质量的产品是毫无意义的。 1996年，麦当劳发布了Arch Deluxe产品，一种为追求精致生活的成年消费者提供的汉堡包。为了收回高质量原料的额外成本以及2亿美元的营销费用，麦当劳对给这一新的三明治定价比巨无霸高32美分。但是他们想试图建立的产品（高质量，溢价）与麦当劳的品牌相冲突（便宜，便利），Arch Deluxe彻底失败。其中一种说法是可能他们给这款汉堡包定价太低，32美分的加价没有给出足够的质量暗示。 你的商业模式和战略需要支撑你的定价模式。如果你有着花费巨大的销售人员开着豪华汽车，带着你的客户出去打高尔夫，你的最终用户需要大量的手把手协助和对你卖给他们的软件进行定制，那么采取低价你是无法支撑的。类似，如果你是在网络上销售简单包装的大众软件，那么高价将毫无成效。 当Red Gate公司试图进入网络负荷自动测试市场时，其中一个失败的原因就是我们在一个高价格解决方案主导的市场采用了低价，高销量方法（还有许多其它原因，包括软件产品也不够好）。我们认为用户会喜欢能够直接下载，试用然后再购买的产品，但实际是我们的客户需要比我们能提供的还多的大量支持。对大多数客户而言，他们需要的不只是一个产品，他们需要的是人员密集的服务，以及一个大名鼎鼎，昂贵的供应商能提供的保证。我们的负荷测试工具获得了普通的成功，但从来没有达到我们梦想的成功。 .com泡沫的破灭包含了大量没有将他们的商业模式与定价模式匹配的例证。例如，Kozmo.com公司提供1小时内向城市居民配送零食，租的DVD和星巴克咖啡的服务。不幸的是，它选择的低价，大用户规模的商业模式与使用昂贵的自行车配送员配送这些小东西的现实相矛盾。也许高价，低用户规模的商业模式可能会成功，英式贵族的管家证明了这一点。 改变战略可能会相当困难。比如当英特尔引入8080处理器时标价340美元。最终它以2美元的价格出售，然而英特尔发现要改变人们心中最初的那个高价的印记却极为困难。 实践出真知原理现在，你已经学习了很多理论，它们基于我的经验和充分的研究（有些你可以在本书后面的文献参考中找到）。但是你的实际情况可能与本书描述的可能不尽相同，因此永远也不要忘了实践出真知原理。 产品定价不仅是科学也是一门艺术，工艺。当然理解定价经济学和心理学会有帮助，但理论只能告诉你那么多。在某个时间，你必须做出决策并实施。利用本书中的信息来进行一个好的定价是多少的有根据的尝试，你的客户会如何反应，实际尝试它。完全正确的价格并不重要-让它大致正确，而不是完全搞砸-之后你可以调整。 你永远也不知道你选择的价格是否正确，但是一旦你设定了一个初始价格你就需要进行试验；不是形成假设的科学意义上的试验，改变一个变量，之后接受或者否定这个假设，而是改变一些项然后看会发生什么。 对定价进行科学实验实在太难，其结果也不明确。这当中有太多变量在变化。当你改变你的产品价格时，你可能只是因为你发布了一个新版本，或者与一个大的市场活动同步，或者其它你无法控制的因素，例如经济状况或者竞争对手的反应将交织在一起。 尽管符合科学的纯粹，不过一次只修改一个变量的单纯科学方法并不是经常有意义的。理论上你不应该同时修改你的产品价格，折扣策略和捆绑销售类型。实际而言，可能这是正确的。解决问题要比理解为什么不行更重要。一个科学家去约会不成功，理论上他一次应该改变一个变量，然后重新约会。首先他改变约会对象，但是还是去看同样的电影，买同样的花。之后，他可以保持约会对象不变，花不变，改变电影，以此类推。但是实用主义者会随意改变，或者应该改变约会的女孩，电影，花，再买件新衣服，刮刮胡子等。如果成功了，他可能不知道为什么，但是他至少会有一个女朋友。 在早期，进行试验可能很容易。你可以进行A/B测试，把用户分到随机组，给每个组不同的小册子，上面有不同价格，然后度量结果。而现在，这有着极大风险。网络让人们很轻松的就知道别人付多少钱。 你可能想先做个调查，测试一下人们对新定价模式或者改变现价的反应。然而，调查很少有效。客户所说的和他们所做的总是不一致。当麦当劳发布它的Arch Deluxe汉堡包时（参见上文），焦点人群中的客户喜欢它，80%的人说他们会买。实际很少人真的买了。 如何改变你的定价你可能会担心当你改变价格时客户会如何反应。完全不必如此。对大多数企业而言，我们的客户有其它更值得担心的事。如果我们把价格从100美元涨到150美元，大多数人不会注意，那些注意到的里面又只有很少人会在意。如果你从Red Gate网站购买SQL Compare，在2000年只要50美元，现在你则需要395美元。购买全套工具则需要1595美元。 当然在Red Gate我们花了几乎10年才达到那一价格。我们花了几百万美元开发软件，这是几十个人年的工作。客户从我们的软件获得的价值大大超过了其成本。然而在我们的几十万客户中只有几百个曾经指出我们的价格上涨了。 不是你的客户说价格多么重要，而是他们的行为。无论何时你改变了价格，密切注意你的客户怎么做。如果他们不再购买，那么就需要重新考虑了。 产品定价清单圆满结束！以下是帮助您决定定价的清单： 你的战略是什么？你是想定低价，大量销售，还是高价，少量？这如何与你的品牌，你的产品，以及你想表达的形象匹配？ 你的产品是什么？不要忘记它不只是你销售软件，它包含软件周围的所有内容。 你的客户会如何判断定价的公平？他们会使用什么参考点？他们会如何决定什么价格是看起来正确的？他们会对你选择的价格感到犹豫或者是他们会接受它？ 谁是你的客户？他们的生意是如何运作的？他们期望的收费方式是什么样的？他们有多少钱？他们喜欢一次性付费，还是每月付费？深入他们的心里！ 谁是你的竞争对手？他们对你的定价如何响应？你的产品比他们的价值多或者少多少？他们的商业模式是什么？他们的价格是多少？如果你定价较低，是否会导致价格战？如果你这样做，钱包的钱是否足够多以支持你打赢价格战？你是要和你的竞争对手并存还是要消灭他们？ 你如何销售你的软件？你需要派你的销售人员带客户去打高尔夫吗？或者你计划采用在网络上的低顾客接触方式的销售？你是否需要一个电话销售团队？每个销售人员的成本是多少？你是通过渠道或者经销商来销售？他们将获得多少分成？ 你可以细分客户，创建不同版本吗？你的软件对不同人群价值是否不同？可以用不同定价来反映这一点吗？例如学生和商务人士，或者普通和高级用户，或者你可以根据地区或品味划分？ 你可以如何捆绑销售你的软件？你可以创建一个更大的包，包含不止一个软件吗？ 有根据的推测你的价格不管任何心理学和经济学，你最终还是需要选择一个价格。某个价格-任意价格-要比没有价格好。 实践实践出真知原理。实践你的定价，看有什么结果。如果你的价格大致正确-并且你已经做了这么多应该做的-那么你还可以以后再调整它。 参考文献Anderson, E. and Simester, D. (2003) ‘Minding your pricing cues’Harvard Business Review, September 2003 Breckon, N. (2009) ‘Valve: Left 4 dead half-price sale saw 3000%increase, beat launch numbers’http://www.shacknews.com/onearticle.x/57308 Chapman, R. (2006)‘In Search of Stupidity’, 2nd ed, Apress, Berkeley Crampes, C. and Laffont, J-J. (2002) ‘Copying and software pricing’ Cusumano, M. (2007) ‘The changing labyrinth of software pricing’ Communications of the ACM, Vol. 50, Issue 7, pp. 19-22Davidow, W. (1986) ‘Marketing high technology – an insider’s view’, The Free Press, NewYork Gallaugher, J. and Wallace, E. (2002) ‘Understanding network effects insoftware markets: evidence from web server pricing’ MIS Quarterly, Vol. 26, No. 4, pp. 303-327 Gilbert, A. (2003) ‘CRM software or CRM shelfware?’http://news.cnet.com/CRM-software-or-CRMShelfware/2100-1012_3-990880.html Gourville, J. and Soman, D. (2002) ‘Pricing and the psychology ofconsumption’ Harvard Business Review, September 2002 Gourville, J. (2006) ‘Eager sellers and stony buyers’,Harvard Business Review, June 2006 Gourville, J. and Soman, D. (2005) ‘Overchoice and assortment type:when and why variety backfires’, Marketing Science, Vol. 24, No. 3, pp. 382-395Gourville, J. and Soman, D. (2001) ‘Transaction decoupling: how pricebundling affects the decision to consume’Journal of Marketing Research, Vol. 38, pp. 30-44 Gourville, J. and Soman, D. (2007) ‘Extremeness seeking: when andwhy consumers prefer the extremes’,Harvard Business Review Harford, T. (2008) ‘Business life: Fair trade or foul’http://timharford.com/2008/04/business-life-fair-trade-or-foul/ Knopf, J. (2000) ‘The Origin of Shareware’http://www.asp-shareware.org/users/history-of-shareware.asp Levitt, T. (1980) ‘Marketing success through differentiation – ofanything’,Harvard Business Review, January – February 1980 Macrovision (2007) ‘Key trends in software pricing and licensing’ Mason, M. (2008) ‘The pirate’s dilemma’,The Free Press, New York Miller, P. (2006) ‘Sony losing mad loot on each PS3’http://www.engadget.com/2006/11/16/sony-losing-mad-loot-on-each-ps3/ Murph, D. (2006) ‘Wii Manufacturing Costs ring up to just $158?’http://www.engadget.com/2006/12/15/wii-manufacturing-costs-ring-up-to-just-158/ Packard, D. (1996)‘The HP Way’, HarperCollins, New York Quint, B. (2001) ‘Dialog rolls out new connect-time pricing’http://www.allbusiness.com/sales/1012692-1.html Sink, E. (2004) ‘Product Pricing Primer’,http://www.ericsink.com/bos/Product_Pricing.html Spolsky, J. (2004) ‘Camels and Rubber Duckies’http://joelonsoftware.com/articles/CamelsandRubberDuckies.html Spolsky, J. (2006) ‘Simplicity’ –http://www.joelonsoftware.com/items/2006/12/09.html Stiff, D. (2007) ‘How DeWALT turned customers into influencers’http://credibilitybranding.typepad.com/blog/2007/03/how_dewalt_turn.html Sutton, J. (2001)‘Technology and market structure’, 2nd ed, The MIT Press Varian, H. (2003)‘Intermediate Microeconomics’, 6th ed, W.M. Norton, New YorkWendt O., von Westarp, F. and König, W. (2000) ‘Pricing in NetworkEffect Markets’, ECIS Proceedings, Paper 82Wayne, B. (2009) ‘YouTube is doomed (GOOG)’http://www.businessinsider.com/is-youtube-doomed-2009-4 Wikipedia – ‘3DO Interactive Multiplayer’http://en.wikipedia.org/wiki/3DO_Interactive_Multiplayer]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[在小吃店遇见凯恩斯景气与需求]]></title>
      <url>%2F2013%2F08%2F22%2F%E5%9C%A8%E5%B0%8F%E5%90%83%E5%BA%97%E9%81%87%E8%A7%81%E5%87%AF%E6%81%A9%E6%96%AF%E6%99%AF%E6%B0%94%E4%B8%8E%E9%9C%80%E6%B1%82%2F</url>
      <content type="text"><![CDATA[我们会经常从报纸或者新闻中听到景气复苏或是衰退了，这里提到的景气就是指的“经济成长的趋势”。一般我们有指针景气与体感景气。指针景气：把一定期间内的经济成长结果，以统计数值的方式来呈现的景气指针。体感景气：指企业和消费者对景气的直觉。通常国民对景气的直觉都要比指针景气后知后觉。这也是为什么股票市场了赚钱的总是少数了，呵呵。 那么什么会影响景气呢？通常消费是一个重要因素。一般而言消费可以分为内销和外销，而内销通常包含： 消费需求 投资需求 政府需求 由于商品价格的变化就导致了需求的弹性。如果商品价格上涨，需求大幅减少那么我们称为“弹性”，反之称为“非弹性”。我们每日的生活必需品就是无弹性的，不过这么样，还是得吃饭啊..]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[十分钟学会Markdown]]></title>
      <url>%2F2013%2F08%2F22%2F%E5%8D%81%E5%88%86%E9%92%9F%E5%AD%A6%E4%BC%9AMarkdown%2F</url>
      <content type="text"><![CDATA[接触到Markdown还是2年前，很快就被它的简洁所折服，下面花10分钟来学习一下吧。 1.标题你只需要在文字前面加上#那么就可以表示，标题，#的数目决定了标题的层级，例如我们如果写： # 标题1 ## 标题2 ### 标题3 #### 标题4 那么实际的显示就是 标题1标题2标题3标题42.强调要表示强调也很简单，用一个*表示斜体，两个**来实现黑体，例如 *斜体*，**黑体**将显示为斜体，黑体 3.分段分段更简单连续的行在markdown里面是表示一段的，要分段只需要一个空行就可以 4.链接的表示链接有两种表示方式，一种简单的直接写比如[我的网站](http://www.zjdian.com)，将显示为我的网站。另一种就是采取引用的方式，比如你可以先写下这是[我的网站][1]，然后在文档最后写[1]: http://www.zjdian.com &quot;我的网站&quot;，结果将显示为我的网站 5.列表列表有两种一种有序列表，一种无序列表 + red + green + blue 1. red 1. green 1. blue 将显示为 red green blue red green blue 6. 引用引用直接用&gt;就可以，比如： &gt;这是一段引用显示为： 这是一段引用 7.图片图片的显示与链接类似，![图片提示][url &quot;标题]将显示url对应图片 8.代码如果要显示代码，那么可以直接用html的tag&lt;pre&gt;&lt;/pre&gt;来显示，也可以用````来显示行内代码 十分钟到了，您现在就可以开始Markdown之旅了，要详细学习markdown，请点击这里]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[在小吃店遇见凯恩斯一个关于GDP的笑话]]></title>
      <url>%2F2013%2F08%2F22%2F%E5%9C%A8%E5%B0%8F%E5%90%83%E5%BA%97%E9%81%87%E8%A7%81%E5%87%AF%E6%81%A9%E6%96%AF%E4%B8%80%E4%B8%AA%E5%85%B3%E4%BA%8EGDP%E7%9A%84%E7%AC%91%E8%AF%9D%2F</url>
      <content type="text"><![CDATA[两个聪明的经济学天才青年，经常为一些高深的经济学理论争辩不休。一天饭后去散步，为了某个数学模型的证明两位杰出青年又争了起来，正在难分高下的时候，突然发现前面的草地上有一堆狗屎。 甲就对乙说，如果你能把它吃下去，我愿意出五千万。五千万的诱惑可真不小，吃还是不吃呢？乙掏出纸笔，进行了精确的数学计算，很快得出了经济学上的最优解：吃！于是甲损失了五千万，当然，乙的这顿加餐吃的也并不轻松。 两个人继续散步，突然又发现一堆狗屎，这时候乙开始剧烈的反胃，而甲也有点心疼刚才花掉的五千万了。于是乙说，你把它吃下去，我也给你五千万。于是，不同的计算方法，相同的计算结果——吃！甲心满意足的收回了五千万，而乙似乎也找到了一点心理平衡。 可突然，天才们同时嚎啕大哭：闹了半天我们什么也没有得到，却白白的吃了两堆狗屎！他们怎么也想不通，只好去请他们的导师，一位著名的经济学泰斗给出解释。 听了两位高足的故事，没想到泰斗也嚎啕大哭起来。好容易等情绪稳定了一点，只见泰斗颤巍巍的举起一根手指头，无比激动地说：“1个亿啊！1个亿啊！我亲爱的同学，我代表祖国和人民感谢你们，你们仅仅吃了两堆狗屎，就为国家的GDP贡献了1个亿的产值！”]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[在小吃店遇见凯恩斯关于金融市场1]]></title>
      <url>%2F2013%2F08%2F22%2F%E5%9C%A8%E5%B0%8F%E5%90%83%E5%BA%97%E9%81%87%E8%A7%81%E5%87%AF%E6%81%A9%E6%96%AF%E5%85%B3%E4%BA%8E%E9%87%91%E8%9E%8D%E5%B8%82%E5%9C%BA1%2F</url>
      <content type="text"><![CDATA[回忆一下我们开始时提到的总需求，它由消费需求，投资需求还有外销等构成。1。外销背后的势力－想想汇率如何影响它2。消费与投资背后的势力－想想利息如何影响人们的决策3。综合股价指数而金融市场则有1。短期金融市场（通常为限期1年一下的短期资金往来）2。长期金融市场－股票市场，债券市场等3。外汇市场4。银行间往来的利息－活期贷款利率]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[在小吃店遇见凯恩斯关于汇率]]></title>
      <url>%2F2013%2F08%2F22%2F%E5%9C%A8%E5%B0%8F%E5%90%83%E5%BA%97%E9%81%87%E8%A7%81%E5%87%AF%E6%81%A9%E6%96%AF%E5%85%B3%E4%BA%8E%E6%B1%87%E7%8E%87%2F</url>
      <content type="text"><![CDATA[假设联想卖了一台笔记本到美国，联想收到的是美金，可是在国内美金是无法使用的，因此这时你需要到外汇市场去把美金换成人民币。汇率那么就是你在进行交换时的比率，象现在美元人民币的比率大概就是6.8左右，就是说1美元可以换6.8人民币。说到这里大概你就明白了“汇率升值”和”汇率贬值“了。升值就是说，国内钱币于外币交换的汇率高了，反之就是贬值了。那么是什么在影响汇率呢？1。外销与进口以我国为例，假设我们外销了很多东西到美国，那么我们得到的货款当然都是美金了，外销越多，美金流入就越多，国内美金过多当然价值就下跌，于是”汇率贬值“。反之进口就会导致反向行为2。景气经济景气，外国投资就会多，这些人在中国做生意当然得用人民币，于是，人民币身价就看涨了，汇率贬值。反之外国投资撤退就是升值了3。利息假设国内利息10％，国外银行的是5％。于是这些老外就会把钱往我们这里存，同样导致汇率贬值，就是人民币升值了。之前我们国家提到的热钱，有的就是进入中国来赚这个利差的，象在深圳和香港还有专门做帮人到大陆存钱的生意的。 分析完了这些就可以来看看国家是如何来管理它的1。浮动汇率制度2。固定汇率制度 下一节将对这两种不同制度进行介绍]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[在小吃店遇见凯恩斯外销是如何进行的]]></title>
      <url>%2F2013%2F08%2F22%2F%E5%9C%A8%E5%B0%8F%E5%90%83%E5%BA%97%E9%81%87%E8%A7%81%E5%87%AF%E6%81%A9%E6%96%AF%E5%A4%96%E9%94%80%E6%98%AF%E5%A6%82%E4%BD%95%E8%BF%9B%E8%A1%8C%E7%9A%84%2F</url>
      <content type="text"><![CDATA[对中国经济而言，多年来外销一直是我们的重点，这也是为何美国衰退，我们就经济就会跟着打喷嚏的原因之一，当然我们有13亿多人，内部消费发动起来肯定也是不小的力量的。闲话少说，我们来看看通常的外销过程是如何的，其实外销最关键的区别就是如何把钱收到。这里要提到一个术语“信用证”，它是一种担保书，也就是说老外想买我的东西，他得到他的往来银行申请一个信用证。具体如下：1。对方提供信用证2。我生产，托运，向船舶公司要一份货物寄出证明3。拿着货物寄出证明和信用证，到国内银行申请付款4。国内银行把货物文件和信用证提供给老外的银行，银行确认后就会付款5。拿到这些东西的银行此时就会通知对方“一手交钱一手交货了”]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[在小吃店遇见凯恩斯拉动内需之消费函数]]></title>
      <url>%2F2013%2F08%2F22%2F%E5%9C%A8%E5%B0%8F%E5%90%83%E5%BA%97%E9%81%87%E8%A7%81%E5%87%AF%E6%81%A9%E6%96%AF%E6%8B%89%E5%8A%A8%E5%86%85%E9%9C%80%E4%B9%8B%E6%B6%88%E8%B4%B9%E5%87%BD%E6%95%B0%2F</url>
      <content type="text"><![CDATA[我国这么多年来一直在提的拉动内需，那么要把大家的钱从口袋里面拿出来，关键就是要大家消费，而消费由什么决定呢？先看看消费函数 消费＝影响力（收入，财产，利息，对于未来的期望等） 按照凯恩斯的说法，对消费影响最大的就是收入了，但是人们并没有把所有的收入都用到消费上面，你挣1000块开始你只花600（当然有的人是花的比挣的多哈：（），其余的放到银行，那么这600实际消费就是“边际消费倾向”。而另一方面不管你挣多少，总要填铇肚子，这部分就是“绝对消费”，把上面的情形用公式表示就是 消费=绝对消费＋边际消费倾向 消费需求在总需求中占了相当大的比重，象亚洲金融危机时，政府就展开了通过振作消费需求的发式来试图拯救经济危机，结果当然是很好了！那么政府可以做的有那些呢？消费性金融贷款－顾名思义就是你想到餐馆饱餐一顿有没钱时，金融机构通融（借）给你的钱就叫消费性金融贷款。日常生活中大家已经习惯的的信用卡实际就是其中一种。减税－－什么时候国家会减税啊！！！！！！！！！！！！！ 顺便说一下，我们的邻居韩国在金融危机是政府为了增加消费就是极为夸张的方式发行信用卡的。不过想想目前的中国也有很多信用卡一族，不知道当他们都还不起钱时，经济会是什么结果]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[在小吃店遇见凯恩斯是什么决定着投资]]></title>
      <url>%2F2013%2F08%2F22%2F%E5%9C%A8%E5%B0%8F%E5%90%83%E5%BA%97%E9%81%87%E8%A7%81%E5%87%AF%E6%81%A9%E6%96%AF%E6%98%AF%E4%BB%80%E4%B9%88%E5%86%B3%E5%AE%9A%E7%9D%80%E6%8A%95%E8%B5%84%2F</url>
      <content type="text"><![CDATA[与消费需求一样，没有无缘无故的爱，也没有无缘无故的恨，在投资的背后冥冥之中有一股力量在决定这投资。那就是投资函数 投资函数＝（预期收益，利息，动物本能，其他） 这里要重点提一下“动物本能”，什么是“动物本能”，通俗的说就是“狗啃骨头，猫吃鱼，奥特曼打小怪兽”。在凯恩斯之前的经济学家都认为利息对个人投资决定造成重大影响，开始凯恩斯却认为不然，应该是企业家的动物本能，乍听之下可能还有点不可理解，开始当你想想为什么富人是富人，穷人是穷人时就明白了，穷人有钱就会存起来，而那些想成为富人的就会投资，这不是动物本能是什么？当然富人里面那些靠贪污腐败，权力而富裕的有另当别论了。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[在小吃店遇见凯恩斯是什么在控制利率]]></title>
      <url>%2F2013%2F08%2F22%2F%E5%9C%A8%E5%B0%8F%E5%90%83%E5%BA%97%E9%81%87%E8%A7%81%E5%87%AF%E6%81%A9%E6%96%AF%E6%98%AF%E4%BB%80%E4%B9%88%E5%9C%A8%E6%8E%A7%E5%88%B6%E5%88%A9%E7%8E%87%2F</url>
      <content type="text"><![CDATA[大家的心都跟着利率在上下波动。那么是什么在控制它呢？ 1.金融市场钱的多寡当债券市场钱满为患时利率就会下降，同理前少了就会上涨2.公开流通的钱－通货量通常市面流通的钱有两种：央行印的钱和准备金通货 央行印钱基于以下情况 政府要借钱 民间银行来调头寸（就是民间银行向央行借钱） 外销增加（想想出口创汇的口号，企业拿到外汇，银行不一定需要，那么这个钱就换到央行了，可是1个美元进去，7块人民币就得出来阿！！） 对于保证金通货，想想银行怎么赚钱就明白了，你在银行存1000块，银行还要付你利息，那他怎么赚钱。银行会把其中的300放在那，以防你随时来取点走，剩余的700就贷出去了。这时我们的通货量就是1000＋700＝1700了。 到此我们可以来解释M1，M2，M3了。每个月国家统计局都有这样的报告， M1（通货）－现金＋存在银行的钱M2（准通货）－M1+储蓄准备金（定期存款）M3（总通货）－不止银行，投资信托公司的钱都算上]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[软件项目经理的五项修炼]]></title>
      <url>%2F2013%2F08%2F22%2F%E8%BD%AF%E4%BB%B6%E9%A1%B9%E7%9B%AE%E7%BB%8F%E7%90%86%E7%9A%84%E4%BA%94%E9%A1%B9%E4%BF%AE%E7%82%BC%2F</url>
      <content type="text"><![CDATA[很早以前就有想对个人这些年的项目管理实践，以及看到的项目管理方法进行一个总结，总算动笔了;-)(太懒了！)各位看官个人的经验主要是基于个人实践，也可能很多是错误的，还欢迎大家拍砖。那么先介绍一下自己的经验： 1.大概我98还在学校时参加了一个商业软件的开发，当时第一个项目做了1年多，系统采用Windows平台，C＋＋，涉及的主要是模具的参数化设计，以及切割仿真。项目有4人参加，我是项目经理兼主要的系统设计者，最终系统是勉强的交付。问题主要在于：a.这是一个合作项目，很多需求不明确，同时我们项目组的成员实际对模具的设计并不是很熟悉，b.我们的开发没有一个明确的项目计划，管理过程等 c.没有任何的软件配置管理系统 d.没有进行系统的测试，几乎没有单元测试 这是我的第一个项目，完全是“山寨”开发模式，也就是从这个项目后我开始认识到了项目管理的重要开始进行系统的学习 2.大概2001年我开始了更加正规的软件开发（CMM）项目，系统学习了CMM开发过程，同时也不断的去摸索，自己也当了兼职的项目配置管理员。这个项目通过了CMM4的评估，项目也获得了公司的质量奖。提一下这个项目采用了C开发，属于嵌入式系统应用，要求HA。这个项目让我有机会参加了一个软件项目从需求到发布的全过程，对我将来作项目经理是收益非浅，个人认为没有这样的全项目开发过程经历的项目经理是很难成功的。 3.2002年下半年我开始再次担任项目经理，同样是嵌入式系统，通讯协议开发，要求高可靠性。期间我通过了PMP的认证应该说这是一个实践中学习的机会，前一个项目我是看别人做，这一个项目我要自己来带，应验了老话“说起来容易，做起来难”，也有犯错误的时候，不过这个项目应该说是比较成功的项目的管理三角“时间，成本，质量”除了进度稍有推迟其余的都很好。这个项目是我的第一个成功项目，同时有让我在系统学习项目管理的知识同时，可以有机会实践。该项目的个人问题是只是项目管理，而忘记了其实项目经理还会涉及人的管理，尽管资源经理或者部门经理管人，但在他们没有注意到问题时，项目经理应该去协调或者提出来） 4.2004初我开始担任一个嵌入式Linux平台项目的经理，有了上一次的经验，这个项目整体还是不错的。不过这个项目给我的体会就是项目经理与部门经理的合作，以及有时项目经理懂技术会很好的帮助你管好项目。所以这期间我通过了系统分析员考试，同时系统的学习一下嵌入式Linux，那么在你和系统架构师沟通时你能知道问题所在，在了解项目状态风险时你也能一语中的。个人认为项目经理应该懂技术。这个项目的问题是，当时我开始接触敏捷和XP，一度想应用，后来发现一些问题还好能即使的纠正;-)，给我的教训在真正开始项目成员的敏捷知识教育，以及没有很好的理解前不要看着新东西就想用。 5.2004年底到2005带了两个项目，前一个项目让我了解了整个手机的开发，该项目相对比较大，涉及了子项目，因此如何进行多项目管理是这里面的主要问题。第二个项目是相对较小的项目，不过这是一个涉及异地开发的项目，让我也获益非浅，因为这里面的状态同步，异地沟通就十分重要了。 5.2006年开始了一个涉及研发从欧洲到中国迁移的项目，这个项目让我掌握了如何进行这一类的研发迁移比如招聘，组建团队，研发人员培训，实验室建立等等。当然技术方面就是对IMS有了更多的学习。 6.2006年后基本就没有做项目经理了改作研发经理。不过这期间系统学习了敏捷开发，Scrum也进行了实践，之后开始学习program manager，以及财务的管理。让我实际对项目管理又有了更进一步认识，这段时间的工作培养了我的大局观，看到的不再是一个项目，而是对Business的影响了。07年又开始了新的挑战，在multi-site，multi-cultural环境下工作更加高效，进一步知道一个研发中心的运作，技术方面就是解决方案的学习了，从“叶”到“树” 总结下来我觉得做一个好的项目经理应该如论语说的”君子不器“，同时应该是不断的学习成长。不过个人认为一个好的软件项目经理应该从以下方面进行学习： 项目管理知识 软件工程知识 相应的软件技术 软件工具 第5项修炼－学习的能力]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[在小吃店遇见凯恩斯浮动汇率与固定汇率]]></title>
      <url>%2F2013%2F08%2F22%2F%E5%9C%A8%E5%B0%8F%E5%90%83%E5%BA%97%E9%81%87%E8%A7%81%E5%87%AF%E6%81%A9%E6%96%AF%E6%B5%AE%E5%8A%A8%E6%B1%87%E7%8E%87%E4%B8%8E%E5%9B%BA%E5%AE%9A%E6%B1%87%E7%8E%87%2F</url>
      <content type="text"><![CDATA[浮动汇率就是说，汇率随时都在变化，今天1美元换7块人民币，下个月说不定就是1美元换6块了。这样带来的问题就是，外销或者进口企业得时常为汇率头痛了，像去年美元贬值，国内的航空业的业绩一下就上去了，其实非也，这是因为航空公司买了飞机钱是用美元记的，现在美元相对人民币贬值了，当然就相当于航空公司赚了。由于汇率的变化，所以就带来了 汇差损 汇差益 好了，有人就说了，既然如此，为什么我们不用固定汇率制度，1美元永远是7人民币。不然，如果这样，假设我们大量的出口美国，随着美金的增多，我国货币就贬值了(学而时习之，想想为什么？)，由于贬值物价就上涨，如果要降低物价，避开与美国的通商压力，最好的办法就是减少外销。为减少出口就得降低汇率，而这种盯死1美元7人民币的制度下这不可能。所以说固定汇率的缺点就是无法调整国际收支。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[在小吃店遇见凯恩斯用于投资的钱从哪里来]]></title>
      <url>%2F2013%2F08%2F22%2F%E5%9C%A8%E5%B0%8F%E5%90%83%E5%BA%97%E9%81%87%E8%A7%81%E5%87%AF%E6%81%A9%E6%96%AF%E7%94%A8%E4%BA%8E%E6%8A%95%E8%B5%84%E7%9A%84%E9%92%B1%E4%BB%8E%E5%93%AA%E9%87%8C%E6%9D%A5%2F</url>
      <content type="text"><![CDATA[企业需要钱来发展规模，那么钱从哪里来呢？根据募集资金的方式可以分为 内部融资 外部融资 内部融资就是从自己口袋拿钱，企业做生意，把自己赚来的钱作为投资资金，就叫内部融资。外部融资就是拿别人的钱，最常见的就是股票，此外还有向人借钱，那就是债券了。 以上是对企业，那么对个人而言无论如何都是自己的钱了。所以投资前一定要考察： 稳定性－我的钱还能回来不 收益性 流动性－是不是能及时变现 金融商品收益低，但流动性和稳定性好股票则收益高，但稳定性差不动产不论稳定性，收益性和流动性都不是最优秀的，不过当物价涨的天昏地暗的时候，可以说是比较可行的投资手段。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[在小吃店遇见凯恩斯经济学他妈]]></title>
      <url>%2F2013%2F08%2F22%2F%E5%9C%A8%E5%B0%8F%E5%90%83%E5%BA%97%E9%81%87%E8%A7%81%E5%87%AF%E6%81%A9%E6%96%AF%E7%BB%8F%E6%B5%8E%E5%AD%A6%E4%BB%96%E5%A6%88%2F</url>
      <content type="text"><![CDATA[亚当.斯密是经济学之父，谁有是他妈呢？先买个关子，一会再说。首先回到18世纪，那时是一个物质匮乏的年代，所有东西“做出来就卖光光”，所谓“供给，它会自行创造需求”。到了20世纪初美国的20年代末经济大萧条，随后席卷全球，人民突然发现不是那么回事了，东西不好卖了。写到这不由想到在世界是平的的今天，美国的次贷危机愈演愈烈，前几天雷曼破产，美林被卖，中国能否独善其身？ 闲话少说，回到刚提到的大萧条，这时凯恩斯跳了出来，他说了一句话“发现需要”，在这个物质过剩的年代我们要发现需要，强调“需求的重要性”。接着凯恩斯有说了“政府不应该继续蹲在角落默不作声，而是应该站到历史的舞台拯救世界”。他的方法很简单，要致富先修路，修桥，其实也就是政府要要拉动需求，因为这样创造就业，人民就有了钱，又可以吃香喝辣了，于是库存就没了。这也是我们国家一直以来采取的手段;-(，不过现在国家开始提拉动内需的了，也就是在经济增长的三架马车投资需求，消费需求，出口需求中的消费需求。不过国家没有解决老百姓的养老，教育，看病问题前，这个内需要拉动还是的费点力气。至少偶这个小白领的内需拉的不是很动，呵呵。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[在小吃店遇见凯恩斯经济学的爹]]></title>
      <url>%2F2013%2F08%2F22%2F%E5%9C%A8%E5%B0%8F%E5%90%83%E5%BA%97%E9%81%87%E8%A7%81%E5%87%AF%E6%81%A9%E6%96%AF%E7%BB%8F%E6%B5%8E%E5%AD%A6%E7%9A%84%E7%88%B9%2F</url>
      <content type="text"><![CDATA[既然要谈经济学的前世今生，那必然先要从什么是经济学开始？简单讲，经济就是“经世济民”的缩写，也可以说为“经营国家，救赎百姓”。在18世纪前人们一直认为黄金是致富的源泉，那时没有美元，交易一律都是用黄金，因为黄金会随着物品进口而流失，所以国家就不计手段尽量出口，减少进口。这时经济学之父亚当.斯密诞生了(1723~1790)，写了《国富论》（国民财富的性质和原因的研究）他提出了几个非常重要的观点： 国家的富裕不在于拥有多少黄金，而是在于民生必需品如食物、衣物外，还有多少其它生活所需要的生活用品而定。他认为国家要致富就得不再加以限制或干涉进出口，（而不是少生孩子多种树）。 发掘利己心。做生意的本质就是赚钱，为了赚钱所以我们才不断的提高产品质量，提供更好的服务。Intel的芯片越卖越便宜，性能越来越好绝不是良心发现，而是为了让你乖乖的掏钞票。当所以的企业相互竞争，我们消费者就得到了更好的服务 发现市场，看不见的手。 最好的办法就是政府不做任何事，静静的等待，即使个人只是为了自身的利益做事，但市场这只看不见的手会把个人做的引导为大家的利益。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[在小吃店遇见凯恩斯经济硬着陆与软着陆-1]]></title>
      <url>%2F2013%2F08%2F22%2F%E5%9C%A8%E5%B0%8F%E5%90%83%E5%BA%97%E9%81%87%E8%A7%81%E5%87%AF%E6%81%A9%E6%96%AF%E7%BB%8F%E6%B5%8E%E7%A1%AC%E7%9D%80%E9%99%86%E4%B8%8E%E8%BD%AF%E7%9D%80%E9%99%86-1%2F</url>
      <content type="text"><![CDATA[这几年我们经常听到国家在说，要防止经济过热，不能够经济硬着陆，要软着陆之类的话语。这是什么意思呢？首先经济和气候一样有景气好，景气不好的时候，对应就有了经济的春天，夏天，秋天和冬天。 经济的冬天2000年的时候华为的任正非的提出了“华为冬天”，今年马云又提出了“互联网的冬天”，到底什么是冬天？简单来说就是景气衰退，企业开始倒下（资产货币膨胀：如股票或不动产各种资产价格下跌。中国的沪指从6000跌到了1800点是不是冬天？呵呵，偶也不知道）于是企业开始缩减开支，失业增加，人们对未来预期降低于是开始减少开支，商家则开始降价打折。随着而来，全面物价下跌，企业也不敢做投资，银行的钱就会过剩，又没人来借，自然利息就下降。用一个公式来说明就是 可能产量 &gt; 消费需求+投资需求+政府需求+外销需求 经济的春天既然冬天来了春天还会远吗？经过一段时间（这个一段有时要好多年，哈哈），乌云总会飘散。还记得前面提到的凯恩斯吗？这个时候就是政府应该出面的时候，于是我们的政府就开始拉动需求，怎么办，进行政府投资，修路，修桥子类，政府不像民间企业牵一发而动全身，于是大量工作机会出现。进而人民收入增加，消费也就产生了，一旦消费增加，企业就好办了，于是景气就好转了，春天来了。同样用一个公式，此时就是：可能产量 = 消费需求+投资需求+政府需求+外销需求 明天再来谈关于经济过热和软着陆的问题.]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[在小吃店遇见凯恩斯经济硬着陆与软着陆-2]]></title>
      <url>%2F2013%2F08%2F22%2F%E5%9C%A8%E5%B0%8F%E5%90%83%E5%BA%97%E9%81%87%E8%A7%81%E5%87%AF%E6%81%A9%E6%96%AF%E7%BB%8F%E6%B5%8E%E7%A1%AC%E7%9D%80%E9%99%86%E4%B8%8E%E8%BD%AF%E7%9D%80%E9%99%86-2%2F</url>
      <content type="text"><![CDATA[经济的夏天 天气温暖过了头可能就变成了炎炎夏日了，严重的话还会被晒伤，变成皮肤癌也说不定。经济也是如此，由于景气的向好，企业开始前仆后继的向银行借钱以设立新工厂，扩大产出，于是银行利息也水涨船高，一旦循环下去就会导致原材料的供不应求，进而影响到经济中的各个方面，于是物价开始上涨，进而全面物价上涨。说到这里你也许开始明白了前几年我们的物价为什么上涨了吧？而且大家的感觉到的时间要比国家提出防止经济过热的时间晚，这也是旁证指针景气要比体感景气先知先觉了;-(如果我们放任这种情形，由于原材料和利息的暴涨，许多跟风的企业可能会出现资金的短缺，一旦这些企业开始关门大吉，银行就会出问题，为什么？钱都是从银行来的。所以在景气太好导致问题前，政府就会天天喊我们要防止经济硬着陆.硬着陆其实就是景气急速变化，经济突然下滑，对应的软着陆就是要像飞机那样一点点的下来，不是垂直起降，否则乘客都得完蛋了。同样的用公式来表示夏天： 可能产量 &lt; 消费需求＋投资需求＋政府需求＋外销需求 最后提一下，我们国家的升息和准备金率。这些年一直都是在加息，准备金率也一度到了16.5，在了解了经济的夏天后，大家可以思考一下为什么？前不久央行宣布了降息和下调准备金虑，是否是进入了降息周期了呢？子曰“学而时习之，不亦乐乎” 经济的秋天 冬天，春天，夏天清楚了，秋天就很好了解了。这时就是好景气到了一定时间，开始慢慢平淡，企业逐步减少投资，国民减少消费…. 关于通货膨胀：就是物价急速上升，反之就是通货紧缩了。通常通货膨胀是伴随景气过热，紧缩则反之。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[在小吃店遇见凯恩斯谁是有钱人以及GDP-GNP-GNI]]></title>
      <url>%2F2013%2F08%2F22%2F%E5%9C%A8%E5%B0%8F%E5%90%83%E5%BA%97%E9%81%87%E8%A7%81%E5%87%AF%E6%81%A9%E6%96%AF%E8%B0%81%E6%98%AF%E6%9C%89%E9%92%B1%E4%BA%BA%E4%BB%A5%E5%8F%8AGDP-GNP-GNI%2F</url>
      <content type="text"><![CDATA[有多少钱才算得上是有钱人？是有家财万贯的有钱人更有钱还是象唐骏这样的高级经理人更有钱？如果你是一位美女，你会挑前者还是后者？刚才看到一篇八卦文章，“向中年名导的太太那样调教老公”提到李安，王家卫和吴宇森，都是有过落魄的日子，不过这些太太的“投资”最后都得到了回报了。所以说选家财万贯，还是投资未来的成长股，这是个问题。与个人一样，判断国家的富有程度也是如此，这个标准就是GNP与GDP。 GNI：Gross National Income 国民总收入，根据国际进口，出口计算的收入。比如说A国1年产3吨土豆，他们卖了2吨，换了1吨牛肉。那么这个国家的GNI就是1吨土豆+1吨牛肉。1年后吃牛肉的国家多了国际上要2.5吨土豆才换得到1吨牛肉了，那么GNI就变成了0.5吨土豆+1吨牛肉。 GDP：Gross Dorrlestic Product 国内生产总值 则是指国内所有市场物品的量，不管你是阿猫还是阿狗生产的，全部加起来就对了。 GNP：是指1年来该国人民所有生产的总值。不管是在月球，还是地球，只要是这个国家的人民生产的就算数 讲到这里，我想大家也许有了点概念了，GNP是考虑了国际间的波动，GDp则把重点放到了国内的数值。那么在外国投资还比较少的适合GNP是考察国家富有程度的指标，不过到了今天GDP则更能反应国家的富有程度了，为什么？你只要到大家上看看，那么多的外资宝马，奔驰…。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[TED城市之歌-我们如何设计我们的城市]]></title>
      <url>%2F2013%2F08%2F16%2FTED%E5%9F%8E%E5%B8%82%E4%B9%8B%E6%AD%8C-%E6%88%91%E4%BB%AC%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E6%88%91%E4%BB%AC%E7%9A%84%E5%9F%8E%E5%B8%82%2F</url>
      <content type="text"><![CDATA[关于城市的演讲。听完了有着许多的感触，如果我们的城市管理者都看看应该多好啊？看看其中的观点: 城市不是一个问题，而应该是一个解决方案 所有的城市都可以在3年内改进 每一个城市都应该有自己的理念，设计 我们要教育下一代 （其中一个通过教育小孩垃圾分类，如何孩子教育父母的例子，让他们的城市达到了70%的垃圾分类！！！不禁让我思考我们的学校仍痴迷于思想品德，政治，我们何时进行过这样的教育！！！） 引用作者的一句话“创意源于将你的预算减少一个0！”，我生活的城市正在经历着大规模的建设，交通拥堵，在我们的政府“慷慨”投入来自我们这些小民的税收进行这些建设的时候也请思考一下：从新修路是唯一选择吗？看着2年前修好的立交，下穿隧道，人行天桥被破坏不禁问这是我们城市的设计和理念吗？我们有“教育”吗？ 附上TED视频]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[TED演讲社会媒体的转折点]]></title>
      <url>%2F2013%2F08%2F16%2FTED%E6%BC%94%E8%AE%B2%E7%A4%BE%E4%BC%9A%E5%AA%92%E4%BD%93%E7%9A%84%E8%BD%AC%E6%8A%98%E7%82%B9%2F</url>
      <content type="text"><![CDATA[这是以印尼海啸后的博客现象的为导入的TED演讲James Surowiecki在演讲中问了我们三个问题，也是思考： -是什么驱使人们做事？ -在一定情境下集体会remarkable intelligent -集体智慧也有着不好的一面，当我们与网络，群体联系越来越紧密的时候，我们正在失去自己的独立性，网络观点影响了你，你没有了自己 博客的流行已经是好几年前的事情了，在微博与微信当道的年代，网络，集体智慧给我们带来了许多的好处，给了我们更多的视角，人们也在分享，协作过程中获得的满足，成就，有了微博上的微公益，有了对公权的监督，我们有了维基百科，我们集体帮助需要帮助的人，解决问题，但是我们是否也应该思考我们是否失去了自己的独立性？是否微博大号成为了你的代言人？ 一方面我们在阅读《失控》，了解蜂群效应，去中心化，自底向上等等，也建议思考网络给我们带来什么？我们在去中心化时，网络的集体智慧，心理学上的社会认同,又会让我们跟随。我们会不会像演讲者说的跟随的蚂蚁那样转圈到死？]]></content>
    </entry>

    
  
  
</search>
